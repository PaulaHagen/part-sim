{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c736373",
   "metadata": {},
   "source": [
    "# Use Centralized PPO from Stable Baselines-3 on Custom Particle Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1b2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paula/.pyenv/versions/part-sim-stable/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "# This is the run-through with random actions\n",
    "import particle_v1\n",
    "env = particle_v1.parallel_env(num_agents=10, num_food_sources=1, flow = 'none', max_cycles=25, render_mode='human')\n",
    "\n",
    "env.reset() # Do seed=42 for reproducibility\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        action = env.action_space(agent).sample() # this is where you would insert your policy\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868167af",
   "metadata": {},
   "source": [
    "We make our environment match the stable baselines algorithms (needs to be vectorised)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "555fdd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import particle_v1\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the PettingZoo environment\n",
    "env = particle_v1.parallel_env(num_agents=10, num_food_sources=1, flow='none', max_cycles=25, render_mode='human')\n",
    "\n",
    "# Convert to parallel API (needed for supersuit + SB3)\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "\n",
    "# Optional: normalize observations and rewards to ease training of neural net (makes sense with extreme values, but here, we have a window of -1 to 1)\n",
    "env = ss.concat_vec_envs_v1(env, num_vec_envs=1, num_cpus=1, base_class=\"stable_baselines3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad2d60",
   "metadata": {},
   "source": [
    "Now, we train a simple PPO model. Note, that this is not true MAPPO (multi-agent), because we have centralised policy AND execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f0372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 9     |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 2067  |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    n_steps= 2048, # (DEFAULT 2048) number of steps to run for each environment per update (will stop after n steps (step() function in env))\n",
    ")\n",
    "# took about 34 minutes for 10000 time steps\n",
    "model.learn(total_timesteps=10000) # The total number of samples (env steps) to train on (over all environments/updates). Only cut-off value.\n",
    "\n",
    "model.save(\"models/mappo_shared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63d894",
   "metadata": {},
   "source": [
    "We evaluate our model on a new environment instance. (Opens visualisation automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7535ab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards at step 0:\n",
      "[-1.1371735  -0.13711435 -0.84482586 -0.25118434 -0.9150127  -0.99484783\n",
      " -0.5816918  -2.2213273  -1.5276202  -0.82559264]\n",
      "Rewards at step 1:\n",
      "[-1.0530511  -0.13102573 -0.7456678  -0.21609263 -0.90993154 -1.0004635\n",
      " -0.65373605 -2.2090368  -1.5537815  -0.8217115 ]\n",
      "Rewards at step 2:\n",
      "[-0.9965045  -0.15468349 -0.77245975 -0.19054812 -0.810871   -0.91794074\n",
      " -0.63505465 -2.2410457  -1.5452732  -0.84965503]\n",
      "Rewards at step 3:\n",
      "[-0.8910552  -0.20312041 -0.73831147 -0.16368963 -0.6774763  -0.8070537\n",
      " -0.6720824  -2.2410457  -1.6245698  -0.86827207]\n",
      "Rewards at step 4:\n",
      "[-0.77923787 -0.2242342  -0.71944207 -0.12690601 -0.57119375 -0.69667387\n",
      " -0.6283018  -2.1555922  -1.8042825  -0.88829845]\n",
      "Rewards at step 5:\n",
      "[-0.6331607  -0.2202357  -0.69307995 -0.14415388 -1.4545912  -0.6916193\n",
      " -1.5362015  -2.0915108  -1.9282336  -0.9469649 ]\n",
      "Rewards at step 6:\n",
      "[-0.58991045 -0.25735444 -0.6882543  -0.16100298 -0.29906115 -0.6624637\n",
      " -0.52737474 -1.9711088  -1.9340165  -1.0109391 ]\n",
      "Rewards at step 7:\n",
      "[-0.5745375  -0.29462683 -0.69199526 -0.1818989  -0.18765861 -0.60614765\n",
      " -0.5981048  -2.9205658  -2.8298292  -1.0076233 ]\n",
      "Rewards at step 8:\n",
      "[-0.5626827  -0.28424743 -0.77186966 -0.23727755 -0.11776541 -0.5958673\n",
      " -0.6406961  -2.7494648  -2.6404622  -1.001056  ]\n",
      "Rewards at step 9:\n",
      "[-0.51874983 -0.26101232 -0.8105206  -0.28966787 -0.07446644 -0.5107078\n",
      " -0.7642711  -2.525352   -2.4011834  -0.98916715]\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of environment\n",
    "import particle_v1\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = particle_v1.parallel_env(num_agents=10, num_food_sources=1, flow='none', max_cycles=25, render_mode='human')\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = ss.concat_vec_envs_v1(env, num_vec_envs=1, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "obs = env.reset()\n",
    "\n",
    "# Reload model\n",
    "model = PPO.load(\"models/mappo_shared\", env=env)\n",
    "\n",
    "for i in range(10):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, rewards, dones, infos = env.step(action) # all vectors of size num_agents\n",
    "    # if done = 1 (True), the action is done \n",
    "    print(f\"Rewards at step {i}:\")\n",
    "    print(rewards)\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a5854",
   "metadata": {},
   "source": [
    "This was a fixed loop but we can also iterate until the agents are all set to \"done\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f3990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import particle_v1\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = particle_v1.parallel_env(num_agents=10, num_food_sources=1, flow='none', max_cycles=25, render_mode='human')\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = ss.concat_vec_envs_v1(env, num_vec_envs=1, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "obs = env.reset() # Do seed=42 for reproducibility\n",
    "\n",
    "# Reload model\n",
    "model = PPO.load(\"models/mappo_shared\", env=env)\n",
    "\n",
    "dones = [False] * env.num_envs  # Track done status for each environment\n",
    "\n",
    "while not all(dones):\n",
    "    # Predict actions for all agents\n",
    "    actions, _ = model.predict(obs)\n",
    "    \n",
    "    # Step through the environment\n",
    "    obs, rewards, dones, infos = env.step(actions)\n",
    "    print(dones)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "part-sim-stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
