{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6080debb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "2025-10-07 15:17:31,312\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "2025-10-07 15:17:31,313\tWARNING algorithm_config.py:5045 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-10-07 15:17:32,484\tINFO worker.py:1951 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     16\u001b[39m tmp_env.close()\n\u001b[32m     18\u001b[39m config = (\n\u001b[32m     19\u001b[39m     PPOConfig()\n\u001b[32m     20\u001b[39m     .environment(\u001b[33m\"\u001b[39m\u001b[33msimple_v3\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     )\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m algo = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(algo.train())\n\u001b[32m     33\u001b[39m algo.save(\u001b[33m\"\u001b[39m\u001b[33mppo_model_checkpoint\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/deprecation.py:128\u001b[39m, in \u001b[36mDeprecated.<locals>._inner.<locals>._ctor\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m     deprecation_warning(\n\u001b[32m    122\u001b[39m         old=old \u001b[38;5;129;01mor\u001b[39;00m obj.\u001b[34m__name__\u001b[39m,\n\u001b[32m    123\u001b[39m         new=new,\n\u001b[32m    124\u001b[39m         help=help,\n\u001b[32m    125\u001b[39m         error=error,\n\u001b[32m    126\u001b[39m     )\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Call the deprecated method/function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm_config.py:5806\u001b[39m, in \u001b[36mAlgorithmConfig.build\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   5804\u001b[39m \u001b[38;5;129m@Deprecated\u001b[39m(new=\u001b[33m\"\u001b[39m\u001b[33mAlgorithmConfig.build_algo\u001b[39m\u001b[33m\"\u001b[39m, error=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   5805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m5806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm_config.py:1001\u001b[39m, in \u001b[36mAlgorithmConfig.build_algo\u001b[39m\u001b[34m(self, env, logger_creator, use_copy)\u001b[39m\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.algo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    999\u001b[39m     algo_class = get_trainable_cls(\u001b[38;5;28mself\u001b[39m.algo_class)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:536\u001b[39m, in \u001b[36mAlgorithm.__init__\u001b[39m\u001b[34m(self, config, env, logger_creator, **kwargs)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# Evaluation EnvRunnerGroup and metrics last returned by `self.evaluate()`.\u001b[39;00m\n\u001b[32m    534\u001b[39m \u001b[38;5;28mself\u001b[39m.eval_env_runner_group: Optional[EnvRunnerGroup] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/trainable/trainable.py:158\u001b[39m, in \u001b[36mTrainable.__init__\u001b[39m\u001b[34m(self, config, logger_creator, storage)\u001b[39m\n\u001b[32m    154\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._open_logfiles(stdout_file, stderr_file)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m setup_time = time.time() - \u001b[38;5;28mself\u001b[39m._start_time\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m setup_time > SETUP_TIME_THRESHOLD:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:644\u001b[39m, in \u001b[36mAlgorithm.setup\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    640\u001b[39m     \u001b[38;5;28mself\u001b[39m.offline_data = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_online \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.enable_env_runner_and_connector_v2:\n\u001b[32m    643\u001b[39m     \u001b[38;5;66;03m# Create a set of env runner actors via a EnvRunnerGroup.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     \u001b[38;5;28mself\u001b[39m.env_runner_group = \u001b[43mEnvRunnerGroup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# New API stack: User decides whether to create local env runner.\u001b[39;49;00m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Old API stack: Always create local EnvRunner.\u001b[39;49;00m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    653\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43menable_env_runner_and_connector_v2\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_local_env_runner\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtune_trial_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;66;03m# Compile, validate, and freeze an evaluation config.\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluation_config = \u001b[38;5;28mself\u001b[39m.config.get_evaluation_config_object()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py:133\u001b[39m, in \u001b[36mEnvRunnerGroup.__init__\u001b[39m\u001b[34m(self, env_creator, validate_env, default_policy_class, config, local_env_runner, logdir, _setup, tune_trial_id, pg_offset, num_env_runners, num_workers, local_worker)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28mself\u001b[39m._policy_class = default_policy_class\n\u001b[32m    132\u001b[39m \u001b[38;5;28mself\u001b[39m._remote_config = config\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28mself\u001b[39m._remote_config_obj_ref = \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_remote_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mself\u001b[39m._remote_args = {\n\u001b[32m    135\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_cpus\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._remote_config.num_cpus_per_env_runner,\n\u001b[32m    136\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_gpus\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._remote_config.num_gpus_per_env_runner,\n\u001b[32m   (...)\u001b[39m\u001b[32m    142\u001b[39m     ),\n\u001b[32m    143\u001b[39m }\n\u001b[32m    144\u001b[39m \u001b[38;5;28mself\u001b[39m._tune_trial_id = tune_trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:21\u001b[39m, in \u001b[36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauto_init_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mauto_init_ray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:15\u001b[39m, in \u001b[36mauto_init_ray\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m auto_init_lock:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ray.is_initialized():\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33minit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/_private/worker.py:1953\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(address, num_cpus, num_gpus, resources, labels, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, logging_config, log_to_driver, namespace, runtime_env, enable_resource_isolation, system_reserved_cpu, system_reserved_memory, **kwargs)\u001b[39m\n\u001b[32m   1950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1951\u001b[39m     logger.info(info_str)\n\u001b[32m-> \u001b[39m\u001b[32m1953\u001b[39m \u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_global_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_global_node\u001b[49m\u001b[43m.\u001b[49m\u001b[43msession_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdriver_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_to_driver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_to_driver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglobal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdriver_object_store_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_driver_object_store_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_private\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_entrypoint_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1964\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1965\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m job_config \u001b[38;5;129;01mand\u001b[39;00m job_config.code_search_path:\n\u001b[32m   1966\u001b[39m     global_worker.set_load_code_from_local(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/_private/worker.py:1096\u001b[39m, in \u001b[36mwith_connect_or_shutdown_lock.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1093\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _connect_or_shutdown_lock:\n\u001b[32m-> \u001b[39m\u001b[32m1096\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/_private/worker.py:2428\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(node, session_name, mode, log_to_driver, worker, driver_object_store_memory, job_id, namespace, job_config, runtime_env_hash, startup_token, ray_debugger_external, entrypoint, worker_launch_time_ms, worker_launched_time_ms, debug_source, enable_resource_isolation)\u001b[39m\n\u001b[32m   2426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2427\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m faulthandler.is_enabled():\n\u001b[32m-> \u001b[39m\u001b[32m2428\u001b[39m         \u001b[43mfaulthandler\u001b[49m\u001b[43m.\u001b[49m\u001b[43menable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2429\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m io.UnsupportedOperation:\n\u001b[32m   2430\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ipykernel/kernelapp.py:529\u001b[39m, in \u001b[36mIPKernelApp.patch_io.<locals>.enable\u001b[39m\u001b[34m(file, all_threads, **kwargs)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34menable\u001b[39m(file=sys.__stderr__, all_threads=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfaulthandler_enable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_threads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 1] Operation not permitted"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from mpe2 import simple_v3\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "\n",
    "def make_env(env_config=None):\n",
    "    return simple_v3.parallel_env(max_cycles=25, render_mode=None)\n",
    "\n",
    "register_env(\"simple_v3\", lambda config: PettingZooEnv(make_env(config)))\n",
    "\n",
    "# Temporary env instance just to grab one agent's space\n",
    "tmp_env = PettingZooEnv(make_env())\n",
    "first_agent = list(tmp_env.observation_space.keys())[0]\n",
    "obs_space = tmp_env.observation_space[first_agent]\n",
    "act_space = tmp_env.action_space[first_agent]\n",
    "tmp_env.close()\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"simple_v3\")\n",
    "    .training(\n",
    "        train_batch_size_per_learner=2000,\n",
    "        lr=0.0004,\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\"shared_policy\": (None, obs_space, act_space, {})},\n",
    "        policy_mapping_fn=lambda agent_id, *args, **kwargs: \"shared_policy\",\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "print(algo.train())\n",
    "algo.save(\"ppo_model_checkpoint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "887d591c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-10-07 13:42:31,791\tERROR actor_manager.py:873 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=26135, ip=127.0.0.1, actor_id=b81f12d1d6e21e392a0034f701000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x155952390>)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/default_ppo_rl_module.py\", line 31, in setup\n",
      "    self.catalog.actor_critic_encoder_config.base_encoder_config,\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'actor_critic_encoder_config'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=26135, ip=127.0.0.1, actor_id=b81f12d1d6e21e392a0034f701000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x155952390>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 130, in __init__\n",
      "    self.make_module()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 888, in make_module\n",
      "    self.module = module_spec.build()\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 606, in build\n",
      "    module = self.multi_rl_module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 126, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 470, in __init__\n",
      "    self.setup()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 144, in setup\n",
      "    self._rl_modules[module_id] = rl_module_spec.build()\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 98, in build\n",
      "    module = self.module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py\", line 24, in __init__\n",
      "    super().__init__(*args, **kwargs, catalog_class=catalog_class)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py\", line 50, in __init__\n",
      "    RLModule.__init__(self, *args, **kwargs)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 473, in __init__\n",
      "    raise (self._catalog_ctor_error or e)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 426, in __init__\n",
      "    self.catalog = catalog_class(\n",
      "                   ^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py\", line 74, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 122, in __init__\n",
      "    self._determine_components_hook()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 142, in _determine_components_hook\n",
      "    self._encoder_config = self._get_encoder_config(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 361, in _get_encoder_config\n",
      "    raise ValueError(\n",
      "ValueError: No default encoder config for obs space=<bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x15938dc40>>, lstm=False found.), taking actor 1 out of service.\n",
      "2025-10-07 13:42:31,792\tERROR actor_manager.py:873 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=26136, ip=127.0.0.1, actor_id=d9e7560a33c93eeca78e2c0b01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x1623532c0>)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/default_ppo_rl_module.py\", line 31, in setup\n",
      "    self.catalog.actor_critic_encoder_config.base_encoder_config,\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'actor_critic_encoder_config'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=26136, ip=127.0.0.1, actor_id=d9e7560a33c93eeca78e2c0b01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x1623532c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 130, in __init__\n",
      "    self.make_module()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 888, in make_module\n",
      "    self.module = module_spec.build()\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 606, in build\n",
      "    module = self.multi_rl_module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 126, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 470, in __init__\n",
      "    self.setup()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 144, in setup\n",
      "    self._rl_modules[module_id] = rl_module_spec.build()\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 98, in build\n",
      "    module = self.module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py\", line 24, in __init__\n",
      "    super().__init__(*args, **kwargs, catalog_class=catalog_class)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py\", line 50, in __init__\n",
      "    RLModule.__init__(self, *args, **kwargs)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 473, in __init__\n",
      "    raise (self._catalog_ctor_error or e)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 426, in __init__\n",
      "    self.catalog = catalog_class(\n",
      "                   ^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py\", line 74, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 122, in __init__\n",
      "    self._determine_components_hook()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 142, in _determine_components_hook\n",
      "    self._encoder_config = self._get_encoder_config(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 361, in _get_encoder_config\n",
      "    raise ValueError(\n",
      "ValueError: No default encoder config for obs space=<bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x16e5ee3f0>>, lstm=False found.), taking actor 2 out of service.\n",
      "2025-10-07 13:42:31,792\tERROR env_runner_group.py:758 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=26135, ip=127.0.0.1, actor_id=b81f12d1d6e21e392a0034f701000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x155952390>)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/default_ppo_rl_module.py\", line 31, in setup\n",
      "    self.catalog.actor_critic_encoder_config.base_encoder_config,\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'actor_critic_encoder_config'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=26135, ip=127.0.0.1, actor_id=b81f12d1d6e21e392a0034f701000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x155952390>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 130, in __init__\n",
      "    self.make_module()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 888, in make_module\n",
      "    self.module = module_spec.build()\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 606, in build\n",
      "    module = self.multi_rl_module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 126, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 470, in __init__\n",
      "    self.setup()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 144, in setup\n",
      "    self._rl_modules[module_id] = rl_module_spec.build()\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 98, in build\n",
      "    module = self.module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py\", line 24, in __init__\n",
      "    super().__init__(*args, **kwargs, catalog_class=catalog_class)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py\", line 50, in __init__\n",
      "    RLModule.__init__(self, *args, **kwargs)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 473, in __init__\n",
      "    raise (self._catalog_ctor_error or e)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 426, in __init__\n",
      "    self.catalog = catalog_class(\n",
      "                   ^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py\", line 74, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 122, in __init__\n",
      "    self._determine_components_hook()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 142, in _determine_components_hook\n",
      "    self._encoder_config = self._get_encoder_config(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 361, in _get_encoder_config\n",
      "    raise ValueError(\n",
      "ValueError: No default encoder config for obs space=<bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x15938dc40>>, lstm=False found.\n",
      "2025-10-07 13:42:31,792\tERROR env_runner_group.py:758 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=26136, ip=127.0.0.1, actor_id=d9e7560a33c93eeca78e2c0b01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x1623532c0>)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/default_ppo_rl_module.py\", line 31, in setup\n",
      "    self.catalog.actor_critic_encoder_config.base_encoder_config,\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'actor_critic_encoder_config'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=26136, ip=127.0.0.1, actor_id=d9e7560a33c93eeca78e2c0b01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x1623532c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 130, in __init__\n",
      "    self.make_module()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 888, in make_module\n",
      "    self.module = module_spec.build()\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 606, in build\n",
      "    module = self.multi_rl_module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 126, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 470, in __init__\n",
      "    self.setup()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 144, in setup\n",
      "    self._rl_modules[module_id] = rl_module_spec.build()\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 98, in build\n",
      "    module = self.module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py\", line 24, in __init__\n",
      "    super().__init__(*args, **kwargs, catalog_class=catalog_class)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py\", line 50, in __init__\n",
      "    RLModule.__init__(self, *args, **kwargs)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 473, in __init__\n",
      "    raise (self._catalog_ctor_error or e)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 426, in __init__\n",
      "    self.catalog = catalog_class(\n",
      "                   ^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py\", line 74, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 122, in __init__\n",
      "    self._determine_components_hook()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 142, in _determine_components_hook\n",
      "    self._encoder_config = self._get_encoder_config(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 361, in _get_encoder_config\n",
      "    raise ValueError(\n",
      "ValueError: No default encoder config for obs space=<bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x16e5ee3f0>>, lstm=False found.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     15\u001b[39m config = (\n\u001b[32m     16\u001b[39m     PPOConfig()\n\u001b[32m     17\u001b[39m     .environment(\u001b[33m\"\u001b[39m\u001b[33msimple_v3\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     )\n\u001b[32m     26\u001b[39m )\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Build the Algorithm.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m algo = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Train for one iteration, which is 2000 timesteps (1 train batch).\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(algo.train())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/deprecation.py:128\u001b[39m, in \u001b[36mDeprecated.<locals>._inner.<locals>._ctor\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m     deprecation_warning(\n\u001b[32m    122\u001b[39m         old=old \u001b[38;5;129;01mor\u001b[39;00m obj.\u001b[34m__name__\u001b[39m,\n\u001b[32m    123\u001b[39m         new=new,\n\u001b[32m    124\u001b[39m         help=help,\n\u001b[32m    125\u001b[39m         error=error,\n\u001b[32m    126\u001b[39m     )\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Call the deprecated method/function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm_config.py:5806\u001b[39m, in \u001b[36mAlgorithmConfig.build\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   5804\u001b[39m \u001b[38;5;129m@Deprecated\u001b[39m(new=\u001b[33m\"\u001b[39m\u001b[33mAlgorithmConfig.build_algo\u001b[39m\u001b[33m\"\u001b[39m, error=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   5805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m5806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm_config.py:1001\u001b[39m, in \u001b[36mAlgorithmConfig.build_algo\u001b[39m\u001b[34m(self, env, logger_creator, use_copy)\u001b[39m\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.algo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    999\u001b[39m     algo_class = get_trainable_cls(\u001b[38;5;28mself\u001b[39m.algo_class)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:536\u001b[39m, in \u001b[36mAlgorithm.__init__\u001b[39m\u001b[34m(self, config, env, logger_creator, **kwargs)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# Evaluation EnvRunnerGroup and metrics last returned by `self.evaluate()`.\u001b[39;00m\n\u001b[32m    534\u001b[39m \u001b[38;5;28mself\u001b[39m.eval_env_runner_group: Optional[EnvRunnerGroup] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/trainable/trainable.py:158\u001b[39m, in \u001b[36mTrainable.__init__\u001b[39m\u001b[34m(self, config, logger_creator, storage)\u001b[39m\n\u001b[32m    154\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._open_logfiles(stdout_file, stderr_file)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m setup_time = time.time() - \u001b[38;5;28mself\u001b[39m._start_time\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m setup_time > SETUP_TIME_THRESHOLD:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:644\u001b[39m, in \u001b[36mAlgorithm.setup\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    640\u001b[39m     \u001b[38;5;28mself\u001b[39m.offline_data = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_online \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.enable_env_runner_and_connector_v2:\n\u001b[32m    643\u001b[39m     \u001b[38;5;66;03m# Create a set of env runner actors via a EnvRunnerGroup.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     \u001b[38;5;28mself\u001b[39m.env_runner_group = \u001b[43mEnvRunnerGroup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# New API stack: User decides whether to create local env runner.\u001b[39;49;00m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Old API stack: Always create local EnvRunner.\u001b[39;49;00m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    653\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43menable_env_runner_and_connector_v2\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_local_env_runner\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtune_trial_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;66;03m# Compile, validate, and freeze an evaluation config.\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluation_config = \u001b[38;5;28mself\u001b[39m.config.get_evaluation_config_object()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py:198\u001b[39m, in \u001b[36mEnvRunnerGroup.__init__\u001b[39m\u001b[34m(self, env_creator, validate_env, default_policy_class, config, local_env_runner, logdir, _setup, tune_trial_id, pg_offset, num_env_runners, num_workers, local_worker)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _setup:\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m                \u001b[49m\u001b[43mnum_env_runners\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    204\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_env_runners\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     \u001b[38;5;66;03m# EnvRunnerGroup creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[32m    209\u001b[39m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[32m    210\u001b[39m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    212\u001b[39m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[32m    213\u001b[39m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[32m    214\u001b[39m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py:286\u001b[39m, in \u001b[36mEnvRunnerGroup._setup\u001b[39m\u001b[34m(self, validate_env, config, num_env_runners, local_env_runner)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;66;03m# If num_env_runners > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    281\u001b[39m     local_env_runner\n\u001b[32m    282\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._worker_manager.num_actors() > \u001b[32m0\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.create_env_on_local_worker\n\u001b[32m    284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config.observation_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.action_space)\n\u001b[32m    285\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     spaces = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    288\u001b[39m     spaces = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py:314\u001b[39m, in \u001b[36mEnvRunnerGroup.get_spaces\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# Get ID of the first remote worker.\u001b[39;00m\n\u001b[32m    308\u001b[39m remote_worker_ids = (\n\u001b[32m    309\u001b[39m     [\u001b[38;5;28mself\u001b[39m._worker_manager.actor_ids()[\u001b[32m0\u001b[39m]]\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._worker_manager.actor_ids()\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    312\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m spaces = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforeach_env_runner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv_runner\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    320\u001b[39m logger.info(\n\u001b[32m    321\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mInferred observation/action spaces from remote \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mworker (local worker has no env): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspaces\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m )\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m spaces\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from mpe2 import simple_v3\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "from supersuit import flatten_v0\n",
    "\n",
    "def make_env(env_config=None):\n",
    "    env = simple_v3.parallel_env(max_cycles=25, render_mode=None)  # parallel API is best for RLlib\n",
    "    return flatten_v0(env)\n",
    "\n",
    "# Register custom environment with Rllib\n",
    "register_env(\"simple_v3\", lambda config: PettingZooEnv(make_env(config))) # we also have to use the Pettingzoo wrapper\n",
    "\n",
    "# Configure.\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"simple_v3\")\n",
    "    .training(\n",
    "        train_batch_size_per_learner=2000, # num of steps per agent\n",
    "        lr=0.0004,\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\"shared_policy\": (None, make_env().observation_space, make_env().action_space, {})},\n",
    "        policy_mapping_fn=lambda agent_id, *args, **kwargs: \"shared_policy\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build the Algorithm.\n",
    "algo = config.build()\n",
    "\n",
    "# Train for one iteration, which is 2000 timesteps (1 train batch).\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d64b1e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-10-07 15:18:30</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:09.91        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.5/18.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/11 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  num_training_step_ca\n",
       "lls_per_iteration</th><th style=\"text-align: right;\">     num_env_steps_sample\n",
       "d_lifetime</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_1846d_00000</td><td>TERMINATED</td><td>127.0.0.1:34554</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.66131</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">4000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=34554)\u001b[0m 2025-10-07 15:18:23,945\tWARNING algorithm_config.py:5045 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(PPO pid=34554)\u001b[0m [2025-10-07 15:18:24,132 E 34554 5407408] core_worker.cc:2246: Actor with class name: 'SingleAgentEnvRunner' and ID: 'a63804794c79139412fe3b5201000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=34606)\u001b[0m 2025-10-07 15:18:27,201\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(PPO(env=CartPole-v1; env-runners=2; learners=0; multi-agent=False) pid=34554)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/paula/ray_results/PPO_2025-10-07_15-18-16/PPO_CartPole-v1_1846d_00000_0_2025-10-07_15-18-20/checkpoint_000000)\n",
      "2025-10-07 15:18:30,606\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/paula/ray_results/PPO_2025-10-07_15-18-16' in 0.0065s.\n",
      "2025-10-07 15:18:30,728\tINFO tune.py:1041 -- Total run time: 10.04 seconds (9.90 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# Configure.\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .training(\n",
    "        train_batch_size_per_learner=2000,\n",
    "        lr=0.0004,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train through Ray Tune.\n",
    "results = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config,\n",
    "    # Train for 4000 timesteps (2 iterations).\n",
    "    run_config=tune.RunConfig(stop={\"num_env_steps_sampled_lifetime\": 4000}),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac43ed83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performing trial's final set of hyperparameters:\n",
      "\n",
      "{'clip_param': 0.3, 'lr': 0.0004, 'minibatch_size': 128, 'num_epochs': 30}\n",
      "\n",
      "Best performing trial's final reported metrics:\n",
      "\n",
      "{'config': {'__stderr_file__': None,\n",
      "            '__stdout_file__': None,\n",
      "            '_disable_action_flattening': False,\n",
      "            '_disable_execution_plan_api': -1,\n",
      "            '_disable_initialize_loss_from_dummy_batch': False,\n",
      "            '_disable_preprocessor_api': False,\n",
      "            '_dont_auto_sync_env_runner_states': False,\n",
      "            '_enable_rl_module_api': -1,\n",
      "            '_env_to_module_connector': None,\n",
      "            '_fake_gpus': False,\n",
      "            '_is_atari': None,\n",
      "            '_is_online': True,\n",
      "            '_learner_class': None,\n",
      "            '_learner_connector': None,\n",
      "            '_model_config': {},\n",
      "            '_module_to_env_connector': None,\n",
      "            '_per_module_overrides': {},\n",
      "            '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
      "            '_rl_module_spec': None,\n",
      "            '_tf_policy_handles_more_than_one_loss': False,\n",
      "            '_torch_grad_scaler_class': None,\n",
      "            '_torch_lr_scheduler_classes': None,\n",
      "            '_train_batch_size_per_learner': 2000,\n",
      "            '_use_msgpack_checkpoints': False,\n",
      "            '_validate_config': True,\n",
      "            'action_mask_key': 'action_mask',\n",
      "            'action_space': None,\n",
      "            'actions_in_input_normalized': False,\n",
      "            'add_default_connectors_to_env_to_module_pipeline': True,\n",
      "            'add_default_connectors_to_learner_pipeline': True,\n",
      "            'add_default_connectors_to_module_to_env_pipeline': True,\n",
      "            'algorithm_config_overrides_per_module': {},\n",
      "            'always_attach_evaluation_results': -1,\n",
      "            'auto_wrap_old_gym_envs': -1,\n",
      "            'batch_mode': 'truncate_episodes',\n",
      "            'broadcast_env_runner_states': True,\n",
      "            'broadcast_offline_eval_runner_states': False,\n",
      "            'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>,\n",
      "            'callbacks_on_algorithm_init': None,\n",
      "            'callbacks_on_checkpoint_loaded': None,\n",
      "            'callbacks_on_env_runners_recreated': None,\n",
      "            'callbacks_on_environment_created': None,\n",
      "            'callbacks_on_episode_created': None,\n",
      "            'callbacks_on_episode_end': None,\n",
      "            'callbacks_on_episode_start': None,\n",
      "            'callbacks_on_episode_step': None,\n",
      "            'callbacks_on_evaluate_end': None,\n",
      "            'callbacks_on_evaluate_offline_end': None,\n",
      "            'callbacks_on_evaluate_offline_start': None,\n",
      "            'callbacks_on_evaluate_start': None,\n",
      "            'callbacks_on_offline_eval_runners_recreated': None,\n",
      "            'callbacks_on_sample_end': None,\n",
      "            'callbacks_on_train_result': None,\n",
      "            'checkpoint_trainable_policies_only': False,\n",
      "            'clip_actions': False,\n",
      "            'clip_param': 0.3,\n",
      "            'clip_rewards': None,\n",
      "            'compress_observations': False,\n",
      "            'count_steps_by': 'env_steps',\n",
      "            'create_env_on_driver': False,\n",
      "            'create_local_env_runner': True,\n",
      "            'custom_async_evaluation_function': -1,\n",
      "            'custom_eval_function': None,\n",
      "            'custom_resources_per_env_runner': {},\n",
      "            'custom_resources_per_offline_eval_runner': {},\n",
      "            'dataset_num_iters_per_eval_runner': 1,\n",
      "            'dataset_num_iters_per_learner': None,\n",
      "            'delay_between_env_runner_restarts_s': 60.0,\n",
      "            'disable_env_checking': False,\n",
      "            'eager_max_retraces': 20,\n",
      "            'eager_tracing': True,\n",
      "            'enable_async_evaluation': -1,\n",
      "            'enable_connectors': -1,\n",
      "            'enable_env_runner_and_connector_v2': True,\n",
      "            'enable_rl_module_and_learner': True,\n",
      "            'enable_tf1_exec_eagerly': False,\n",
      "            'entropy_coeff': 0.0,\n",
      "            'entropy_coeff_schedule': None,\n",
      "            'env': 'CartPole-v1',\n",
      "            'env_config': {},\n",
      "            'env_runner_cls': None,\n",
      "            'env_runner_health_probe_timeout_s': 30.0,\n",
      "            'env_runner_restore_timeout_s': 1800.0,\n",
      "            'env_task_fn': -1,\n",
      "            'episode_lookback_horizon': 1,\n",
      "            'episodes_to_numpy': True,\n",
      "            'evaluation_auto_duration_max_env_steps_per_sample': 2000,\n",
      "            'evaluation_auto_duration_min_env_steps_per_sample': 100,\n",
      "            'evaluation_config': None,\n",
      "            'evaluation_duration': 10,\n",
      "            'evaluation_duration_unit': 'episodes',\n",
      "            'evaluation_force_reset_envs_before_iteration': True,\n",
      "            'evaluation_interval': None,\n",
      "            'evaluation_num_env_runners': 0,\n",
      "            'evaluation_parallel_to_training': False,\n",
      "            'evaluation_sample_timeout_s': 120.0,\n",
      "            'exploration_config': {},\n",
      "            'explore': True,\n",
      "            'export_native_model_files': False,\n",
      "            'extra_python_environs_for_driver': {},\n",
      "            'extra_python_environs_for_worker': {},\n",
      "            'fake_sampler': False,\n",
      "            'framework': 'torch',\n",
      "            'gamma': 0.99,\n",
      "            'grad_clip': None,\n",
      "            'grad_clip_by': 'global_norm',\n",
      "            'gym_env_vectorize_mode': 'SYNC',\n",
      "            'ignore_env_runner_failures': False,\n",
      "            'ignore_final_observation': False,\n",
      "            'ignore_offline_eval_runner_failures': False,\n",
      "            'in_evaluation': False,\n",
      "            'input': 'sampler',\n",
      "            'input_compress_columns': ['obs', 'new_obs'],\n",
      "            'input_config': {},\n",
      "            'input_filesystem': None,\n",
      "            'input_filesystem_kwargs': {},\n",
      "            'input_read_batch_size': None,\n",
      "            'input_read_episodes': False,\n",
      "            'input_read_method': 'read_parquet',\n",
      "            'input_read_method_kwargs': {},\n",
      "            'input_read_sample_batches': False,\n",
      "            'input_read_schema': {},\n",
      "            'input_spaces_jsonable': True,\n",
      "            'iter_batches_kwargs': {},\n",
      "            'keep_per_episode_custom_metrics': False,\n",
      "            'kl_coeff': 0.2,\n",
      "            'kl_target': 0.01,\n",
      "            'lambda': 1.0,\n",
      "            'learner_config_dict': {},\n",
      "            'local_gpu_idx': 0,\n",
      "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                                      'intra_op_parallelism_threads': 8},\n",
      "            'log_gradients': True,\n",
      "            'log_level': 'WARN',\n",
      "            'log_sys_usage': True,\n",
      "            'logger_config': None,\n",
      "            'logger_creator': None,\n",
      "            'lr': 0.0004,\n",
      "            'lr_schedule': None,\n",
      "            'map_batches_kwargs': {},\n",
      "            'materialize_data': False,\n",
      "            'materialize_mapped_data': True,\n",
      "            'max_num_env_runner_restarts': 1000,\n",
      "            'max_num_offline_eval_runner_restarts': 1000,\n",
      "            'max_requests_in_flight_per_aggregator_actor': 3,\n",
      "            'max_requests_in_flight_per_env_runner': 1,\n",
      "            'max_requests_in_flight_per_learner': 3,\n",
      "            'max_requests_in_flight_per_offline_eval_runner': 1,\n",
      "            'merge_env_runner_states': 'training_only',\n",
      "            'metrics_episode_collection_timeout_s': 60.0,\n",
      "            'metrics_num_episodes_for_smoothing': 100,\n",
      "            'min_sample_timesteps_per_iteration': 0,\n",
      "            'min_time_s_per_iteration': None,\n",
      "            'min_train_timesteps_per_iteration': 0,\n",
      "            'minibatch_size': 128,\n",
      "            'model': {'_disable_action_flattening': False,\n",
      "                      '_disable_preprocessor_api': False,\n",
      "                      '_time_major': False,\n",
      "                      '_use_default_native_models': -1,\n",
      "                      'always_check_shapes': False,\n",
      "                      'attention_dim': 64,\n",
      "                      'attention_head_dim': 32,\n",
      "                      'attention_init_gru_gate_bias': 2.0,\n",
      "                      'attention_memory_inference': 50,\n",
      "                      'attention_memory_training': 50,\n",
      "                      'attention_num_heads': 1,\n",
      "                      'attention_num_transformer_units': 1,\n",
      "                      'attention_position_wise_mlp_dim': 32,\n",
      "                      'attention_use_n_prev_actions': 0,\n",
      "                      'attention_use_n_prev_rewards': 0,\n",
      "                      'conv_activation': 'relu',\n",
      "                      'conv_bias_initializer': None,\n",
      "                      'conv_bias_initializer_config': None,\n",
      "                      'conv_filters': None,\n",
      "                      'conv_kernel_initializer': None,\n",
      "                      'conv_kernel_initializer_config': None,\n",
      "                      'conv_transpose_bias_initializer': None,\n",
      "                      'conv_transpose_bias_initializer_config': None,\n",
      "                      'conv_transpose_kernel_initializer': None,\n",
      "                      'conv_transpose_kernel_initializer_config': None,\n",
      "                      'custom_action_dist': None,\n",
      "                      'custom_model': None,\n",
      "                      'custom_model_config': {},\n",
      "                      'custom_preprocessor': None,\n",
      "                      'dim': 84,\n",
      "                      'encoder_latent_dim': None,\n",
      "                      'fcnet_activation': 'tanh',\n",
      "                      'fcnet_bias_initializer': None,\n",
      "                      'fcnet_bias_initializer_config': None,\n",
      "                      'fcnet_hiddens': [256, 256],\n",
      "                      'fcnet_weights_initializer': None,\n",
      "                      'fcnet_weights_initializer_config': None,\n",
      "                      'framestack': True,\n",
      "                      'free_log_std': False,\n",
      "                      'grayscale': False,\n",
      "                      'log_std_clip_param': 20.0,\n",
      "                      'lstm_bias_initializer': None,\n",
      "                      'lstm_bias_initializer_config': None,\n",
      "                      'lstm_cell_size': 256,\n",
      "                      'lstm_use_prev_action': False,\n",
      "                      'lstm_use_prev_action_reward': -1,\n",
      "                      'lstm_use_prev_reward': False,\n",
      "                      'lstm_weights_initializer': None,\n",
      "                      'lstm_weights_initializer_config': None,\n",
      "                      'max_seq_len': 20,\n",
      "                      'no_final_linear': False,\n",
      "                      'post_fcnet_activation': 'relu',\n",
      "                      'post_fcnet_bias_initializer': None,\n",
      "                      'post_fcnet_bias_initializer_config': None,\n",
      "                      'post_fcnet_hiddens': [],\n",
      "                      'post_fcnet_weights_initializer': None,\n",
      "                      'post_fcnet_weights_initializer_config': None,\n",
      "                      'use_attention': False,\n",
      "                      'use_lstm': False,\n",
      "                      'vf_share_layers': False,\n",
      "                      'zero_mean': True},\n",
      "            'normalize_actions': True,\n",
      "            'num_aggregator_actors_per_learner': 0,\n",
      "            'num_consecutive_env_runner_failures_tolerance': 100,\n",
      "            'num_cpus_for_main_process': 1,\n",
      "            'num_cpus_per_env_runner': 1,\n",
      "            'num_cpus_per_learner': 'auto',\n",
      "            'num_cpus_per_offline_eval_runner': 1,\n",
      "            'num_env_runners': 2,\n",
      "            'num_envs_per_env_runner': 1,\n",
      "            'num_epochs': 30,\n",
      "            'num_gpus': 0,\n",
      "            'num_gpus_per_env_runner': 0,\n",
      "            'num_gpus_per_learner': 0,\n",
      "            'num_gpus_per_offline_eval_runner': 0,\n",
      "            'num_learners': 0,\n",
      "            'num_offline_eval_runners': 0,\n",
      "            'observation_filter': 'NoFilter',\n",
      "            'observation_fn': None,\n",
      "            'observation_space': None,\n",
      "            'off_policy_estimation_methods': {},\n",
      "            'offline_data_class': None,\n",
      "            'offline_eval_batch_size_per_runner': 256,\n",
      "            'offline_eval_rl_module_inference_only': False,\n",
      "            'offline_eval_runner_class': None,\n",
      "            'offline_eval_runner_health_probe_timeout_s': 30.0,\n",
      "            'offline_eval_runner_restore_timeout_s': 1800.0,\n",
      "            'offline_evaluation_duration': 1,\n",
      "            'offline_evaluation_interval': None,\n",
      "            'offline_evaluation_parallel_to_training': False,\n",
      "            'offline_evaluation_timeout_s': 120.0,\n",
      "            'offline_evaluation_type': None,\n",
      "            'offline_loss_for_module_fn': None,\n",
      "            'offline_sampling': False,\n",
      "            'ope_split_batch_by_episode': True,\n",
      "            'optimizer': {},\n",
      "            'output': None,\n",
      "            'output_compress_columns': ['obs', 'new_obs'],\n",
      "            'output_config': {},\n",
      "            'output_filesystem': None,\n",
      "            'output_filesystem_kwargs': {},\n",
      "            'output_max_file_size': 67108864,\n",
      "            'output_max_rows_per_file': None,\n",
      "            'output_write_episodes': True,\n",
      "            'output_write_method': 'write_parquet',\n",
      "            'output_write_method_kwargs': {},\n",
      "            'output_write_remaining_data': False,\n",
      "            'placement_strategy': 'PACK',\n",
      "            'policies': {'default_policy': (None, None, None, None)},\n",
      "            'policies_to_train': None,\n",
      "            'policy_map_cache': -1,\n",
      "            'policy_map_capacity': 100,\n",
      "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x30c88c720>,\n",
      "            'policy_states_are_swappable': False,\n",
      "            'postprocess_inputs': False,\n",
      "            'prelearner_buffer_class': None,\n",
      "            'prelearner_buffer_kwargs': {},\n",
      "            'prelearner_class': None,\n",
      "            'prelearner_module_synch_period': 10,\n",
      "            'preprocessor_pref': 'deepmind',\n",
      "            'remote_env_batch_wait_ms': 0,\n",
      "            'remote_worker_envs': False,\n",
      "            'render_env': False,\n",
      "            'replay_sequence_length': None,\n",
      "            'restart_failed_env_runners': True,\n",
      "            'restart_failed_offline_eval_runners': True,\n",
      "            'restart_failed_sub_environments': False,\n",
      "            'rollout_fragment_length': 'auto',\n",
      "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      "            'sample_timeout_s': 60.0,\n",
      "            'sampler_perf_stats_ema_coef': None,\n",
      "            'seed': None,\n",
      "            'sgd_minibatch_size': -1,\n",
      "            'shuffle_batch_per_epoch': True,\n",
      "            'shuffle_buffer_size': 0,\n",
      "            'simple_optimizer': False,\n",
      "            'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
      "            'synchronize_filters': -1,\n",
      "            'tf_session_args': {'allow_soft_placement': True,\n",
      "                                'device_count': {'CPU': 1},\n",
      "                                'gpu_options': {'allow_growth': True},\n",
      "                                'inter_op_parallelism_threads': 2,\n",
      "                                'intra_op_parallelism_threads': 2,\n",
      "                                'log_device_placement': False},\n",
      "            'torch_compile_learner': False,\n",
      "            'torch_compile_learner_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_learner_dynamo_mode': None,\n",
      "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
      "            'torch_compile_worker': False,\n",
      "            'torch_compile_worker_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_worker_dynamo_mode': None,\n",
      "            'torch_ddp_kwargs': {},\n",
      "            'torch_skip_nan_gradients': False,\n",
      "            'train_batch_size': 4000,\n",
      "            'update_worker_filter_stats': True,\n",
      "            'use_critic': True,\n",
      "            'use_gae': True,\n",
      "            'use_kl_loss': True,\n",
      "            'use_worker_filter_stats': True,\n",
      "            'validate_env_runners_after_construction': True,\n",
      "            'validate_offline_eval_runners_after_construction': True,\n",
      "            'vf_clip_param': 10.0,\n",
      "            'vf_loss_coeff': 1.0,\n",
      "            'vf_share_layers': -1,\n",
      "            'worker_cls': -1},\n",
      " 'date': '2025-10-07_15-18-30',\n",
      " 'done': True,\n",
      " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
      " 'env_runners': {'agent_episode_return_mean': {'default_agent': 32.4},\n",
      "                 'env_reset_timer': 0.0001806660002330318,\n",
      "                 'env_step_timer': 1.9199410687790467e-05,\n",
      "                 'env_to_module_connector': {'connector_pipeline_timer': 4.65685692193089e-05,\n",
      "                                             'timers': {'connectors': {'add_observations_from_episodes_to_batch': 2.939154353315449e-06,\n",
      "                                                                       'add_states_from_episodes_to_batch': 8.73828385034704e-07,\n",
      "                                                                       'add_time_dim_to_batch_and_zero_pad': 1.071824325444134e-06,\n",
      "                                                                       'batch_individual_items': 6.64464394221726e-06,\n",
      "                                                                       'numpy_to_tensor': 1.0023616749300623e-05}}},\n",
      "                 'env_to_module_sum_episodes_length_in': 26.02632457375219,\n",
      "                 'env_to_module_sum_episodes_length_out': 26.02632457375219,\n",
      "                 'episode_duration_sec_mean': 0.008645496720273513,\n",
      "                 'episode_len_max': 107,\n",
      "                 'episode_len_mean': 32.4,\n",
      "                 'episode_len_min': 10,\n",
      "                 'episode_return_max': 107.0,\n",
      "                 'episode_return_mean': 32.4,\n",
      "                 'episode_return_min': 10.0,\n",
      "                 'module_episode_return_mean': {'default_policy': 32.4},\n",
      "                 'module_to_env_connector': {'connector_pipeline_timer': 0.00012192296849072568,\n",
      "                                             'timers': {'connectors': {'get_actions': 4.5050584678381405e-05,\n",
      "                                                                       'listify_data_for_vector_env': 1.1319425371004265e-05,\n",
      "                                                                       'normalize_and_clip_actions': 1.0374122779442235e-05,\n",
      "                                                                       'remove_single_ts_time_rank_from_batch': 7.936262877014586e-07,\n",
      "                                                                       'tensor_to_numpy': 1.679585178959098e-05,\n",
      "                                                                       'un_batch_to_individual_items': 6.506766581113947e-06}}},\n",
      "                 'num_agent_steps_sampled': {'default_agent': 2000.0},\n",
      "                 'num_agent_steps_sampled_lifetime': {'default_agent': 4000.0},\n",
      "                 'num_env_steps_sampled': 2000.0,\n",
      "                 'num_env_steps_sampled_lifetime': 4000.0,\n",
      "                 'num_env_steps_sampled_lifetime_throughput': 2962.3283596205515,\n",
      "                 'num_episodes': 62.0,\n",
      "                 'num_episodes_lifetime': 160.0,\n",
      "                 'num_module_steps_sampled': {'default_policy': 2000.0},\n",
      "                 'num_module_steps_sampled_lifetime': {'default_policy': 4000.0},\n",
      "                 'rlmodule_inference_timer': 3.5502372579155956e-05,\n",
      "                 'sample': 0.29259307887354224,\n",
      "                 'time_between_sampling': 1.070853604003787,\n",
      "                 'weights_seq_no': 1.0},\n",
      " 'experiment_tag': '0',\n",
      " 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0},\n",
      " 'hostname': 'scadsdp25.misc.intern.uni-leipzig.de',\n",
      " 'iterations_since_restore': 2,\n",
      " 'learners': {'__all_modules__': {'learner_connector': {'connector_pipeline_timer': 0.04630796815268695,\n",
      "                                                        'timers': {'connectors': {'add_columns_from_episodes_to_train_batch': 0.017850233660574304,\n",
      "                                                                                  'add_observations_from_episodes_to_batch': 0.00045830841409042475,\n",
      "                                                                                  'add_one_ts_to_episodes_and_truncate': 0.00406241884935298,\n",
      "                                                                                  'add_states_from_episodes_to_batch': 7.055919268168508e-06,\n",
      "                                                                                  'add_time_dim_to_batch_and_zero_pad': 2.5116578908637165e-05,\n",
      "                                                                                  'batch_individual_items': 0.015591761490359204,\n",
      "                                                                                  'general_advantage_estimation': 0.007913540496811039,\n",
      "                                                                                  'numpy_to_tensor': 0.00013972792468848637}}},\n",
      "                                  'learner_connector_sum_episodes_length_in': 2000.0,\n",
      "                                  'learner_connector_sum_episodes_length_out': 2099.64,\n",
      "                                  'num_env_steps_trained': 998976,\n",
      "                                  'num_env_steps_trained_lifetime': 2034276,\n",
      "                                  'num_env_steps_trained_lifetime_throughput': 1074378.308506909,\n",
      "                                  'num_module_steps_trained': 61952,\n",
      "                                  'num_module_steps_trained_lifetime': 125056,\n",
      "                                  'num_module_steps_trained_lifetime_throughput': 65544.25374707916,\n",
      "                                  'num_module_steps_trained_throughput': 65545.37395397507,\n",
      "                                  'num_non_trainable_parameters': 0,\n",
      "                                  'num_trainable_parameters': 134915},\n",
      "              'default_policy': {'curr_entropy_coeff': 0.0,\n",
      "                                 'curr_kl_coeff': 0.30000001192092896,\n",
      "                                 'default_optimizer_learning_rate': 0.0004,\n",
      "                                 'diff_num_grad_updates_vs_sampler_policy': 1.0,\n",
      "                                 'entropy': 0.6219468,\n",
      "                                 'gradients_default_optimizer_global_norm': 2.3400145,\n",
      "                                 'mean_kl_loss': 0.03174403,\n",
      "                                 'module_train_batch_size_mean': 128.0,\n",
      "                                 'num_module_steps_trained': 61952,\n",
      "                                 'num_module_steps_trained_lifetime': 125056,\n",
      "                                 'num_module_steps_trained_lifetime_throughput': 65546.60766929487,\n",
      "                                 'num_trainable_parameters': 134915,\n",
      "                                 'policy_loss': -0.047003586,\n",
      "                                 'total_loss': 6.9056454,\n",
      "                                 'vf_explained_var': 0.22462672,\n",
      "                                 'vf_loss': 6.946301,\n",
      "                                 'vf_loss_unclipped': 211.32085,\n",
      "                                 'weights_seq_no': 2.0}},\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_env_steps_sampled_lifetime': 4000.0,\n",
      " 'num_training_step_calls_per_iteration': 1,\n",
      " 'perf': {'cpu_util_percent': 20.1, 'ram_util_percent': 69.3},\n",
      " 'pid': 34554,\n",
      " 'time_since_restore': 2.661314010620117,\n",
      " 'time_this_iter_s': 1.3138329982757568,\n",
      " 'time_total_s': 2.661314010620117,\n",
      " 'timers': {'env_runner_sampling_timer': 0.30800753282106597,\n",
      "            'learner_update_timer': 1.032305712911475,\n",
      "            'restore_env_runners': 3.182168074999936e-05,\n",
      "            'synch_env_connectors': 0.003312208005809225,\n",
      "            'synch_weights': 0.0013184479314077178,\n",
      "            'training_iteration': 1.3424651132420695,\n",
      "            'training_step': 1.342267293818004},\n",
      " 'timestamp': 1759843110,\n",
      " 'training_iteration': 2,\n",
      " 'trial_id': '1846d_00000'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "best_result = results.get_best_result()\n",
    "hyperparam_mutations = [\"clip_param\", \"lr\", \"num_epochs\", \"minibatch_size\", \"train_batch_size_per_learner\"]\n",
    "\n",
    "print(\"Best performing trial's final set of hyperparameters:\\n\")\n",
    "pprint.pprint(\n",
    "    {k: v for k, v in best_result.config.items()if k in hyperparam_mutations}\n",
    ")\n",
    "\n",
    "print(\"\\nBest performing trial's final reported metrics:\\n\")\n",
    "\n",
    "metrics_to_print = [\n",
    "    \"episode_reward_mean\",\n",
    "    \"episode_reward_max\",\n",
    "    \"episode_reward_min\",\n",
    "    \"episode_len_mean\",\n",
    "]\n",
    "pprint.pprint({k: v for k, v in best_result.metrics.items()}) #if k in metrics_to_print})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "part-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
