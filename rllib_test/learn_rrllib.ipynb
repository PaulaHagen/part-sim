{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17afb024",
   "metadata": {},
   "source": [
    "# Ray Overview Tutorials: RLLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27e29f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 15:56:39,160\tWARNING algorithm_config.py:5074 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the PPO configuration...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     86\u001b[39m config = (\n\u001b[32m     87\u001b[39m     PPOConfig().environment(\n\u001b[32m     88\u001b[39m         \u001b[38;5;66;03m# Env class to use (our custom gymnasium environment).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m     .training(model={\u001b[33m\"\u001b[39m\u001b[33mfcnet_hiddens\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m64\u001b[39m, \u001b[32m64\u001b[39m]})\n\u001b[32m     98\u001b[39m )\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Construct the actual PPO algorithm object from the config.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m algo = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m rl_module = algo.get_module()\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Train for n iterations and report results (mean episode rewards).\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Optimal reward calculation:\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# - Need at least 19 steps to reach the goal (from position 0 to 19)\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# - Each step (except last) gets -0.1 reward: 18 * (-0.1) = -1.8\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# - Final step gets +1.0 reward\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# - Total optimal reward: -1.8 + 1.0 = -0.8\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm_config.py:1001\u001b[39m, in \u001b[36mAlgorithmConfig.build_algo\u001b[39m\u001b[34m(self, env, logger_creator, use_copy)\u001b[39m\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.algo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    999\u001b[39m     algo_class = get_trainable_cls(\u001b[38;5;28mself\u001b[39m.algo_class)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:536\u001b[39m, in \u001b[36mAlgorithm.__init__\u001b[39m\u001b[34m(self, config, env, logger_creator, **kwargs)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# Evaluation EnvRunnerGroup and metrics last returned by `self.evaluate()`.\u001b[39;00m\n\u001b[32m    534\u001b[39m \u001b[38;5;28mself\u001b[39m.eval_env_runner_group: Optional[EnvRunnerGroup] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/trainable/trainable.py:158\u001b[39m, in \u001b[36mTrainable.__init__\u001b[39m\u001b[34m(self, config, logger_creator, storage)\u001b[39m\n\u001b[32m    154\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._open_logfiles(stdout_file, stderr_file)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m setup_time = time.time() - \u001b[38;5;28mself\u001b[39m._start_time\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m setup_time > SETUP_TIME_THRESHOLD:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:644\u001b[39m, in \u001b[36mAlgorithm.setup\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    640\u001b[39m     \u001b[38;5;28mself\u001b[39m.offline_data = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_online \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.enable_env_runner_and_connector_v2:\n\u001b[32m    643\u001b[39m     \u001b[38;5;66;03m# Create a set of env runner actors via a EnvRunnerGroup.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     \u001b[38;5;28mself\u001b[39m.env_runner_group = \u001b[43mEnvRunnerGroup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# New API stack: User decides whether to create local env runner.\u001b[39;49;00m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Old API stack: Always create local EnvRunner.\u001b[39;49;00m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    653\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43menable_env_runner_and_connector_v2\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_local_env_runner\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtune_trial_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;66;03m# Compile, validate, and freeze an evaluation config.\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluation_config = \u001b[38;5;28mself\u001b[39m.config.get_evaluation_config_object()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py:198\u001b[39m, in \u001b[36mEnvRunnerGroup.__init__\u001b[39m\u001b[34m(self, env_creator, validate_env, default_policy_class, config, local_env_runner, logdir, _setup, tune_trial_id, pg_offset, num_env_runners, num_workers, local_worker)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _setup:\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m                \u001b[49m\u001b[43mnum_env_runners\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    204\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_env_runners\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     \u001b[38;5;66;03m# EnvRunnerGroup creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[32m    209\u001b[39m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[32m    210\u001b[39m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    212\u001b[39m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[32m    213\u001b[39m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[32m    214\u001b[39m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py:272\u001b[39m, in \u001b[36mEnvRunnerGroup._setup\u001b[39m\u001b[34m(self, validate_env, config, num_env_runners, local_env_runner)\u001b[39m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m._ds_shards = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_env_runners_after_construction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[38;5;66;03m# If num_env_runners > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    281\u001b[39m     local_env_runner\n\u001b[32m    282\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._worker_manager.num_actors() > \u001b[32m0\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.create_env_on_local_worker\n\u001b[32m    284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config.observation_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.action_space)\n\u001b[32m    285\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py:750\u001b[39m, in \u001b[36mEnvRunnerGroup.add_workers\u001b[39m\u001b[34m(self, num_workers, validate)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Validate here, whether all remote workers have been constructed properly\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;66;03m# and are \"up and running\". Establish initial states.\u001b[39;00m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_worker_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43massert_healthy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# Simiply raise the error, which will get handled by the try-except\u001b[39;00m\n\u001b[32m    754\u001b[39m         \u001b[38;5;66;03m# clause around the _setup().\u001b[39;00m\n\u001b[32m    755\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result.ok:\n\u001b[32m    756\u001b[39m             e = result.get()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py:461\u001b[39m, in \u001b[36mFaultTolerantActorManager.foreach_actor\u001b[39m\u001b[34m(self, func, kwargs, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[39m\n\u001b[32m    454\u001b[39m remote_calls = \u001b[38;5;28mself\u001b[39m._call_actors(\n\u001b[32m    455\u001b[39m     func=func,\n\u001b[32m    456\u001b[39m     kwargs=kwargs,\n\u001b[32m    457\u001b[39m     remote_actor_ids=remote_actor_ids,\n\u001b[32m    458\u001b[39m )\n\u001b[32m    460\u001b[39m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m _, remote_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py:839\u001b[39m, in \u001b[36mFaultTolerantActorManager._fetch_result\u001b[39m\u001b[34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m readies, _ = \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[32m    848\u001b[39m remote_results = RemoteCallResults()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22\u001b[39m, in \u001b[36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauto_init_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     21\u001b[39m     auto_init_ray()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33minit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/_private/worker.py:3113\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[39m\n\u001b[32m   3111\u001b[39m timeout = timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m10\u001b[39m**\u001b[32m6\u001b[39m\n\u001b[32m   3112\u001b[39m timeout_milliseconds = \u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3113\u001b[39m ready_ids, remaining_ids = \u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore_worker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3118\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpython/ray/_raylet.pyx:3486\u001b[39m, in \u001b[36mray._raylet.CoreWorker.wait\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpython/ray/includes/common.pxi:96\u001b[39m, in \u001b[36mray._raylet.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict, Tuple, Any, Optional\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "\n",
    "# Define your problem using python and Farama-Foundation's gymnasium API:\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Corridor environment where an agent must learn to move right to reach the exit.\n",
    "\n",
    "    ---------------------\n",
    "    | S | 1 | 2 | 3 | G |   S=start; G=goal; corridor_length=5\n",
    "    ---------------------\n",
    "\n",
    "    Actions:\n",
    "        0: Move left\n",
    "        1: Move right\n",
    "\n",
    "    Observations:\n",
    "        A single float representing the agent's current position (index)\n",
    "        starting at 0.0 and ending at corridor_length\n",
    "\n",
    "    Rewards:\n",
    "        -0.1 for each step\n",
    "        +1.0 when reaching the goal\n",
    "\n",
    "    Episode termination:\n",
    "        When the agent reaches the goal (position >= corridor_length)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0.0\n",
    "        self.action_space = gym.spaces.Discrete(2)  # 0=left, 1=right\n",
    "        self.observation_space = gym.spaces.Box(0.0, self.end_pos, (1,), np.float32)\n",
    "\n",
    "    def reset(\n",
    "        self, *, seed: Optional[int] = None, options: Optional[Dict] = None\n",
    "    ) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Reset the environment for a new episode.\n",
    "\n",
    "        Args:\n",
    "            seed: Random seed for reproducibility\n",
    "            options: Additional options (not used in this environment)\n",
    "\n",
    "        Returns:\n",
    "            Initial observation of the new episode and an info dict.\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)  # Initialize RNG if seed is provided\n",
    "        self.cur_pos = 0.0\n",
    "        # Return initial observation.\n",
    "        return np.array([self.cur_pos], np.float32), {}\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        \"\"\"Take a single step in the environment based on the provided action.\n",
    "\n",
    "        Args:\n",
    "            action: 0 for left, 1 for right\n",
    "\n",
    "        Returns:\n",
    "            A tuple of (observation, reward, terminated, truncated, info):\n",
    "                observation: Agent's new position\n",
    "                reward: Reward from taking the action (-0.1 or +1.0)\n",
    "                terminated: Whether episode is done (reached goal)\n",
    "                truncated: Whether episode was truncated (always False here)\n",
    "                info: Additional information (empty dict)\n",
    "        \"\"\"\n",
    "        # Walk left if action is 0 and we're not at the leftmost position\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        # Walk right if action is 1\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "        # Set `terminated` flag when end of corridor (goal) reached.\n",
    "        terminated = self.cur_pos >= self.end_pos\n",
    "        truncated = False\n",
    "        # +1 when goal reached, otherwise -0.1.\n",
    "        reward = 1.0 if terminated else -0.1\n",
    "        return np.array([self.cur_pos], np.float32), reward, terminated, truncated, {}\n",
    "\n",
    "\n",
    "# Create an RLlib Algorithm instance from a PPOConfig object.\n",
    "print(\"Setting up the PPO configuration...\")\n",
    "config = (\n",
    "    PPOConfig().environment(\n",
    "        # Env class to use (our custom gymnasium environment).\n",
    "        SimpleCorridor,\n",
    "        # Config dict passed to our custom env's constructor.\n",
    "        # Use corridor with 20 fields (including start and goal).\n",
    "        env_config={\"corridor_length\": 20},\n",
    "    )\n",
    "    # Parallelize environment rollouts for faster training.\n",
    "    .env_runners(num_env_runners=3)\n",
    "    # Use a smaller network for this simple task\n",
    "    .training(model={\"fcnet_hiddens\": [64, 64]})\n",
    ")\n",
    "\n",
    "# Construct the actual PPO algorithm object from the config.\n",
    "algo = config.build_algo()\n",
    "rl_module = algo.get_module()\n",
    "\n",
    "# Train for n iterations and report results (mean episode rewards).\n",
    "# Optimal reward calculation:\n",
    "# - Need at least 19 steps to reach the goal (from position 0 to 19)\n",
    "# - Each step (except last) gets -0.1 reward: 18 * (-0.1) = -1.8\n",
    "# - Final step gets +1.0 reward\n",
    "# - Total optimal reward: -1.8 + 1.0 = -0.8\n",
    "print(\"\\nStarting training loop...\")\n",
    "for i in range(5):\n",
    "    results = algo.train()\n",
    "\n",
    "    # Log the metrics from training results\n",
    "    print(f\"Iteration {i+1}\")\n",
    "    print(f\"  Training metrics: {results['env_runners']}\")\n",
    "\n",
    "# Save the trained algorithm (optional)\n",
    "checkpoint_dir = algo.save()\n",
    "print(f\"\\nSaved model checkpoint to: {checkpoint_dir}\")\n",
    "\n",
    "print(\"\\nRunning inference with the trained policy...\")\n",
    "# Create a test environment with a shorter corridor to verify the agent's behavior\n",
    "env = SimpleCorridor({\"corridor_length\": 10})\n",
    "# Get the initial observation (should be: [0.0] for the starting position).\n",
    "obs, info = env.reset()\n",
    "terminated = truncated = False\n",
    "total_reward = 0.0\n",
    "step_count = 0\n",
    "\n",
    "# Play one episode and track the agent's trajectory\n",
    "print(\"\\nAgent trajectory:\")\n",
    "positions = [float(obs[0])]  # Track positions for visualization\n",
    "\n",
    "while not terminated and not truncated and step_count < 1000:\n",
    "    # Compute an action given the current observation\n",
    "    action_logits = rl_module.forward_inference(\n",
    "        {\"obs\": torch.from_numpy(obs).unsqueeze(0)}\n",
    "    )[\"action_dist_inputs\"].numpy()[\n",
    "        0\n",
    "    ]  # [0]: Batch dimension=1\n",
    "\n",
    "    # Get the action with highest probability\n",
    "    action = np.argmax(action_logits)\n",
    "\n",
    "    # Log the agent's decision\n",
    "    action_name = \"LEFT\" if action == 0 else \"RIGHT\"\n",
    "    print(f\"  Step {step_count}: Position {obs[0]:.1f}, Action: {action_name}\")\n",
    "\n",
    "    # Apply the computed action in the environment\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    positions.append(float(obs[0]))\n",
    "\n",
    "    # Sum up rewards\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "\n",
    "# Report final results\n",
    "print(f\"\\nEpisode complete:\")\n",
    "print(f\"  Steps taken: {step_count}\")\n",
    "print(f\"  Total reward: {total_reward:.2f}\")\n",
    "print(f\"  Final position: {obs[0]:.1f}\")\n",
    "\n",
    "# Verify the agent has learned the optimal policy\n",
    "if total_reward > -0.5 and obs[0] >= 9.0:\n",
    "    print(\"  Success! The agent has learned the optimal policy (always move right).\")\n",
    "else:\n",
    "    print(\"  Failure! The agent didn't reach the goal within 1000 timesteps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d4f82",
   "metadata": {},
   "source": [
    "# RLLib Getting Started "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db809642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# Create a config instance for the PPO algorithm.\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"Pendulum-v1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3e81d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x312ca0f80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.training(\n",
    "    lr=0.0002,\n",
    "    train_batch_size_per_learner=2000,\n",
    "    num_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58bfe3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 15:48:36,918\tWARNING algorithm_config.py:5045 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-10-06 15:48:38,225\tINFO worker.py:1951 -- Started a local Ray instance.\n",
      "[2025-10-06 15:48:40,053 E 29591 995670] core_worker.cc:2246: Actor with class name: 'SingleAgentEnvRunner' and ID: '949fa719064dcafb83332ba501000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "2025-10-06 15:48:42,001\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29625)\u001b[0m 2025-10-06 15:48:41,939\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-10-06 15:48:42,635\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': {'_disable_action_flattening': False,\n",
      "            '_disable_execution_plan_api': -1,\n",
      "            '_disable_initialize_loss_from_dummy_batch': False,\n",
      "            '_disable_preprocessor_api': False,\n",
      "            '_dont_auto_sync_env_runner_states': False,\n",
      "            '_enable_rl_module_api': -1,\n",
      "            '_env_to_module_connector': None,\n",
      "            '_fake_gpus': False,\n",
      "            '_is_atari': None,\n",
      "            '_is_online': True,\n",
      "            '_learner_class': None,\n",
      "            '_learner_connector': None,\n",
      "            '_model_config': {},\n",
      "            '_module_to_env_connector': None,\n",
      "            '_per_module_overrides': {},\n",
      "            '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
      "            '_rl_module_spec': None,\n",
      "            '_tf_policy_handles_more_than_one_loss': False,\n",
      "            '_torch_grad_scaler_class': None,\n",
      "            '_torch_lr_scheduler_classes': None,\n",
      "            '_train_batch_size_per_learner': 2000,\n",
      "            '_use_msgpack_checkpoints': False,\n",
      "            '_validate_config': True,\n",
      "            'action_mask_key': 'action_mask',\n",
      "            'action_space': None,\n",
      "            'actions_in_input_normalized': False,\n",
      "            'add_default_connectors_to_env_to_module_pipeline': True,\n",
      "            'add_default_connectors_to_learner_pipeline': True,\n",
      "            'add_default_connectors_to_module_to_env_pipeline': True,\n",
      "            'algorithm_config_overrides_per_module': {},\n",
      "            'always_attach_evaluation_results': -1,\n",
      "            'auto_wrap_old_gym_envs': -1,\n",
      "            'batch_mode': 'truncate_episodes',\n",
      "            'broadcast_env_runner_states': True,\n",
      "            'broadcast_offline_eval_runner_states': False,\n",
      "            'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>,\n",
      "            'callbacks_on_algorithm_init': None,\n",
      "            'callbacks_on_checkpoint_loaded': None,\n",
      "            'callbacks_on_env_runners_recreated': None,\n",
      "            'callbacks_on_environment_created': None,\n",
      "            'callbacks_on_episode_created': None,\n",
      "            'callbacks_on_episode_end': None,\n",
      "            'callbacks_on_episode_start': None,\n",
      "            'callbacks_on_episode_step': None,\n",
      "            'callbacks_on_evaluate_end': None,\n",
      "            'callbacks_on_evaluate_offline_end': None,\n",
      "            'callbacks_on_evaluate_offline_start': None,\n",
      "            'callbacks_on_evaluate_start': None,\n",
      "            'callbacks_on_offline_eval_runners_recreated': None,\n",
      "            'callbacks_on_sample_end': None,\n",
      "            'callbacks_on_train_result': None,\n",
      "            'checkpoint_trainable_policies_only': False,\n",
      "            'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>,\n",
      "            'clip_actions': False,\n",
      "            'clip_param': 0.3,\n",
      "            'clip_rewards': None,\n",
      "            'compress_observations': False,\n",
      "            'count_steps_by': 'env_steps',\n",
      "            'create_env_on_driver': False,\n",
      "            'create_local_env_runner': True,\n",
      "            'custom_async_evaluation_function': -1,\n",
      "            'custom_eval_function': None,\n",
      "            'custom_resources_per_env_runner': {},\n",
      "            'custom_resources_per_offline_eval_runner': {},\n",
      "            'dataset_num_iters_per_eval_runner': 1,\n",
      "            'dataset_num_iters_per_learner': None,\n",
      "            'delay_between_env_runner_restarts_s': 60.0,\n",
      "            'disable_env_checking': False,\n",
      "            'eager_max_retraces': 20,\n",
      "            'eager_tracing': True,\n",
      "            'enable_async_evaluation': -1,\n",
      "            'enable_connectors': -1,\n",
      "            'enable_env_runner_and_connector_v2': True,\n",
      "            'enable_rl_module_and_learner': True,\n",
      "            'enable_tf1_exec_eagerly': False,\n",
      "            'entropy_coeff': 0.0,\n",
      "            'entropy_coeff_schedule': None,\n",
      "            'env': 'Pendulum-v1',\n",
      "            'env_config': {},\n",
      "            'env_runner_cls': None,\n",
      "            'env_runner_health_probe_timeout_s': 30.0,\n",
      "            'env_runner_restore_timeout_s': 1800.0,\n",
      "            'env_task_fn': -1,\n",
      "            'episode_lookback_horizon': 1,\n",
      "            'episodes_to_numpy': True,\n",
      "            'evaluation_auto_duration_max_env_steps_per_sample': 2000,\n",
      "            'evaluation_auto_duration_min_env_steps_per_sample': 100,\n",
      "            'evaluation_config': None,\n",
      "            'evaluation_duration': 10,\n",
      "            'evaluation_duration_unit': 'episodes',\n",
      "            'evaluation_force_reset_envs_before_iteration': True,\n",
      "            'evaluation_interval': None,\n",
      "            'evaluation_num_env_runners': 0,\n",
      "            'evaluation_parallel_to_training': False,\n",
      "            'evaluation_sample_timeout_s': 120.0,\n",
      "            'exploration_config': {},\n",
      "            'explore': True,\n",
      "            'export_native_model_files': False,\n",
      "            'extra_python_environs_for_driver': {},\n",
      "            'extra_python_environs_for_worker': {},\n",
      "            'fake_sampler': False,\n",
      "            'framework': 'torch',\n",
      "            'gamma': 0.99,\n",
      "            'grad_clip': None,\n",
      "            'grad_clip_by': 'global_norm',\n",
      "            'gym_env_vectorize_mode': 'SYNC',\n",
      "            'ignore_env_runner_failures': False,\n",
      "            'ignore_final_observation': False,\n",
      "            'ignore_offline_eval_runner_failures': False,\n",
      "            'in_evaluation': False,\n",
      "            'input': 'sampler',\n",
      "            'input_compress_columns': ['obs', 'new_obs'],\n",
      "            'input_config': {},\n",
      "            'input_filesystem': None,\n",
      "            'input_filesystem_kwargs': {},\n",
      "            'input_read_batch_size': None,\n",
      "            'input_read_episodes': False,\n",
      "            'input_read_method': 'read_parquet',\n",
      "            'input_read_method_kwargs': {},\n",
      "            'input_read_sample_batches': False,\n",
      "            'input_read_schema': {},\n",
      "            'input_spaces_jsonable': True,\n",
      "            'iter_batches_kwargs': {},\n",
      "            'keep_per_episode_custom_metrics': False,\n",
      "            'kl_coeff': 0.2,\n",
      "            'kl_target': 0.01,\n",
      "            'lambda': 1.0,\n",
      "            'learner_config_dict': {},\n",
      "            'local_gpu_idx': 0,\n",
      "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                                      'intra_op_parallelism_threads': 8},\n",
      "            'log_gradients': True,\n",
      "            'log_level': 'WARN',\n",
      "            'log_sys_usage': True,\n",
      "            'logger_config': None,\n",
      "            'logger_creator': None,\n",
      "            'lr': 0.0002,\n",
      "            'lr_schedule': None,\n",
      "            'map_batches_kwargs': {},\n",
      "            'materialize_data': False,\n",
      "            'materialize_mapped_data': True,\n",
      "            'max_num_env_runner_restarts': 1000,\n",
      "            'max_num_offline_eval_runner_restarts': 1000,\n",
      "            'max_requests_in_flight_per_aggregator_actor': 3,\n",
      "            'max_requests_in_flight_per_env_runner': 1,\n",
      "            'max_requests_in_flight_per_learner': 3,\n",
      "            'max_requests_in_flight_per_offline_eval_runner': 1,\n",
      "            'merge_env_runner_states': 'training_only',\n",
      "            'metrics_episode_collection_timeout_s': 60.0,\n",
      "            'metrics_num_episodes_for_smoothing': 100,\n",
      "            'min_sample_timesteps_per_iteration': 0,\n",
      "            'min_time_s_per_iteration': None,\n",
      "            'min_train_timesteps_per_iteration': 0,\n",
      "            'minibatch_size': 128,\n",
      "            'model': {'_disable_action_flattening': False,\n",
      "                      '_disable_preprocessor_api': False,\n",
      "                      '_time_major': False,\n",
      "                      '_use_default_native_models': -1,\n",
      "                      'always_check_shapes': False,\n",
      "                      'attention_dim': 64,\n",
      "                      'attention_head_dim': 32,\n",
      "                      'attention_init_gru_gate_bias': 2.0,\n",
      "                      'attention_memory_inference': 50,\n",
      "                      'attention_memory_training': 50,\n",
      "                      'attention_num_heads': 1,\n",
      "                      'attention_num_transformer_units': 1,\n",
      "                      'attention_position_wise_mlp_dim': 32,\n",
      "                      'attention_use_n_prev_actions': 0,\n",
      "                      'attention_use_n_prev_rewards': 0,\n",
      "                      'conv_activation': 'relu',\n",
      "                      'conv_bias_initializer': None,\n",
      "                      'conv_bias_initializer_config': None,\n",
      "                      'conv_filters': None,\n",
      "                      'conv_kernel_initializer': None,\n",
      "                      'conv_kernel_initializer_config': None,\n",
      "                      'conv_transpose_bias_initializer': None,\n",
      "                      'conv_transpose_bias_initializer_config': None,\n",
      "                      'conv_transpose_kernel_initializer': None,\n",
      "                      'conv_transpose_kernel_initializer_config': None,\n",
      "                      'custom_action_dist': None,\n",
      "                      'custom_model': None,\n",
      "                      'custom_model_config': {},\n",
      "                      'custom_preprocessor': None,\n",
      "                      'dim': 84,\n",
      "                      'encoder_latent_dim': None,\n",
      "                      'fcnet_activation': 'tanh',\n",
      "                      'fcnet_bias_initializer': None,\n",
      "                      'fcnet_bias_initializer_config': None,\n",
      "                      'fcnet_hiddens': [256, 256],\n",
      "                      'fcnet_weights_initializer': None,\n",
      "                      'fcnet_weights_initializer_config': None,\n",
      "                      'framestack': True,\n",
      "                      'free_log_std': False,\n",
      "                      'grayscale': False,\n",
      "                      'log_std_clip_param': 20.0,\n",
      "                      'lstm_bias_initializer': None,\n",
      "                      'lstm_bias_initializer_config': None,\n",
      "                      'lstm_cell_size': 256,\n",
      "                      'lstm_use_prev_action': False,\n",
      "                      'lstm_use_prev_action_reward': -1,\n",
      "                      'lstm_use_prev_reward': False,\n",
      "                      'lstm_weights_initializer': None,\n",
      "                      'lstm_weights_initializer_config': None,\n",
      "                      'max_seq_len': 20,\n",
      "                      'no_final_linear': False,\n",
      "                      'post_fcnet_activation': 'relu',\n",
      "                      'post_fcnet_bias_initializer': None,\n",
      "                      'post_fcnet_bias_initializer_config': None,\n",
      "                      'post_fcnet_hiddens': [],\n",
      "                      'post_fcnet_weights_initializer': None,\n",
      "                      'post_fcnet_weights_initializer_config': None,\n",
      "                      'use_attention': False,\n",
      "                      'use_lstm': False,\n",
      "                      'vf_share_layers': False,\n",
      "                      'zero_mean': True},\n",
      "            'normalize_actions': True,\n",
      "            'num_aggregator_actors_per_learner': 0,\n",
      "            'num_consecutive_env_runner_failures_tolerance': 100,\n",
      "            'num_cpus_for_main_process': 1,\n",
      "            'num_cpus_per_env_runner': 1,\n",
      "            'num_cpus_per_learner': 'auto',\n",
      "            'num_cpus_per_offline_eval_runner': 1,\n",
      "            'num_env_runners': 2,\n",
      "            'num_envs_per_env_runner': 1,\n",
      "            'num_epochs': 10,\n",
      "            'num_gpus': 0,\n",
      "            'num_gpus_per_env_runner': 0,\n",
      "            'num_gpus_per_learner': 0,\n",
      "            'num_gpus_per_offline_eval_runner': 0,\n",
      "            'num_learners': 0,\n",
      "            'num_offline_eval_runners': 0,\n",
      "            'observation_filter': 'NoFilter',\n",
      "            'observation_fn': None,\n",
      "            'observation_space': None,\n",
      "            'off_policy_estimation_methods': {},\n",
      "            'offline_data_class': None,\n",
      "            'offline_eval_batch_size_per_runner': 256,\n",
      "            'offline_eval_rl_module_inference_only': False,\n",
      "            'offline_eval_runner_class': None,\n",
      "            'offline_eval_runner_health_probe_timeout_s': 30.0,\n",
      "            'offline_eval_runner_restore_timeout_s': 1800.0,\n",
      "            'offline_evaluation_duration': 1,\n",
      "            'offline_evaluation_interval': None,\n",
      "            'offline_evaluation_parallel_to_training': False,\n",
      "            'offline_evaluation_timeout_s': 120.0,\n",
      "            'offline_evaluation_type': None,\n",
      "            'offline_loss_for_module_fn': None,\n",
      "            'offline_sampling': False,\n",
      "            'ope_split_batch_by_episode': True,\n",
      "            'optimizer': {},\n",
      "            'output': None,\n",
      "            'output_compress_columns': ['obs', 'new_obs'],\n",
      "            'output_config': {},\n",
      "            'output_filesystem': None,\n",
      "            'output_filesystem_kwargs': {},\n",
      "            'output_max_file_size': 67108864,\n",
      "            'output_max_rows_per_file': None,\n",
      "            'output_write_episodes': True,\n",
      "            'output_write_method': 'write_parquet',\n",
      "            'output_write_method_kwargs': {},\n",
      "            'output_write_remaining_data': False,\n",
      "            'placement_strategy': 'PACK',\n",
      "            'policies': {'default_policy': (None, None, None, None)},\n",
      "            'policies_to_train': None,\n",
      "            'policy_map_cache': -1,\n",
      "            'policy_map_capacity': 100,\n",
      "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x31a3ac900>,\n",
      "            'policy_states_are_swappable': False,\n",
      "            'postprocess_inputs': False,\n",
      "            'prelearner_buffer_class': None,\n",
      "            'prelearner_buffer_kwargs': {},\n",
      "            'prelearner_class': None,\n",
      "            'prelearner_module_synch_period': 10,\n",
      "            'preprocessor_pref': 'deepmind',\n",
      "            'remote_env_batch_wait_ms': 0,\n",
      "            'remote_worker_envs': False,\n",
      "            'render_env': False,\n",
      "            'replay_sequence_length': None,\n",
      "            'restart_failed_env_runners': True,\n",
      "            'restart_failed_offline_eval_runners': True,\n",
      "            'restart_failed_sub_environments': False,\n",
      "            'rollout_fragment_length': 'auto',\n",
      "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      "            'sample_timeout_s': 60.0,\n",
      "            'sampler_perf_stats_ema_coef': None,\n",
      "            'seed': None,\n",
      "            'sgd_minibatch_size': -1,\n",
      "            'shuffle_batch_per_epoch': True,\n",
      "            'shuffle_buffer_size': 0,\n",
      "            'simple_optimizer': False,\n",
      "            'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
      "            'synchronize_filters': -1,\n",
      "            'tf_session_args': {'allow_soft_placement': True,\n",
      "                                'device_count': {'CPU': 1},\n",
      "                                'gpu_options': {'allow_growth': True},\n",
      "                                'inter_op_parallelism_threads': 2,\n",
      "                                'intra_op_parallelism_threads': 2,\n",
      "                                'log_device_placement': False},\n",
      "            'torch_compile_learner': False,\n",
      "            'torch_compile_learner_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_learner_dynamo_mode': None,\n",
      "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
      "            'torch_compile_worker': False,\n",
      "            'torch_compile_worker_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_worker_dynamo_mode': None,\n",
      "            'torch_ddp_kwargs': {},\n",
      "            'torch_skip_nan_gradients': False,\n",
      "            'train_batch_size': 4000,\n",
      "            'update_worker_filter_stats': True,\n",
      "            'use_critic': True,\n",
      "            'use_gae': True,\n",
      "            'use_kl_loss': True,\n",
      "            'use_worker_filter_stats': True,\n",
      "            'validate_env_runners_after_construction': True,\n",
      "            'validate_offline_eval_runners_after_construction': True,\n",
      "            'vf_clip_param': 10.0,\n",
      "            'vf_loss_coeff': 1.0,\n",
      "            'vf_share_layers': -1,\n",
      "            'worker_cls': -1},\n",
      " 'date': '2025-10-06_15-48-43',\n",
      " 'done': False,\n",
      " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
      " 'env_runners': {'agent_episode_return_mean': {'default_agent': -1152.256336285302},\n",
      "                 'env_reset_timer': 0.00019991699809907002,\n",
      "                 'env_step_timer': 2.4074341011980424e-05,\n",
      "                 'env_to_module_connector': {'connector_pipeline_timer': 4.600451234920202e-05,\n",
      "                                             'timers': {'connectors': {'add_observations_from_episodes_to_batch': 2.910562465931456e-06,\n",
      "                                                                       'add_states_from_episodes_to_batch': 8.361893287716006e-07,\n",
      "                                                                       'add_time_dim_to_batch_and_zero_pad': 1.0640755198553905e-06,\n",
      "                                                                       'batch_individual_items': 6.307814726139481e-06,\n",
      "                                                                       'numpy_to_tensor': 1.0123252875187423e-05}}},\n",
      "                 'env_to_module_sum_episodes_length_in': 131.7322456618175,\n",
      "                 'env_to_module_sum_episodes_length_out': 131.7322456618175,\n",
      "                 'episode_duration_sec_mean': 0.057422416599001734,\n",
      "                 'episode_len_max': 200,\n",
      "                 'episode_len_mean': 200.0,\n",
      "                 'episode_len_min': 200,\n",
      "                 'episode_return_max': -874.5551333575025,\n",
      "                 'episode_return_mean': -1152.256336285302,\n",
      "                 'episode_return_min': -1538.052170450519,\n",
      "                 'module_episode_return_mean': {'default_policy': -1152.256336285302},\n",
      "                 'module_to_env_connector': {'connector_pipeline_timer': 0.00012387872969280188,\n",
      "                                             'timers': {'connectors': {'get_actions': 4.001291629733131e-05,\n",
      "                                                                       'listify_data_for_vector_env': 1.0476020087846979e-05,\n",
      "                                                                       'normalize_and_clip_actions': 1.8644892133505044e-05,\n",
      "                                                                       'remove_single_ts_time_rank_from_batch': 7.741850853792509e-07,\n",
      "                                                                       'tensor_to_numpy': 1.6355782976263068e-05,\n",
      "                                                                       'un_batch_to_individual_items': 6.239628305735687e-06}}},\n",
      "                 'num_agent_steps_sampled': {'default_agent': 2000.0},\n",
      "                 'num_agent_steps_sampled_lifetime': {'default_agent': 2000.0},\n",
      "                 'num_env_steps_sampled': 2000.0,\n",
      "                 'num_env_steps_sampled_lifetime': 2000.0,\n",
      "                 'num_env_steps_sampled_lifetime_throughput': 3364.821918625235,\n",
      "                 'num_episodes': 10.0,\n",
      "                 'num_episodes_lifetime': 10.0,\n",
      "                 'num_module_steps_sampled': {'default_policy': 2000.0},\n",
      "                 'num_module_steps_sampled_lifetime': {'default_policy': 2000.0},\n",
      "                 'rlmodule_inference_timer': 4.543536301798672e-05,\n",
      "                 'sample': 0.29531931200108374,\n",
      "                 'weights_seq_no': 0.0},\n",
      " 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0},\n",
      " 'hostname': 'scadsdp25.misc.intern.uni-leipzig.de',\n",
      " 'iterations_since_restore': 1,\n",
      " 'learners': {'__all_modules__': {'learner_connector': {'connector_pipeline_timer': 0.04463579200091772,\n",
      "                                                        'timers': {'connectors': {'add_columns_from_episodes_to_train_batch': 0.01606904099753592,\n",
      "                                                                                  'add_observations_from_episodes_to_batch': 5.562499427469447e-05,\n",
      "                                                                                  'add_one_ts_to_episodes_and_truncate': 0.001141000000643544,\n",
      "                                                                                  'add_states_from_episodes_to_batch': 9.625000529922545e-06,\n",
      "                                                                                  'add_time_dim_to_batch_and_zero_pad': 1.8292004824616015e-05,\n",
      "                                                                                  'batch_individual_items': 0.01422387499769684,\n",
      "                                                                                  'general_advantage_estimation': 0.012795167000149377,\n",
      "                                                                                  'numpy_to_tensor': 0.0001115830018534325}}},\n",
      "                                  'learner_connector_sum_episodes_length_in': 2000,\n",
      "                                  'learner_connector_sum_episodes_length_out': 2010,\n",
      "                                  'num_env_steps_trained': 317580,\n",
      "                                  'num_env_steps_trained_lifetime': 317580,\n",
      "                                  'num_env_steps_trained_lifetime_throughput': 927621.3392163919,\n",
      "                                  'num_module_steps_trained': 20224,\n",
      "                                  'num_module_steps_trained_lifetime': 20224,\n",
      "                                  'num_module_steps_trained_lifetime_throughput': 59073.5613990931,\n",
      "                                  'num_module_steps_trained_throughput': 59075.48611192916,\n",
      "                                  'num_non_trainable_parameters': 0,\n",
      "                                  'num_trainable_parameters': 134403},\n",
      "              'default_policy': {'curr_entropy_coeff': 0.0,\n",
      "                                 'curr_kl_coeff': 0.20000000298023224,\n",
      "                                 'default_optimizer_learning_rate': 0.0002,\n",
      "                                 'diff_num_grad_updates_vs_sampler_policy': 1.0,\n",
      "                                 'entropy': 1.3829737,\n",
      "                                 'gradients_default_optimizer_global_norm': 1.7754287,\n",
      "                                 'mean_kl_loss': 0.012402565,\n",
      "                                 'module_train_batch_size_mean': 128.0,\n",
      "                                 'num_module_steps_trained': 20224,\n",
      "                                 'num_module_steps_trained_lifetime': 20224,\n",
      "                                 'num_module_steps_trained_lifetime_throughput': 59074.59066727641,\n",
      "                                 'num_trainable_parameters': 134403,\n",
      "                                 'policy_loss': 0.18146858,\n",
      "                                 'total_loss': 10.183949,\n",
      "                                 'vf_explained_var': -0.006386876,\n",
      "                                 'vf_loss': 10.0,\n",
      "                                 'vf_loss_unclipped': 152347.14,\n",
      "                                 'weights_seq_no': 1.0}},\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_env_steps_sampled_lifetime': 2000.0,\n",
      " 'num_training_step_calls_per_iteration': 1,\n",
      " 'perf': {'cpu_util_percent': 0.0, 'ram_util_percent': 68.5},\n",
      " 'pid': 29591,\n",
      " 'time_since_restore': 0.7221298217773438,\n",
      " 'time_this_iter_s': 0.7221298217773438,\n",
      " 'time_total_s': 0.7221298217773438,\n",
      " 'timers': {'env_runner_sampling_timer': 0.30494145899865543,\n",
      "            'learner_update_timer': 0.411687041996629,\n",
      "            'restore_env_runners': 7.0829992182552814e-06,\n",
      "            'synch_weights': 0.0009716249987832271,\n",
      "            'training_iteration': 0.7178707079947344,\n",
      "            'training_step': 0.7177664169939817},\n",
      " 'timestamp': 1759758523,\n",
      " 'training_iteration': 1,\n",
      " 'trial_id': 'default'}\n",
      "{'config': {'_disable_action_flattening': False,\n",
      "            '_disable_execution_plan_api': -1,\n",
      "            '_disable_initialize_loss_from_dummy_batch': False,\n",
      "            '_disable_preprocessor_api': False,\n",
      "            '_dont_auto_sync_env_runner_states': False,\n",
      "            '_enable_rl_module_api': -1,\n",
      "            '_env_to_module_connector': None,\n",
      "            '_fake_gpus': False,\n",
      "            '_is_atari': None,\n",
      "            '_is_online': True,\n",
      "            '_learner_class': None,\n",
      "            '_learner_connector': None,\n",
      "            '_model_config': {},\n",
      "            '_module_to_env_connector': None,\n",
      "            '_per_module_overrides': {},\n",
      "            '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
      "            '_rl_module_spec': None,\n",
      "            '_tf_policy_handles_more_than_one_loss': False,\n",
      "            '_torch_grad_scaler_class': None,\n",
      "            '_torch_lr_scheduler_classes': None,\n",
      "            '_train_batch_size_per_learner': 2000,\n",
      "            '_use_msgpack_checkpoints': False,\n",
      "            '_validate_config': True,\n",
      "            'action_mask_key': 'action_mask',\n",
      "            'action_space': None,\n",
      "            'actions_in_input_normalized': False,\n",
      "            'add_default_connectors_to_env_to_module_pipeline': True,\n",
      "            'add_default_connectors_to_learner_pipeline': True,\n",
      "            'add_default_connectors_to_module_to_env_pipeline': True,\n",
      "            'algorithm_config_overrides_per_module': {},\n",
      "            'always_attach_evaluation_results': -1,\n",
      "            'auto_wrap_old_gym_envs': -1,\n",
      "            'batch_mode': 'truncate_episodes',\n",
      "            'broadcast_env_runner_states': True,\n",
      "            'broadcast_offline_eval_runner_states': False,\n",
      "            'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>,\n",
      "            'callbacks_on_algorithm_init': None,\n",
      "            'callbacks_on_checkpoint_loaded': None,\n",
      "            'callbacks_on_env_runners_recreated': None,\n",
      "            'callbacks_on_environment_created': None,\n",
      "            'callbacks_on_episode_created': None,\n",
      "            'callbacks_on_episode_end': None,\n",
      "            'callbacks_on_episode_start': None,\n",
      "            'callbacks_on_episode_step': None,\n",
      "            'callbacks_on_evaluate_end': None,\n",
      "            'callbacks_on_evaluate_offline_end': None,\n",
      "            'callbacks_on_evaluate_offline_start': None,\n",
      "            'callbacks_on_evaluate_start': None,\n",
      "            'callbacks_on_offline_eval_runners_recreated': None,\n",
      "            'callbacks_on_sample_end': None,\n",
      "            'callbacks_on_train_result': None,\n",
      "            'checkpoint_trainable_policies_only': False,\n",
      "            'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>,\n",
      "            'clip_actions': False,\n",
      "            'clip_param': 0.3,\n",
      "            'clip_rewards': None,\n",
      "            'compress_observations': False,\n",
      "            'count_steps_by': 'env_steps',\n",
      "            'create_env_on_driver': False,\n",
      "            'create_local_env_runner': True,\n",
      "            'custom_async_evaluation_function': -1,\n",
      "            'custom_eval_function': None,\n",
      "            'custom_resources_per_env_runner': {},\n",
      "            'custom_resources_per_offline_eval_runner': {},\n",
      "            'dataset_num_iters_per_eval_runner': 1,\n",
      "            'dataset_num_iters_per_learner': None,\n",
      "            'delay_between_env_runner_restarts_s': 60.0,\n",
      "            'disable_env_checking': False,\n",
      "            'eager_max_retraces': 20,\n",
      "            'eager_tracing': True,\n",
      "            'enable_async_evaluation': -1,\n",
      "            'enable_connectors': -1,\n",
      "            'enable_env_runner_and_connector_v2': True,\n",
      "            'enable_rl_module_and_learner': True,\n",
      "            'enable_tf1_exec_eagerly': False,\n",
      "            'entropy_coeff': 0.0,\n",
      "            'entropy_coeff_schedule': None,\n",
      "            'env': 'Pendulum-v1',\n",
      "            'env_config': {},\n",
      "            'env_runner_cls': None,\n",
      "            'env_runner_health_probe_timeout_s': 30.0,\n",
      "            'env_runner_restore_timeout_s': 1800.0,\n",
      "            'env_task_fn': -1,\n",
      "            'episode_lookback_horizon': 1,\n",
      "            'episodes_to_numpy': True,\n",
      "            'evaluation_auto_duration_max_env_steps_per_sample': 2000,\n",
      "            'evaluation_auto_duration_min_env_steps_per_sample': 100,\n",
      "            'evaluation_config': None,\n",
      "            'evaluation_duration': 10,\n",
      "            'evaluation_duration_unit': 'episodes',\n",
      "            'evaluation_force_reset_envs_before_iteration': True,\n",
      "            'evaluation_interval': None,\n",
      "            'evaluation_num_env_runners': 0,\n",
      "            'evaluation_parallel_to_training': False,\n",
      "            'evaluation_sample_timeout_s': 120.0,\n",
      "            'exploration_config': {},\n",
      "            'explore': True,\n",
      "            'export_native_model_files': False,\n",
      "            'extra_python_environs_for_driver': {},\n",
      "            'extra_python_environs_for_worker': {},\n",
      "            'fake_sampler': False,\n",
      "            'framework': 'torch',\n",
      "            'gamma': 0.99,\n",
      "            'grad_clip': None,\n",
      "            'grad_clip_by': 'global_norm',\n",
      "            'gym_env_vectorize_mode': 'SYNC',\n",
      "            'ignore_env_runner_failures': False,\n",
      "            'ignore_final_observation': False,\n",
      "            'ignore_offline_eval_runner_failures': False,\n",
      "            'in_evaluation': False,\n",
      "            'input': 'sampler',\n",
      "            'input_compress_columns': ['obs', 'new_obs'],\n",
      "            'input_config': {},\n",
      "            'input_filesystem': None,\n",
      "            'input_filesystem_kwargs': {},\n",
      "            'input_read_batch_size': None,\n",
      "            'input_read_episodes': False,\n",
      "            'input_read_method': 'read_parquet',\n",
      "            'input_read_method_kwargs': {},\n",
      "            'input_read_sample_batches': False,\n",
      "            'input_read_schema': {},\n",
      "            'input_spaces_jsonable': True,\n",
      "            'iter_batches_kwargs': {},\n",
      "            'keep_per_episode_custom_metrics': False,\n",
      "            'kl_coeff': 0.2,\n",
      "            'kl_target': 0.01,\n",
      "            'lambda': 1.0,\n",
      "            'learner_config_dict': {},\n",
      "            'local_gpu_idx': 0,\n",
      "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                                      'intra_op_parallelism_threads': 8},\n",
      "            'log_gradients': True,\n",
      "            'log_level': 'WARN',\n",
      "            'log_sys_usage': True,\n",
      "            'logger_config': None,\n",
      "            'logger_creator': None,\n",
      "            'lr': 0.0002,\n",
      "            'lr_schedule': None,\n",
      "            'map_batches_kwargs': {},\n",
      "            'materialize_data': False,\n",
      "            'materialize_mapped_data': True,\n",
      "            'max_num_env_runner_restarts': 1000,\n",
      "            'max_num_offline_eval_runner_restarts': 1000,\n",
      "            'max_requests_in_flight_per_aggregator_actor': 3,\n",
      "            'max_requests_in_flight_per_env_runner': 1,\n",
      "            'max_requests_in_flight_per_learner': 3,\n",
      "            'max_requests_in_flight_per_offline_eval_runner': 1,\n",
      "            'merge_env_runner_states': 'training_only',\n",
      "            'metrics_episode_collection_timeout_s': 60.0,\n",
      "            'metrics_num_episodes_for_smoothing': 100,\n",
      "            'min_sample_timesteps_per_iteration': 0,\n",
      "            'min_time_s_per_iteration': None,\n",
      "            'min_train_timesteps_per_iteration': 0,\n",
      "            'minibatch_size': 128,\n",
      "            'model': {'_disable_action_flattening': False,\n",
      "                      '_disable_preprocessor_api': False,\n",
      "                      '_time_major': False,\n",
      "                      '_use_default_native_models': -1,\n",
      "                      'always_check_shapes': False,\n",
      "                      'attention_dim': 64,\n",
      "                      'attention_head_dim': 32,\n",
      "                      'attention_init_gru_gate_bias': 2.0,\n",
      "                      'attention_memory_inference': 50,\n",
      "                      'attention_memory_training': 50,\n",
      "                      'attention_num_heads': 1,\n",
      "                      'attention_num_transformer_units': 1,\n",
      "                      'attention_position_wise_mlp_dim': 32,\n",
      "                      'attention_use_n_prev_actions': 0,\n",
      "                      'attention_use_n_prev_rewards': 0,\n",
      "                      'conv_activation': 'relu',\n",
      "                      'conv_bias_initializer': None,\n",
      "                      'conv_bias_initializer_config': None,\n",
      "                      'conv_filters': None,\n",
      "                      'conv_kernel_initializer': None,\n",
      "                      'conv_kernel_initializer_config': None,\n",
      "                      'conv_transpose_bias_initializer': None,\n",
      "                      'conv_transpose_bias_initializer_config': None,\n",
      "                      'conv_transpose_kernel_initializer': None,\n",
      "                      'conv_transpose_kernel_initializer_config': None,\n",
      "                      'custom_action_dist': None,\n",
      "                      'custom_model': None,\n",
      "                      'custom_model_config': {},\n",
      "                      'custom_preprocessor': None,\n",
      "                      'dim': 84,\n",
      "                      'encoder_latent_dim': None,\n",
      "                      'fcnet_activation': 'tanh',\n",
      "                      'fcnet_bias_initializer': None,\n",
      "                      'fcnet_bias_initializer_config': None,\n",
      "                      'fcnet_hiddens': [256, 256],\n",
      "                      'fcnet_weights_initializer': None,\n",
      "                      'fcnet_weights_initializer_config': None,\n",
      "                      'framestack': True,\n",
      "                      'free_log_std': False,\n",
      "                      'grayscale': False,\n",
      "                      'log_std_clip_param': 20.0,\n",
      "                      'lstm_bias_initializer': None,\n",
      "                      'lstm_bias_initializer_config': None,\n",
      "                      'lstm_cell_size': 256,\n",
      "                      'lstm_use_prev_action': False,\n",
      "                      'lstm_use_prev_action_reward': -1,\n",
      "                      'lstm_use_prev_reward': False,\n",
      "                      'lstm_weights_initializer': None,\n",
      "                      'lstm_weights_initializer_config': None,\n",
      "                      'max_seq_len': 20,\n",
      "                      'no_final_linear': False,\n",
      "                      'post_fcnet_activation': 'relu',\n",
      "                      'post_fcnet_bias_initializer': None,\n",
      "                      'post_fcnet_bias_initializer_config': None,\n",
      "                      'post_fcnet_hiddens': [],\n",
      "                      'post_fcnet_weights_initializer': None,\n",
      "                      'post_fcnet_weights_initializer_config': None,\n",
      "                      'use_attention': False,\n",
      "                      'use_lstm': False,\n",
      "                      'vf_share_layers': False,\n",
      "                      'zero_mean': True},\n",
      "            'normalize_actions': True,\n",
      "            'num_aggregator_actors_per_learner': 0,\n",
      "            'num_consecutive_env_runner_failures_tolerance': 100,\n",
      "            'num_cpus_for_main_process': 1,\n",
      "            'num_cpus_per_env_runner': 1,\n",
      "            'num_cpus_per_learner': 'auto',\n",
      "            'num_cpus_per_offline_eval_runner': 1,\n",
      "            'num_env_runners': 2,\n",
      "            'num_envs_per_env_runner': 1,\n",
      "            'num_epochs': 10,\n",
      "            'num_gpus': 0,\n",
      "            'num_gpus_per_env_runner': 0,\n",
      "            'num_gpus_per_learner': 0,\n",
      "            'num_gpus_per_offline_eval_runner': 0,\n",
      "            'num_learners': 0,\n",
      "            'num_offline_eval_runners': 0,\n",
      "            'observation_filter': 'NoFilter',\n",
      "            'observation_fn': None,\n",
      "            'observation_space': None,\n",
      "            'off_policy_estimation_methods': {},\n",
      "            'offline_data_class': None,\n",
      "            'offline_eval_batch_size_per_runner': 256,\n",
      "            'offline_eval_rl_module_inference_only': False,\n",
      "            'offline_eval_runner_class': None,\n",
      "            'offline_eval_runner_health_probe_timeout_s': 30.0,\n",
      "            'offline_eval_runner_restore_timeout_s': 1800.0,\n",
      "            'offline_evaluation_duration': 1,\n",
      "            'offline_evaluation_interval': None,\n",
      "            'offline_evaluation_parallel_to_training': False,\n",
      "            'offline_evaluation_timeout_s': 120.0,\n",
      "            'offline_evaluation_type': None,\n",
      "            'offline_loss_for_module_fn': None,\n",
      "            'offline_sampling': False,\n",
      "            'ope_split_batch_by_episode': True,\n",
      "            'optimizer': {},\n",
      "            'output': None,\n",
      "            'output_compress_columns': ['obs', 'new_obs'],\n",
      "            'output_config': {},\n",
      "            'output_filesystem': None,\n",
      "            'output_filesystem_kwargs': {},\n",
      "            'output_max_file_size': 67108864,\n",
      "            'output_max_rows_per_file': None,\n",
      "            'output_write_episodes': True,\n",
      "            'output_write_method': 'write_parquet',\n",
      "            'output_write_method_kwargs': {},\n",
      "            'output_write_remaining_data': False,\n",
      "            'placement_strategy': 'PACK',\n",
      "            'policies': {'default_policy': (None, None, None, None)},\n",
      "            'policies_to_train': None,\n",
      "            'policy_map_cache': -1,\n",
      "            'policy_map_capacity': 100,\n",
      "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x31a3ac900>,\n",
      "            'policy_states_are_swappable': False,\n",
      "            'postprocess_inputs': False,\n",
      "            'prelearner_buffer_class': None,\n",
      "            'prelearner_buffer_kwargs': {},\n",
      "            'prelearner_class': None,\n",
      "            'prelearner_module_synch_period': 10,\n",
      "            'preprocessor_pref': 'deepmind',\n",
      "            'remote_env_batch_wait_ms': 0,\n",
      "            'remote_worker_envs': False,\n",
      "            'render_env': False,\n",
      "            'replay_sequence_length': None,\n",
      "            'restart_failed_env_runners': True,\n",
      "            'restart_failed_offline_eval_runners': True,\n",
      "            'restart_failed_sub_environments': False,\n",
      "            'rollout_fragment_length': 'auto',\n",
      "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      "            'sample_timeout_s': 60.0,\n",
      "            'sampler_perf_stats_ema_coef': None,\n",
      "            'seed': None,\n",
      "            'sgd_minibatch_size': -1,\n",
      "            'shuffle_batch_per_epoch': True,\n",
      "            'shuffle_buffer_size': 0,\n",
      "            'simple_optimizer': False,\n",
      "            'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
      "            'synchronize_filters': -1,\n",
      "            'tf_session_args': {'allow_soft_placement': True,\n",
      "                                'device_count': {'CPU': 1},\n",
      "                                'gpu_options': {'allow_growth': True},\n",
      "                                'inter_op_parallelism_threads': 2,\n",
      "                                'intra_op_parallelism_threads': 2,\n",
      "                                'log_device_placement': False},\n",
      "            'torch_compile_learner': False,\n",
      "            'torch_compile_learner_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_learner_dynamo_mode': None,\n",
      "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
      "            'torch_compile_worker': False,\n",
      "            'torch_compile_worker_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_worker_dynamo_mode': None,\n",
      "            'torch_ddp_kwargs': {},\n",
      "            'torch_skip_nan_gradients': False,\n",
      "            'train_batch_size': 4000,\n",
      "            'update_worker_filter_stats': True,\n",
      "            'use_critic': True,\n",
      "            'use_gae': True,\n",
      "            'use_kl_loss': True,\n",
      "            'use_worker_filter_stats': True,\n",
      "            'validate_env_runners_after_construction': True,\n",
      "            'validate_offline_eval_runners_after_construction': True,\n",
      "            'vf_clip_param': 10.0,\n",
      "            'vf_loss_coeff': 1.0,\n",
      "            'vf_share_layers': -1,\n",
      "            'worker_cls': -1},\n",
      " 'date': '2025-10-06_15-48-44',\n",
      " 'done': False,\n",
      " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
      " 'env_runners': {'agent_episode_return_mean': {'default_agent': -1161.3659355148852},\n",
      "                 'env_reset_timer': 0.00019991699809907002,\n",
      "                 'env_step_timer': 2.348898304128621e-05,\n",
      "                 'env_to_module_connector': {'connector_pipeline_timer': 4.765912262860864e-05,\n",
      "                                             'timers': {'connectors': {'add_observations_from_episodes_to_batch': 3.0226680289751513e-06,\n",
      "                                                                       'add_states_from_episodes_to_batch': 8.975998141135808e-07,\n",
      "                                                                       'add_time_dim_to_batch_and_zero_pad': 1.152242843369236e-06,\n",
      "                                                                       'batch_individual_items': 6.667825554233419e-06,\n",
      "                                                                       'numpy_to_tensor': 1.0408139817546178e-05}}},\n",
      "                 'env_to_module_sum_episodes_length_in': 131.73765398537716,\n",
      "                 'env_to_module_sum_episodes_length_out': 131.73765398537716,\n",
      "                 'episode_duration_sec_mean': 0.05763144589982403,\n",
      "                 'episode_len_max': 200,\n",
      "                 'episode_len_mean': 200.0,\n",
      "                 'episode_len_min': 200,\n",
      "                 'episode_return_max': -765.5095064681235,\n",
      "                 'episode_return_mean': -1161.3659355148852,\n",
      "                 'episode_return_min': -1569.605529083327,\n",
      "                 'module_episode_return_mean': {'default_policy': -1161.3659355148852},\n",
      "                 'module_to_env_connector': {'connector_pipeline_timer': 0.00012675448080768446,\n",
      "                                             'timers': {'connectors': {'get_actions': 4.116418199060261e-05,\n",
      "                                                                       'listify_data_for_vector_env': 1.0408067151833427e-05,\n",
      "                                                                       'normalize_and_clip_actions': 1.8884501919548923e-05,\n",
      "                                                                       'remove_single_ts_time_rank_from_batch': 8.123397108163333e-07,\n",
      "                                                                       'tensor_to_numpy': 1.7249127754721634e-05,\n",
      "                                                                       'un_batch_to_individual_items': 6.476816110760352e-06}}},\n",
      "                 'num_agent_steps_sampled': {'default_agent': 2000.0},\n",
      "                 'num_agent_steps_sampled_lifetime': {'default_agent': 4000.0},\n",
      "                 'num_env_steps_sampled': 2000.0,\n",
      "                 'num_env_steps_sampled_lifetime': 4000.0,\n",
      "                 'num_env_steps_sampled_lifetime_throughput': 3355.0366099115554,\n",
      "                 'num_episodes': 10.0,\n",
      "                 'num_episodes_lifetime': 20.0,\n",
      "                 'num_module_steps_sampled': {'default_policy': 2000.0},\n",
      "                 'num_module_steps_sampled_lifetime': {'default_policy': 4000.0},\n",
      "                 'rlmodule_inference_timer': 4.694211690250206e-05,\n",
      "                 'sample': 0.29534046221106114,\n",
      "                 'time_between_sampling': 0.43914502099869424,\n",
      "                 'weights_seq_no': 1.0},\n",
      " 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0},\n",
      " 'hostname': 'scadsdp25.misc.intern.uni-leipzig.de',\n",
      " 'iterations_since_restore': 2,\n",
      " 'learners': {'__all_modules__': {'learner_connector': {'connector_pipeline_timer': 0.044532229080869005,\n",
      "                                                        'timers': {'connectors': {'add_columns_from_episodes_to_train_batch': 0.01606667808759084,\n",
      "                                                                                  'add_observations_from_episodes_to_batch': 5.558166434639134e-05,\n",
      "                                                                                  'add_one_ts_to_episodes_and_truncate': 0.0011391891606035642,\n",
      "                                                                                  'add_states_from_episodes_to_batch': 9.56542047788389e-06,\n",
      "                                                                                  'add_time_dim_to_batch_and_zero_pad': 1.8216994794784115e-05,\n",
      "                                                                                  'batch_individual_items': 0.014223617497700616,\n",
      "                                                                                  'general_advantage_estimation': 0.012697500740105169,\n",
      "                                                                                  'numpy_to_tensor': 0.00011149342179123779}}},\n",
      "                                  'learner_connector_sum_episodes_length_in': 2000.0,\n",
      "                                  'learner_connector_sum_episodes_length_out': 2010.0,\n",
      "                                  'num_env_steps_trained': 317580,\n",
      "                                  'num_env_steps_trained_lifetime': 635160,\n",
      "                                  'num_env_steps_trained_lifetime_throughput': 927014.5049722145,\n",
      "                                  'num_module_steps_trained': 20224,\n",
      "                                  'num_module_steps_trained_lifetime': 40448,\n",
      "                                  'num_module_steps_trained_lifetime_throughput': 59034.790341072876,\n",
      "                                  'num_module_steps_trained_throughput': 59036.610726184896,\n",
      "                                  'num_non_trainable_parameters': 0,\n",
      "                                  'num_trainable_parameters': 134403},\n",
      "              'default_policy': {'curr_entropy_coeff': 0.0,\n",
      "                                 'curr_kl_coeff': 0.30000001192092896,\n",
      "                                 'default_optimizer_learning_rate': 0.0002,\n",
      "                                 'diff_num_grad_updates_vs_sampler_policy': 1.0,\n",
      "                                 'entropy': 1.3896846,\n",
      "                                 'gradients_default_optimizer_global_norm': 1.6759937,\n",
      "                                 'mean_kl_loss': 0.031294838,\n",
      "                                 'module_train_batch_size_mean': 128.0,\n",
      "                                 'num_module_steps_trained': 20224,\n",
      "                                 'num_module_steps_trained_lifetime': 40448,\n",
      "                                 'num_module_steps_trained_lifetime_throughput': 59035.76015842645,\n",
      "                                 'num_trainable_parameters': 134403,\n",
      "                                 'policy_loss': 0.0051421416,\n",
      "                                 'total_loss': 9.85819,\n",
      "                                 'vf_explained_var': 0.00060510635,\n",
      "                                 'vf_loss': 9.846789,\n",
      "                                 'vf_loss_unclipped': 134870.4,\n",
      "                                 'weights_seq_no': 2.0}},\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_env_steps_sampled_lifetime': 4000.0,\n",
      " 'num_training_step_calls_per_iteration': 1,\n",
      " 'perf': {'cpu_util_percent': 23.5, 'ram_util_percent': 68.5},\n",
      " 'pid': 29591,\n",
      " 'time_since_restore': 1.4187791347503662,\n",
      " 'time_this_iter_s': 0.6966493129730225,\n",
      " 'time_total_s': 1.4187791347503662,\n",
      " 'timers': {'env_runner_sampling_timer': 0.3049414194086421,\n",
      "            'learner_update_timer': 0.4114282936566451,\n",
      "            'restore_env_runners': 7.0884192246012384e-06,\n",
      "            'synch_env_connectors': 0.00281912500213366,\n",
      "            'synch_weights': 0.0009713991587341298,\n",
      "            'training_iteration': 0.7176118738247896,\n",
      "            'training_step': 0.7175071978240157},\n",
      " 'timestamp': 1759758524,\n",
      " 'training_iteration': 2,\n",
      " 'trial_id': 'default'}\n",
      "{'config': {'_disable_action_flattening': False,\n",
      "            '_disable_execution_plan_api': -1,\n",
      "            '_disable_initialize_loss_from_dummy_batch': False,\n",
      "            '_disable_preprocessor_api': False,\n",
      "            '_dont_auto_sync_env_runner_states': False,\n",
      "            '_enable_rl_module_api': -1,\n",
      "            '_env_to_module_connector': None,\n",
      "            '_fake_gpus': False,\n",
      "            '_is_atari': None,\n",
      "            '_is_online': True,\n",
      "            '_learner_class': None,\n",
      "            '_learner_connector': None,\n",
      "            '_model_config': {},\n",
      "            '_module_to_env_connector': None,\n",
      "            '_per_module_overrides': {},\n",
      "            '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
      "            '_rl_module_spec': None,\n",
      "            '_tf_policy_handles_more_than_one_loss': False,\n",
      "            '_torch_grad_scaler_class': None,\n",
      "            '_torch_lr_scheduler_classes': None,\n",
      "            '_train_batch_size_per_learner': 2000,\n",
      "            '_use_msgpack_checkpoints': False,\n",
      "            '_validate_config': True,\n",
      "            'action_mask_key': 'action_mask',\n",
      "            'action_space': None,\n",
      "            'actions_in_input_normalized': False,\n",
      "            'add_default_connectors_to_env_to_module_pipeline': True,\n",
      "            'add_default_connectors_to_learner_pipeline': True,\n",
      "            'add_default_connectors_to_module_to_env_pipeline': True,\n",
      "            'algorithm_config_overrides_per_module': {},\n",
      "            'always_attach_evaluation_results': -1,\n",
      "            'auto_wrap_old_gym_envs': -1,\n",
      "            'batch_mode': 'truncate_episodes',\n",
      "            'broadcast_env_runner_states': True,\n",
      "            'broadcast_offline_eval_runner_states': False,\n",
      "            'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>,\n",
      "            'callbacks_on_algorithm_init': None,\n",
      "            'callbacks_on_checkpoint_loaded': None,\n",
      "            'callbacks_on_env_runners_recreated': None,\n",
      "            'callbacks_on_environment_created': None,\n",
      "            'callbacks_on_episode_created': None,\n",
      "            'callbacks_on_episode_end': None,\n",
      "            'callbacks_on_episode_start': None,\n",
      "            'callbacks_on_episode_step': None,\n",
      "            'callbacks_on_evaluate_end': None,\n",
      "            'callbacks_on_evaluate_offline_end': None,\n",
      "            'callbacks_on_evaluate_offline_start': None,\n",
      "            'callbacks_on_evaluate_start': None,\n",
      "            'callbacks_on_offline_eval_runners_recreated': None,\n",
      "            'callbacks_on_sample_end': None,\n",
      "            'callbacks_on_train_result': None,\n",
      "            'checkpoint_trainable_policies_only': False,\n",
      "            'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>,\n",
      "            'clip_actions': False,\n",
      "            'clip_param': 0.3,\n",
      "            'clip_rewards': None,\n",
      "            'compress_observations': False,\n",
      "            'count_steps_by': 'env_steps',\n",
      "            'create_env_on_driver': False,\n",
      "            'create_local_env_runner': True,\n",
      "            'custom_async_evaluation_function': -1,\n",
      "            'custom_eval_function': None,\n",
      "            'custom_resources_per_env_runner': {},\n",
      "            'custom_resources_per_offline_eval_runner': {},\n",
      "            'dataset_num_iters_per_eval_runner': 1,\n",
      "            'dataset_num_iters_per_learner': None,\n",
      "            'delay_between_env_runner_restarts_s': 60.0,\n",
      "            'disable_env_checking': False,\n",
      "            'eager_max_retraces': 20,\n",
      "            'eager_tracing': True,\n",
      "            'enable_async_evaluation': -1,\n",
      "            'enable_connectors': -1,\n",
      "            'enable_env_runner_and_connector_v2': True,\n",
      "            'enable_rl_module_and_learner': True,\n",
      "            'enable_tf1_exec_eagerly': False,\n",
      "            'entropy_coeff': 0.0,\n",
      "            'entropy_coeff_schedule': None,\n",
      "            'env': 'Pendulum-v1',\n",
      "            'env_config': {},\n",
      "            'env_runner_cls': None,\n",
      "            'env_runner_health_probe_timeout_s': 30.0,\n",
      "            'env_runner_restore_timeout_s': 1800.0,\n",
      "            'env_task_fn': -1,\n",
      "            'episode_lookback_horizon': 1,\n",
      "            'episodes_to_numpy': True,\n",
      "            'evaluation_auto_duration_max_env_steps_per_sample': 2000,\n",
      "            'evaluation_auto_duration_min_env_steps_per_sample': 100,\n",
      "            'evaluation_config': None,\n",
      "            'evaluation_duration': 10,\n",
      "            'evaluation_duration_unit': 'episodes',\n",
      "            'evaluation_force_reset_envs_before_iteration': True,\n",
      "            'evaluation_interval': None,\n",
      "            'evaluation_num_env_runners': 0,\n",
      "            'evaluation_parallel_to_training': False,\n",
      "            'evaluation_sample_timeout_s': 120.0,\n",
      "            'exploration_config': {},\n",
      "            'explore': True,\n",
      "            'export_native_model_files': False,\n",
      "            'extra_python_environs_for_driver': {},\n",
      "            'extra_python_environs_for_worker': {},\n",
      "            'fake_sampler': False,\n",
      "            'framework': 'torch',\n",
      "            'gamma': 0.99,\n",
      "            'grad_clip': None,\n",
      "            'grad_clip_by': 'global_norm',\n",
      "            'gym_env_vectorize_mode': 'SYNC',\n",
      "            'ignore_env_runner_failures': False,\n",
      "            'ignore_final_observation': False,\n",
      "            'ignore_offline_eval_runner_failures': False,\n",
      "            'in_evaluation': False,\n",
      "            'input': 'sampler',\n",
      "            'input_compress_columns': ['obs', 'new_obs'],\n",
      "            'input_config': {},\n",
      "            'input_filesystem': None,\n",
      "            'input_filesystem_kwargs': {},\n",
      "            'input_read_batch_size': None,\n",
      "            'input_read_episodes': False,\n",
      "            'input_read_method': 'read_parquet',\n",
      "            'input_read_method_kwargs': {},\n",
      "            'input_read_sample_batches': False,\n",
      "            'input_read_schema': {},\n",
      "            'input_spaces_jsonable': True,\n",
      "            'iter_batches_kwargs': {},\n",
      "            'keep_per_episode_custom_metrics': False,\n",
      "            'kl_coeff': 0.2,\n",
      "            'kl_target': 0.01,\n",
      "            'lambda': 1.0,\n",
      "            'learner_config_dict': {},\n",
      "            'local_gpu_idx': 0,\n",
      "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                                      'intra_op_parallelism_threads': 8},\n",
      "            'log_gradients': True,\n",
      "            'log_level': 'WARN',\n",
      "            'log_sys_usage': True,\n",
      "            'logger_config': None,\n",
      "            'logger_creator': None,\n",
      "            'lr': 0.0002,\n",
      "            'lr_schedule': None,\n",
      "            'map_batches_kwargs': {},\n",
      "            'materialize_data': False,\n",
      "            'materialize_mapped_data': True,\n",
      "            'max_num_env_runner_restarts': 1000,\n",
      "            'max_num_offline_eval_runner_restarts': 1000,\n",
      "            'max_requests_in_flight_per_aggregator_actor': 3,\n",
      "            'max_requests_in_flight_per_env_runner': 1,\n",
      "            'max_requests_in_flight_per_learner': 3,\n",
      "            'max_requests_in_flight_per_offline_eval_runner': 1,\n",
      "            'merge_env_runner_states': 'training_only',\n",
      "            'metrics_episode_collection_timeout_s': 60.0,\n",
      "            'metrics_num_episodes_for_smoothing': 100,\n",
      "            'min_sample_timesteps_per_iteration': 0,\n",
      "            'min_time_s_per_iteration': None,\n",
      "            'min_train_timesteps_per_iteration': 0,\n",
      "            'minibatch_size': 128,\n",
      "            'model': {'_disable_action_flattening': False,\n",
      "                      '_disable_preprocessor_api': False,\n",
      "                      '_time_major': False,\n",
      "                      '_use_default_native_models': -1,\n",
      "                      'always_check_shapes': False,\n",
      "                      'attention_dim': 64,\n",
      "                      'attention_head_dim': 32,\n",
      "                      'attention_init_gru_gate_bias': 2.0,\n",
      "                      'attention_memory_inference': 50,\n",
      "                      'attention_memory_training': 50,\n",
      "                      'attention_num_heads': 1,\n",
      "                      'attention_num_transformer_units': 1,\n",
      "                      'attention_position_wise_mlp_dim': 32,\n",
      "                      'attention_use_n_prev_actions': 0,\n",
      "                      'attention_use_n_prev_rewards': 0,\n",
      "                      'conv_activation': 'relu',\n",
      "                      'conv_bias_initializer': None,\n",
      "                      'conv_bias_initializer_config': None,\n",
      "                      'conv_filters': None,\n",
      "                      'conv_kernel_initializer': None,\n",
      "                      'conv_kernel_initializer_config': None,\n",
      "                      'conv_transpose_bias_initializer': None,\n",
      "                      'conv_transpose_bias_initializer_config': None,\n",
      "                      'conv_transpose_kernel_initializer': None,\n",
      "                      'conv_transpose_kernel_initializer_config': None,\n",
      "                      'custom_action_dist': None,\n",
      "                      'custom_model': None,\n",
      "                      'custom_model_config': {},\n",
      "                      'custom_preprocessor': None,\n",
      "                      'dim': 84,\n",
      "                      'encoder_latent_dim': None,\n",
      "                      'fcnet_activation': 'tanh',\n",
      "                      'fcnet_bias_initializer': None,\n",
      "                      'fcnet_bias_initializer_config': None,\n",
      "                      'fcnet_hiddens': [256, 256],\n",
      "                      'fcnet_weights_initializer': None,\n",
      "                      'fcnet_weights_initializer_config': None,\n",
      "                      'framestack': True,\n",
      "                      'free_log_std': False,\n",
      "                      'grayscale': False,\n",
      "                      'log_std_clip_param': 20.0,\n",
      "                      'lstm_bias_initializer': None,\n",
      "                      'lstm_bias_initializer_config': None,\n",
      "                      'lstm_cell_size': 256,\n",
      "                      'lstm_use_prev_action': False,\n",
      "                      'lstm_use_prev_action_reward': -1,\n",
      "                      'lstm_use_prev_reward': False,\n",
      "                      'lstm_weights_initializer': None,\n",
      "                      'lstm_weights_initializer_config': None,\n",
      "                      'max_seq_len': 20,\n",
      "                      'no_final_linear': False,\n",
      "                      'post_fcnet_activation': 'relu',\n",
      "                      'post_fcnet_bias_initializer': None,\n",
      "                      'post_fcnet_bias_initializer_config': None,\n",
      "                      'post_fcnet_hiddens': [],\n",
      "                      'post_fcnet_weights_initializer': None,\n",
      "                      'post_fcnet_weights_initializer_config': None,\n",
      "                      'use_attention': False,\n",
      "                      'use_lstm': False,\n",
      "                      'vf_share_layers': False,\n",
      "                      'zero_mean': True},\n",
      "            'normalize_actions': True,\n",
      "            'num_aggregator_actors_per_learner': 0,\n",
      "            'num_consecutive_env_runner_failures_tolerance': 100,\n",
      "            'num_cpus_for_main_process': 1,\n",
      "            'num_cpus_per_env_runner': 1,\n",
      "            'num_cpus_per_learner': 'auto',\n",
      "            'num_cpus_per_offline_eval_runner': 1,\n",
      "            'num_env_runners': 2,\n",
      "            'num_envs_per_env_runner': 1,\n",
      "            'num_epochs': 10,\n",
      "            'num_gpus': 0,\n",
      "            'num_gpus_per_env_runner': 0,\n",
      "            'num_gpus_per_learner': 0,\n",
      "            'num_gpus_per_offline_eval_runner': 0,\n",
      "            'num_learners': 0,\n",
      "            'num_offline_eval_runners': 0,\n",
      "            'observation_filter': 'NoFilter',\n",
      "            'observation_fn': None,\n",
      "            'observation_space': None,\n",
      "            'off_policy_estimation_methods': {},\n",
      "            'offline_data_class': None,\n",
      "            'offline_eval_batch_size_per_runner': 256,\n",
      "            'offline_eval_rl_module_inference_only': False,\n",
      "            'offline_eval_runner_class': None,\n",
      "            'offline_eval_runner_health_probe_timeout_s': 30.0,\n",
      "            'offline_eval_runner_restore_timeout_s': 1800.0,\n",
      "            'offline_evaluation_duration': 1,\n",
      "            'offline_evaluation_interval': None,\n",
      "            'offline_evaluation_parallel_to_training': False,\n",
      "            'offline_evaluation_timeout_s': 120.0,\n",
      "            'offline_evaluation_type': None,\n",
      "            'offline_loss_for_module_fn': None,\n",
      "            'offline_sampling': False,\n",
      "            'ope_split_batch_by_episode': True,\n",
      "            'optimizer': {},\n",
      "            'output': None,\n",
      "            'output_compress_columns': ['obs', 'new_obs'],\n",
      "            'output_config': {},\n",
      "            'output_filesystem': None,\n",
      "            'output_filesystem_kwargs': {},\n",
      "            'output_max_file_size': 67108864,\n",
      "            'output_max_rows_per_file': None,\n",
      "            'output_write_episodes': True,\n",
      "            'output_write_method': 'write_parquet',\n",
      "            'output_write_method_kwargs': {},\n",
      "            'output_write_remaining_data': False,\n",
      "            'placement_strategy': 'PACK',\n",
      "            'policies': {'default_policy': (None, None, None, None)},\n",
      "            'policies_to_train': None,\n",
      "            'policy_map_cache': -1,\n",
      "            'policy_map_capacity': 100,\n",
      "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x31a3ac900>,\n",
      "            'policy_states_are_swappable': False,\n",
      "            'postprocess_inputs': False,\n",
      "            'prelearner_buffer_class': None,\n",
      "            'prelearner_buffer_kwargs': {},\n",
      "            'prelearner_class': None,\n",
      "            'prelearner_module_synch_period': 10,\n",
      "            'preprocessor_pref': 'deepmind',\n",
      "            'remote_env_batch_wait_ms': 0,\n",
      "            'remote_worker_envs': False,\n",
      "            'render_env': False,\n",
      "            'replay_sequence_length': None,\n",
      "            'restart_failed_env_runners': True,\n",
      "            'restart_failed_offline_eval_runners': True,\n",
      "            'restart_failed_sub_environments': False,\n",
      "            'rollout_fragment_length': 'auto',\n",
      "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      "            'sample_timeout_s': 60.0,\n",
      "            'sampler_perf_stats_ema_coef': None,\n",
      "            'seed': None,\n",
      "            'sgd_minibatch_size': -1,\n",
      "            'shuffle_batch_per_epoch': True,\n",
      "            'shuffle_buffer_size': 0,\n",
      "            'simple_optimizer': False,\n",
      "            'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
      "            'synchronize_filters': -1,\n",
      "            'tf_session_args': {'allow_soft_placement': True,\n",
      "                                'device_count': {'CPU': 1},\n",
      "                                'gpu_options': {'allow_growth': True},\n",
      "                                'inter_op_parallelism_threads': 2,\n",
      "                                'intra_op_parallelism_threads': 2,\n",
      "                                'log_device_placement': False},\n",
      "            'torch_compile_learner': False,\n",
      "            'torch_compile_learner_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_learner_dynamo_mode': None,\n",
      "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
      "            'torch_compile_worker': False,\n",
      "            'torch_compile_worker_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_worker_dynamo_mode': None,\n",
      "            'torch_ddp_kwargs': {},\n",
      "            'torch_skip_nan_gradients': False,\n",
      "            'train_batch_size': 4000,\n",
      "            'update_worker_filter_stats': True,\n",
      "            'use_critic': True,\n",
      "            'use_gae': True,\n",
      "            'use_kl_loss': True,\n",
      "            'use_worker_filter_stats': True,\n",
      "            'validate_env_runners_after_construction': True,\n",
      "            'validate_offline_eval_runners_after_construction': True,\n",
      "            'vf_clip_param': 10.0,\n",
      "            'vf_loss_coeff': 1.0,\n",
      "            'vf_share_layers': -1,\n",
      "            'worker_cls': -1},\n",
      " 'date': '2025-10-06_15-48-44',\n",
      " 'done': False,\n",
      " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
      " 'env_runners': {'agent_episode_return_mean': {'default_agent': -1184.8258979722405},\n",
      "                 'env_reset_timer': 0.00019991699809907002,\n",
      "                 'env_step_timer': 2.2674225530529e-05,\n",
      "                 'env_to_module_connector': {'connector_pipeline_timer': 4.590080153738083e-05,\n",
      "                                             'timers': {'connectors': {'add_observations_from_episodes_to_batch': 2.902156636048052e-06,\n",
      "                                                                       'add_states_from_episodes_to_batch': 8.629063737646581e-07,\n",
      "                                                                       'add_time_dim_to_batch_and_zero_pad': 1.1842431393680584e-06,\n",
      "                                                                       'batch_individual_items': 6.355163714752784e-06,\n",
      "                                                                       'numpy_to_tensor': 1.0050889149406807e-05}}},\n",
      "                 'env_to_module_sum_episodes_length_in': 131.73765420741816,\n",
      "                 'env_to_module_sum_episodes_length_out': 131.73765420741816,\n",
      "                 'episode_duration_sec_mean': 0.05734378063352778,\n",
      "                 'episode_len_max': 200,\n",
      "                 'episode_len_mean': 200.0,\n",
      "                 'episode_len_min': 200,\n",
      "                 'episode_return_max': -765.5095064681235,\n",
      "                 'episode_return_mean': -1184.8258979722405,\n",
      "                 'episode_return_min': -1673.637579512455,\n",
      "                 'module_episode_return_mean': {'default_policy': -1184.8258979722405},\n",
      "                 'module_to_env_connector': {'connector_pipeline_timer': 0.0001237441628325641,\n",
      "                                             'timers': {'connectors': {'get_actions': 4.068912357406731e-05,\n",
      "                                                                       'listify_data_for_vector_env': 1.0072714291984201e-05,\n",
      "                                                                       'normalize_and_clip_actions': 1.8531327299793038e-05,\n",
      "                                                                       'remove_single_ts_time_rank_from_batch': 7.688605323405353e-07,\n",
      "                                                                       'tensor_to_numpy': 1.6450280733863978e-05,\n",
      "                                                                       'un_batch_to_individual_items': 6.258353082319887e-06}}},\n",
      "                 'num_agent_steps_sampled': {'default_agent': 2000.0},\n",
      "                 'num_agent_steps_sampled_lifetime': {'default_agent': 6000.0},\n",
      "                 'num_env_steps_sampled': 2000.0,\n",
      "                 'num_env_steps_sampled_lifetime': 6000.0,\n",
      "                 'num_env_steps_sampled_lifetime_throughput': 3337.235595383119,\n",
      "                 'num_episodes': 10.0,\n",
      "                 'num_episodes_lifetime': 30.0,\n",
      "                 'num_module_steps_sampled': {'default_policy': 2000.0},\n",
      "                 'num_module_steps_sampled_lifetime': {'default_policy': 6000.0},\n",
      "                 'rlmodule_inference_timer': 4.6050784102286526e-05,\n",
      "                 'sample': 0.29530767821395965,\n",
      "                 'time_between_sampling': 0.438852671413697,\n",
      "                 'weights_seq_no': 2.0},\n",
      " 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0},\n",
      " 'hostname': 'scadsdp25.misc.intern.uni-leipzig.de',\n",
      " 'iterations_since_restore': 3,\n",
      " 'learners': {'__all_modules__': {'learner_connector': {'connector_pipeline_timer': 0.04443346720002229,\n",
      "                                                        'timers': {'connectors': {'add_columns_from_episodes_to_train_batch': 0.01606593796669549,\n",
      "                                                                                  'add_observations_from_episodes_to_batch': 5.5562097711663224e-05,\n",
      "                                                                                  'add_one_ts_to_episodes_and_truncate': 0.0011380343590251868,\n",
      "                                                                                  'add_states_from_episodes_to_batch': 9.498516273743008e-06,\n",
      "                                                                                  'add_time_dim_to_batch_and_zero_pad': 1.8138574836484624e-05,\n",
      "                                                                                  'batch_individual_items': 0.014222820072720788,\n",
      "                                                                                  'general_advantage_estimation': 0.012602999062697198,\n",
      "                                                                                  'numpy_to_tensor': 0.00011124348761150031}}},\n",
      "                                  'learner_connector_sum_episodes_length_in': 2000.0,\n",
      "                                  'learner_connector_sum_episodes_length_out': 2010.0,\n",
      "                                  'num_env_steps_trained': 317580,\n",
      "                                  'num_env_steps_trained_lifetime': 952740,\n",
      "                                  'num_env_steps_trained_lifetime_throughput': 926711.760592015,\n",
      "                                  'num_module_steps_trained': 20224,\n",
      "                                  'num_module_steps_trained_lifetime': 60672,\n",
      "                                  'num_module_steps_trained_lifetime_throughput': 59015.251178497514,\n",
      "                                  'num_module_steps_trained_throughput': 59016.977002247266,\n",
      "                                  'num_non_trainable_parameters': 0,\n",
      "                                  'num_trainable_parameters': 134403},\n",
      "              'default_policy': {'curr_entropy_coeff': 0.0,\n",
      "                                 'curr_kl_coeff': 0.15000000596046448,\n",
      "                                 'default_optimizer_learning_rate': 0.0002,\n",
      "                                 'diff_num_grad_updates_vs_sampler_policy': 1.0,\n",
      "                                 'entropy': 1.4449276,\n",
      "                                 'gradients_default_optimizer_global_norm': 0.6816221,\n",
      "                                 'mean_kl_loss': 0.0048695453,\n",
      "                                 'module_train_batch_size_mean': 128.0,\n",
      "                                 'num_module_steps_trained': 20224,\n",
      "                                 'num_module_steps_trained_lifetime': 60672,\n",
      "                                 'num_module_steps_trained_lifetime_throughput': 59016.287785726614,\n",
      "                                 'num_trainable_parameters': 134403,\n",
      "                                 'policy_loss': -0.021522963,\n",
      "                                 'total_loss': 9.9799385,\n",
      "                                 'vf_explained_var': 0.00032699108,\n",
      "                                 'vf_loss': 10.0,\n",
      "                                 'vf_loss_unclipped': 139619.72,\n",
      "                                 'weights_seq_no': 3.0}},\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_env_steps_sampled_lifetime': 6000.0,\n",
      " 'num_training_step_calls_per_iteration': 1,\n",
      " 'perf': {'cpu_util_percent': 23.4, 'ram_util_percent': 68.5},\n",
      " 'pid': 29591,\n",
      " 'time_since_restore': 2.112711191177368,\n",
      " 'time_this_iter_s': 0.693932056427002,\n",
      " 'time_total_s': 2.112711191177368,\n",
      " 'timers': {'env_runner_sampling_timer': 0.3048960660445453,\n",
      "            'learner_update_timer': 0.41119451572005783,\n",
      "            'restore_env_runners': 7.06961502728518e-06,\n",
      "            'synch_env_connectors': 0.0028235929121728986,\n",
      "            'synch_weights': 0.0009709130871648086,\n",
      "            'training_iteration': 0.7173323917464907,\n",
      "            'training_step': 0.7172273825158073},\n",
      " 'timestamp': 1759758524,\n",
      " 'training_iteration': 3,\n",
      " 'trial_id': 'default'}\n",
      "{'config': {'_disable_action_flattening': False,\n",
      "            '_disable_execution_plan_api': -1,\n",
      "            '_disable_initialize_loss_from_dummy_batch': False,\n",
      "            '_disable_preprocessor_api': False,\n",
      "            '_dont_auto_sync_env_runner_states': False,\n",
      "            '_enable_rl_module_api': -1,\n",
      "            '_env_to_module_connector': None,\n",
      "            '_fake_gpus': False,\n",
      "            '_is_atari': None,\n",
      "            '_is_online': True,\n",
      "            '_learner_class': None,\n",
      "            '_learner_connector': None,\n",
      "            '_model_config': {},\n",
      "            '_module_to_env_connector': None,\n",
      "            '_per_module_overrides': {},\n",
      "            '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
      "            '_rl_module_spec': None,\n",
      "            '_tf_policy_handles_more_than_one_loss': False,\n",
      "            '_torch_grad_scaler_class': None,\n",
      "            '_torch_lr_scheduler_classes': None,\n",
      "            '_train_batch_size_per_learner': 2000,\n",
      "            '_use_msgpack_checkpoints': False,\n",
      "            '_validate_config': True,\n",
      "            'action_mask_key': 'action_mask',\n",
      "            'action_space': None,\n",
      "            'actions_in_input_normalized': False,\n",
      "            'add_default_connectors_to_env_to_module_pipeline': True,\n",
      "            'add_default_connectors_to_learner_pipeline': True,\n",
      "            'add_default_connectors_to_module_to_env_pipeline': True,\n",
      "            'algorithm_config_overrides_per_module': {},\n",
      "            'always_attach_evaluation_results': -1,\n",
      "            'auto_wrap_old_gym_envs': -1,\n",
      "            'batch_mode': 'truncate_episodes',\n",
      "            'broadcast_env_runner_states': True,\n",
      "            'broadcast_offline_eval_runner_states': False,\n",
      "            'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>,\n",
      "            'callbacks_on_algorithm_init': None,\n",
      "            'callbacks_on_checkpoint_loaded': None,\n",
      "            'callbacks_on_env_runners_recreated': None,\n",
      "            'callbacks_on_environment_created': None,\n",
      "            'callbacks_on_episode_created': None,\n",
      "            'callbacks_on_episode_end': None,\n",
      "            'callbacks_on_episode_start': None,\n",
      "            'callbacks_on_episode_step': None,\n",
      "            'callbacks_on_evaluate_end': None,\n",
      "            'callbacks_on_evaluate_offline_end': None,\n",
      "            'callbacks_on_evaluate_offline_start': None,\n",
      "            'callbacks_on_evaluate_start': None,\n",
      "            'callbacks_on_offline_eval_runners_recreated': None,\n",
      "            'callbacks_on_sample_end': None,\n",
      "            'callbacks_on_train_result': None,\n",
      "            'checkpoint_trainable_policies_only': False,\n",
      "            'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>,\n",
      "            'clip_actions': False,\n",
      "            'clip_param': 0.3,\n",
      "            'clip_rewards': None,\n",
      "            'compress_observations': False,\n",
      "            'count_steps_by': 'env_steps',\n",
      "            'create_env_on_driver': False,\n",
      "            'create_local_env_runner': True,\n",
      "            'custom_async_evaluation_function': -1,\n",
      "            'custom_eval_function': None,\n",
      "            'custom_resources_per_env_runner': {},\n",
      "            'custom_resources_per_offline_eval_runner': {},\n",
      "            'dataset_num_iters_per_eval_runner': 1,\n",
      "            'dataset_num_iters_per_learner': None,\n",
      "            'delay_between_env_runner_restarts_s': 60.0,\n",
      "            'disable_env_checking': False,\n",
      "            'eager_max_retraces': 20,\n",
      "            'eager_tracing': True,\n",
      "            'enable_async_evaluation': -1,\n",
      "            'enable_connectors': -1,\n",
      "            'enable_env_runner_and_connector_v2': True,\n",
      "            'enable_rl_module_and_learner': True,\n",
      "            'enable_tf1_exec_eagerly': False,\n",
      "            'entropy_coeff': 0.0,\n",
      "            'entropy_coeff_schedule': None,\n",
      "            'env': 'Pendulum-v1',\n",
      "            'env_config': {},\n",
      "            'env_runner_cls': None,\n",
      "            'env_runner_health_probe_timeout_s': 30.0,\n",
      "            'env_runner_restore_timeout_s': 1800.0,\n",
      "            'env_task_fn': -1,\n",
      "            'episode_lookback_horizon': 1,\n",
      "            'episodes_to_numpy': True,\n",
      "            'evaluation_auto_duration_max_env_steps_per_sample': 2000,\n",
      "            'evaluation_auto_duration_min_env_steps_per_sample': 100,\n",
      "            'evaluation_config': None,\n",
      "            'evaluation_duration': 10,\n",
      "            'evaluation_duration_unit': 'episodes',\n",
      "            'evaluation_force_reset_envs_before_iteration': True,\n",
      "            'evaluation_interval': None,\n",
      "            'evaluation_num_env_runners': 0,\n",
      "            'evaluation_parallel_to_training': False,\n",
      "            'evaluation_sample_timeout_s': 120.0,\n",
      "            'exploration_config': {},\n",
      "            'explore': True,\n",
      "            'export_native_model_files': False,\n",
      "            'extra_python_environs_for_driver': {},\n",
      "            'extra_python_environs_for_worker': {},\n",
      "            'fake_sampler': False,\n",
      "            'framework': 'torch',\n",
      "            'gamma': 0.99,\n",
      "            'grad_clip': None,\n",
      "            'grad_clip_by': 'global_norm',\n",
      "            'gym_env_vectorize_mode': 'SYNC',\n",
      "            'ignore_env_runner_failures': False,\n",
      "            'ignore_final_observation': False,\n",
      "            'ignore_offline_eval_runner_failures': False,\n",
      "            'in_evaluation': False,\n",
      "            'input': 'sampler',\n",
      "            'input_compress_columns': ['obs', 'new_obs'],\n",
      "            'input_config': {},\n",
      "            'input_filesystem': None,\n",
      "            'input_filesystem_kwargs': {},\n",
      "            'input_read_batch_size': None,\n",
      "            'input_read_episodes': False,\n",
      "            'input_read_method': 'read_parquet',\n",
      "            'input_read_method_kwargs': {},\n",
      "            'input_read_sample_batches': False,\n",
      "            'input_read_schema': {},\n",
      "            'input_spaces_jsonable': True,\n",
      "            'iter_batches_kwargs': {},\n",
      "            'keep_per_episode_custom_metrics': False,\n",
      "            'kl_coeff': 0.2,\n",
      "            'kl_target': 0.01,\n",
      "            'lambda': 1.0,\n",
      "            'learner_config_dict': {},\n",
      "            'local_gpu_idx': 0,\n",
      "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                                      'intra_op_parallelism_threads': 8},\n",
      "            'log_gradients': True,\n",
      "            'log_level': 'WARN',\n",
      "            'log_sys_usage': True,\n",
      "            'logger_config': None,\n",
      "            'logger_creator': None,\n",
      "            'lr': 0.0002,\n",
      "            'lr_schedule': None,\n",
      "            'map_batches_kwargs': {},\n",
      "            'materialize_data': False,\n",
      "            'materialize_mapped_data': True,\n",
      "            'max_num_env_runner_restarts': 1000,\n",
      "            'max_num_offline_eval_runner_restarts': 1000,\n",
      "            'max_requests_in_flight_per_aggregator_actor': 3,\n",
      "            'max_requests_in_flight_per_env_runner': 1,\n",
      "            'max_requests_in_flight_per_learner': 3,\n",
      "            'max_requests_in_flight_per_offline_eval_runner': 1,\n",
      "            'merge_env_runner_states': 'training_only',\n",
      "            'metrics_episode_collection_timeout_s': 60.0,\n",
      "            'metrics_num_episodes_for_smoothing': 100,\n",
      "            'min_sample_timesteps_per_iteration': 0,\n",
      "            'min_time_s_per_iteration': None,\n",
      "            'min_train_timesteps_per_iteration': 0,\n",
      "            'minibatch_size': 128,\n",
      "            'model': {'_disable_action_flattening': False,\n",
      "                      '_disable_preprocessor_api': False,\n",
      "                      '_time_major': False,\n",
      "                      '_use_default_native_models': -1,\n",
      "                      'always_check_shapes': False,\n",
      "                      'attention_dim': 64,\n",
      "                      'attention_head_dim': 32,\n",
      "                      'attention_init_gru_gate_bias': 2.0,\n",
      "                      'attention_memory_inference': 50,\n",
      "                      'attention_memory_training': 50,\n",
      "                      'attention_num_heads': 1,\n",
      "                      'attention_num_transformer_units': 1,\n",
      "                      'attention_position_wise_mlp_dim': 32,\n",
      "                      'attention_use_n_prev_actions': 0,\n",
      "                      'attention_use_n_prev_rewards': 0,\n",
      "                      'conv_activation': 'relu',\n",
      "                      'conv_bias_initializer': None,\n",
      "                      'conv_bias_initializer_config': None,\n",
      "                      'conv_filters': None,\n",
      "                      'conv_kernel_initializer': None,\n",
      "                      'conv_kernel_initializer_config': None,\n",
      "                      'conv_transpose_bias_initializer': None,\n",
      "                      'conv_transpose_bias_initializer_config': None,\n",
      "                      'conv_transpose_kernel_initializer': None,\n",
      "                      'conv_transpose_kernel_initializer_config': None,\n",
      "                      'custom_action_dist': None,\n",
      "                      'custom_model': None,\n",
      "                      'custom_model_config': {},\n",
      "                      'custom_preprocessor': None,\n",
      "                      'dim': 84,\n",
      "                      'encoder_latent_dim': None,\n",
      "                      'fcnet_activation': 'tanh',\n",
      "                      'fcnet_bias_initializer': None,\n",
      "                      'fcnet_bias_initializer_config': None,\n",
      "                      'fcnet_hiddens': [256, 256],\n",
      "                      'fcnet_weights_initializer': None,\n",
      "                      'fcnet_weights_initializer_config': None,\n",
      "                      'framestack': True,\n",
      "                      'free_log_std': False,\n",
      "                      'grayscale': False,\n",
      "                      'log_std_clip_param': 20.0,\n",
      "                      'lstm_bias_initializer': None,\n",
      "                      'lstm_bias_initializer_config': None,\n",
      "                      'lstm_cell_size': 256,\n",
      "                      'lstm_use_prev_action': False,\n",
      "                      'lstm_use_prev_action_reward': -1,\n",
      "                      'lstm_use_prev_reward': False,\n",
      "                      'lstm_weights_initializer': None,\n",
      "                      'lstm_weights_initializer_config': None,\n",
      "                      'max_seq_len': 20,\n",
      "                      'no_final_linear': False,\n",
      "                      'post_fcnet_activation': 'relu',\n",
      "                      'post_fcnet_bias_initializer': None,\n",
      "                      'post_fcnet_bias_initializer_config': None,\n",
      "                      'post_fcnet_hiddens': [],\n",
      "                      'post_fcnet_weights_initializer': None,\n",
      "                      'post_fcnet_weights_initializer_config': None,\n",
      "                      'use_attention': False,\n",
      "                      'use_lstm': False,\n",
      "                      'vf_share_layers': False,\n",
      "                      'zero_mean': True},\n",
      "            'normalize_actions': True,\n",
      "            'num_aggregator_actors_per_learner': 0,\n",
      "            'num_consecutive_env_runner_failures_tolerance': 100,\n",
      "            'num_cpus_for_main_process': 1,\n",
      "            'num_cpus_per_env_runner': 1,\n",
      "            'num_cpus_per_learner': 'auto',\n",
      "            'num_cpus_per_offline_eval_runner': 1,\n",
      "            'num_env_runners': 2,\n",
      "            'num_envs_per_env_runner': 1,\n",
      "            'num_epochs': 10,\n",
      "            'num_gpus': 0,\n",
      "            'num_gpus_per_env_runner': 0,\n",
      "            'num_gpus_per_learner': 0,\n",
      "            'num_gpus_per_offline_eval_runner': 0,\n",
      "            'num_learners': 0,\n",
      "            'num_offline_eval_runners': 0,\n",
      "            'observation_filter': 'NoFilter',\n",
      "            'observation_fn': None,\n",
      "            'observation_space': None,\n",
      "            'off_policy_estimation_methods': {},\n",
      "            'offline_data_class': None,\n",
      "            'offline_eval_batch_size_per_runner': 256,\n",
      "            'offline_eval_rl_module_inference_only': False,\n",
      "            'offline_eval_runner_class': None,\n",
      "            'offline_eval_runner_health_probe_timeout_s': 30.0,\n",
      "            'offline_eval_runner_restore_timeout_s': 1800.0,\n",
      "            'offline_evaluation_duration': 1,\n",
      "            'offline_evaluation_interval': None,\n",
      "            'offline_evaluation_parallel_to_training': False,\n",
      "            'offline_evaluation_timeout_s': 120.0,\n",
      "            'offline_evaluation_type': None,\n",
      "            'offline_loss_for_module_fn': None,\n",
      "            'offline_sampling': False,\n",
      "            'ope_split_batch_by_episode': True,\n",
      "            'optimizer': {},\n",
      "            'output': None,\n",
      "            'output_compress_columns': ['obs', 'new_obs'],\n",
      "            'output_config': {},\n",
      "            'output_filesystem': None,\n",
      "            'output_filesystem_kwargs': {},\n",
      "            'output_max_file_size': 67108864,\n",
      "            'output_max_rows_per_file': None,\n",
      "            'output_write_episodes': True,\n",
      "            'output_write_method': 'write_parquet',\n",
      "            'output_write_method_kwargs': {},\n",
      "            'output_write_remaining_data': False,\n",
      "            'placement_strategy': 'PACK',\n",
      "            'policies': {'default_policy': (None, None, None, None)},\n",
      "            'policies_to_train': None,\n",
      "            'policy_map_cache': -1,\n",
      "            'policy_map_capacity': 100,\n",
      "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x31a3ac900>,\n",
      "            'policy_states_are_swappable': False,\n",
      "            'postprocess_inputs': False,\n",
      "            'prelearner_buffer_class': None,\n",
      "            'prelearner_buffer_kwargs': {},\n",
      "            'prelearner_class': None,\n",
      "            'prelearner_module_synch_period': 10,\n",
      "            'preprocessor_pref': 'deepmind',\n",
      "            'remote_env_batch_wait_ms': 0,\n",
      "            'remote_worker_envs': False,\n",
      "            'render_env': False,\n",
      "            'replay_sequence_length': None,\n",
      "            'restart_failed_env_runners': True,\n",
      "            'restart_failed_offline_eval_runners': True,\n",
      "            'restart_failed_sub_environments': False,\n",
      "            'rollout_fragment_length': 'auto',\n",
      "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      "            'sample_timeout_s': 60.0,\n",
      "            'sampler_perf_stats_ema_coef': None,\n",
      "            'seed': None,\n",
      "            'sgd_minibatch_size': -1,\n",
      "            'shuffle_batch_per_epoch': True,\n",
      "            'shuffle_buffer_size': 0,\n",
      "            'simple_optimizer': False,\n",
      "            'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
      "            'synchronize_filters': -1,\n",
      "            'tf_session_args': {'allow_soft_placement': True,\n",
      "                                'device_count': {'CPU': 1},\n",
      "                                'gpu_options': {'allow_growth': True},\n",
      "                                'inter_op_parallelism_threads': 2,\n",
      "                                'intra_op_parallelism_threads': 2,\n",
      "                                'log_device_placement': False},\n",
      "            'torch_compile_learner': False,\n",
      "            'torch_compile_learner_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_learner_dynamo_mode': None,\n",
      "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
      "            'torch_compile_worker': False,\n",
      "            'torch_compile_worker_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_worker_dynamo_mode': None,\n",
      "            'torch_ddp_kwargs': {},\n",
      "            'torch_skip_nan_gradients': False,\n",
      "            'train_batch_size': 4000,\n",
      "            'update_worker_filter_stats': True,\n",
      "            'use_critic': True,\n",
      "            'use_gae': True,\n",
      "            'use_kl_loss': True,\n",
      "            'use_worker_filter_stats': True,\n",
      "            'validate_env_runners_after_construction': True,\n",
      "            'validate_offline_eval_runners_after_construction': True,\n",
      "            'vf_clip_param': 10.0,\n",
      "            'vf_loss_coeff': 1.0,\n",
      "            'vf_share_layers': -1,\n",
      "            'worker_cls': -1},\n",
      " 'date': '2025-10-06_15-48-45',\n",
      " 'done': False,\n",
      " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
      " 'env_runners': {'agent_episode_return_mean': {'default_agent': -1154.9387931199703},\n",
      "                 'env_reset_timer': 0.00019991699809907002,\n",
      "                 'env_step_timer': 2.3203235514757607e-05,\n",
      "                 'env_to_module_connector': {'connector_pipeline_timer': 4.573233043225842e-05,\n",
      "                                             'timers': {'connectors': {'add_observations_from_episodes_to_batch': 2.9593162575606145e-06,\n",
      "                                                                       'add_states_from_episodes_to_batch': 8.59505963925236e-07,\n",
      "                                                                       'add_time_dim_to_batch_and_zero_pad': 1.1043133445854445e-06,\n",
      "                                                                       'batch_individual_items': 6.311947821788291e-06,\n",
      "                                                                       'numpy_to_tensor': 9.770845998620042e-06}}},\n",
      "                 'env_to_module_sum_episodes_length_in': 131.73765420742725,\n",
      "                 'env_to_module_sum_episodes_length_out': 131.73765420742725,\n",
      "                 'episode_duration_sec_mean': 0.0572025427754852,\n",
      "                 'episode_len_max': 200,\n",
      "                 'episode_len_mean': 200.0,\n",
      "                 'episode_len_min': 200,\n",
      "                 'episode_return_max': -765.5095064681235,\n",
      "                 'episode_return_mean': -1154.9387931199703,\n",
      "                 'episode_return_min': -1673.637579512455,\n",
      "                 'module_episode_return_mean': {'default_policy': -1154.9387931199703},\n",
      "                 'module_to_env_connector': {'connector_pipeline_timer': 0.0001233726162020227,\n",
      "                                             'timers': {'connectors': {'get_actions': 3.972838883666753e-05,\n",
      "                                                                       'listify_data_for_vector_env': 1.0294802247908716e-05,\n",
      "                                                                       'normalize_and_clip_actions': 1.8778956333794784e-05,\n",
      "                                                                       'remove_single_ts_time_rank_from_batch': 8.096251421086578e-07,\n",
      "                                                                       'tensor_to_numpy': 1.6167199972856416e-05,\n",
      "                                                                       'un_batch_to_individual_items': 6.357897435208357e-06}}},\n",
      "                 'num_agent_steps_sampled': {'default_agent': 2000.0},\n",
      "                 'num_agent_steps_sampled_lifetime': {'default_agent': 8000.0},\n",
      "                 'num_env_steps_sampled': 2000.0,\n",
      "                 'num_env_steps_sampled_lifetime': 8000.0,\n",
      "                 'num_env_steps_sampled_lifetime_throughput': 3312.618023572245,\n",
      "                 'num_episodes': 10.0,\n",
      "                 'num_episodes_lifetime': 40.0,\n",
      "                 'num_module_steps_sampled': {'default_policy': 2000.0},\n",
      "                 'num_module_steps_sampled_lifetime': {'default_policy': 8000.0},\n",
      "                 'rlmodule_inference_timer': 4.516754526332415e-05,\n",
      "                 'sample': 0.29527624517681417,\n",
      "                 'time_between_sampling': 0.43859355573953585,\n",
      "                 'weights_seq_no': 3.0},\n",
      " 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0},\n",
      " 'hostname': 'scadsdp25.misc.intern.uni-leipzig.de',\n",
      " 'iterations_since_restore': 4,\n",
      " 'learners': {'__all_modules__': {'learner_connector': {'connector_pipeline_timer': 0.04433196752799606,\n",
      "                                                        'timers': {'connectors': {'add_columns_from_episodes_to_train_batch': 0.01606585942697575,\n",
      "                                                                                  'add_observations_from_episodes_to_batch': 5.5508136695265416e-05,\n",
      "                                                                                  'add_one_ts_to_episodes_and_truncate': 0.0011366506854130713,\n",
      "                                                                                  'add_states_from_episodes_to_batch': 9.429781165366876e-06,\n",
      "                                                                                  'add_time_dim_to_batch_and_zero_pad': 1.8039269093205805e-05,\n",
      "                                                                                  'batch_individual_items': 0.014220491461988036,\n",
      "                                                                                  'general_advantage_estimation': 0.012506901572108056,\n",
      "                                                                                  'numpy_to_tensor': 0.00011100605277694557}}},\n",
      "                                  'learner_connector_sum_episodes_length_in': 2000.0,\n",
      "                                  'learner_connector_sum_episodes_length_out': 2010.0,\n",
      "                                  'num_env_steps_trained': 317580,\n",
      "                                  'num_env_steps_trained_lifetime': 1270320,\n",
      "                                  'num_env_steps_trained_lifetime_throughput': 925600.4963607967,\n",
      "                                  'num_module_steps_trained': 20224,\n",
      "                                  'num_module_steps_trained_lifetime': 80896,\n",
      "                                  'num_module_steps_trained_lifetime_throughput': 58944.500651348426,\n",
      "                                  'num_module_steps_trained_throughput': 58946.23744207472,\n",
      "                                  'num_non_trainable_parameters': 0,\n",
      "                                  'num_trainable_parameters': 134403},\n",
      "              'default_policy': {'curr_entropy_coeff': 0.0,\n",
      "                                 'curr_kl_coeff': 0.15000000596046448,\n",
      "                                 'default_optimizer_learning_rate': 0.0002,\n",
      "                                 'diff_num_grad_updates_vs_sampler_policy': 1.0,\n",
      "                                 'entropy': 1.3492703,\n",
      "                                 'gradients_default_optimizer_global_norm': 1.4278545,\n",
      "                                 'mean_kl_loss': 0.010211778,\n",
      "                                 'module_train_batch_size_mean': 128.0,\n",
      "                                 'num_module_steps_trained': 20224,\n",
      "                                 'num_module_steps_trained_lifetime': 80896,\n",
      "                                 'num_module_steps_trained_lifetime_throughput': 58945.69887708817,\n",
      "                                 'num_trainable_parameters': 134403,\n",
      "                                 'policy_loss': -0.020234859,\n",
      "                                 'total_loss': 9.9072275,\n",
      "                                 'vf_explained_var': -0.00045323372,\n",
      "                                 'vf_loss': 9.925932,\n",
      "                                 'vf_loss_unclipped': 104895.78,\n",
      "                                 'weights_seq_no': 4.0}},\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_env_steps_sampled_lifetime': 8000.0,\n",
      " 'num_training_step_calls_per_iteration': 1,\n",
      " 'perf': {'cpu_util_percent': 24.3, 'ram_util_percent': 68.1},\n",
      " 'pid': 29591,\n",
      " 'time_since_restore': 2.8087921142578125,\n",
      " 'time_this_iter_s': 0.6960809230804443,\n",
      " 'time_total_s': 2.8087921142578125,\n",
      " 'timers': {'env_runner_sampling_timer': 0.30486211913407296,\n",
      "            'learner_update_timer': 0.4109707384828703,\n",
      "            'restore_env_runners': 7.058498856291407e-06,\n",
      "            'synch_env_connectors': 0.002822940312989522,\n",
      "            'synch_weights': 0.0009697577063058126,\n",
      "            'training_iteration': 0.7170746936589593,\n",
      "            'training_step': 0.7169692095206123},\n",
      " 'timestamp': 1759758525,\n",
      " 'training_iteration': 4,\n",
      " 'trial_id': 'default'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=29622)\u001b[0m 2025-10-06 15:52:29,720\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29623)\u001b[0m 2025-10-06 15:55:07,959\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29621)\u001b[0m 2025-10-06 15:56:41,198\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +10m23s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[33m(autoscaler +10m23s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +10m58s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +11m33s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +12m8s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +12m44s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +13m18s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +13m53s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +14m29s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +15m4s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +15m39s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +16m14s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +16m49s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +17m24s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +17m59s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +18m34s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +19m9s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +19m44s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +20m19s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +20m54s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +21m29s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +22m5s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +22m40s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +23m15s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=29871)\u001b[0m 2025-10-06 16:10:27,628\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +24m15s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +24m50s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +25m25s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +26m0s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +26m35s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +27m10s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +27m45s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +28m20s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +28m55s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +29m31s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +30m6s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +30m41s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +31m16s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +31m51s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +32m26s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +33m1s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +33m36s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +34m11s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +34m46s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "ppo = config.build_algo()\n",
    "for _ in range(4):\n",
    "    pprint(ppo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a207b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check policy for a trained and cached algorithm instance\n",
    "rl_module = ppo.get_module(\"default_policy\")  # Equivalent to `rl_module = ppo.get_module()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a103b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-10-06 15:52:31,433\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': {'_disable_action_flattening': False,\n",
      "            '_disable_execution_plan_api': -1,\n",
      "            '_disable_initialize_loss_from_dummy_batch': False,\n",
      "            '_disable_preprocessor_api': False,\n",
      "            '_dont_auto_sync_env_runner_states': False,\n",
      "            '_enable_rl_module_api': -1,\n",
      "            '_env_to_module_connector': None,\n",
      "            '_fake_gpus': False,\n",
      "            '_is_atari': None,\n",
      "            '_is_online': True,\n",
      "            '_learner_class': None,\n",
      "            '_learner_connector': None,\n",
      "            '_model_config': {},\n",
      "            '_module_to_env_connector': None,\n",
      "            '_per_module_overrides': {},\n",
      "            '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
      "            '_rl_module_spec': None,\n",
      "            '_tf_policy_handles_more_than_one_loss': False,\n",
      "            '_torch_grad_scaler_class': None,\n",
      "            '_torch_lr_scheduler_classes': None,\n",
      "            '_train_batch_size_per_learner': 2000,\n",
      "            '_use_msgpack_checkpoints': False,\n",
      "            '_validate_config': True,\n",
      "            'action_mask_key': 'action_mask',\n",
      "            'action_space': None,\n",
      "            'actions_in_input_normalized': False,\n",
      "            'add_default_connectors_to_env_to_module_pipeline': True,\n",
      "            'add_default_connectors_to_learner_pipeline': True,\n",
      "            'add_default_connectors_to_module_to_env_pipeline': True,\n",
      "            'algorithm_config_overrides_per_module': {},\n",
      "            'always_attach_evaluation_results': -1,\n",
      "            'auto_wrap_old_gym_envs': -1,\n",
      "            'batch_mode': 'truncate_episodes',\n",
      "            'broadcast_env_runner_states': True,\n",
      "            'broadcast_offline_eval_runner_states': False,\n",
      "            'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>,\n",
      "            'callbacks_on_algorithm_init': None,\n",
      "            'callbacks_on_checkpoint_loaded': None,\n",
      "            'callbacks_on_env_runners_recreated': None,\n",
      "            'callbacks_on_environment_created': None,\n",
      "            'callbacks_on_episode_created': None,\n",
      "            'callbacks_on_episode_end': None,\n",
      "            'callbacks_on_episode_start': None,\n",
      "            'callbacks_on_episode_step': None,\n",
      "            'callbacks_on_evaluate_end': None,\n",
      "            'callbacks_on_evaluate_offline_end': None,\n",
      "            'callbacks_on_evaluate_offline_start': None,\n",
      "            'callbacks_on_evaluate_start': None,\n",
      "            'callbacks_on_offline_eval_runners_recreated': None,\n",
      "            'callbacks_on_sample_end': None,\n",
      "            'callbacks_on_train_result': None,\n",
      "            'checkpoint_trainable_policies_only': False,\n",
      "            'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>,\n",
      "            'clip_actions': False,\n",
      "            'clip_param': 0.3,\n",
      "            'clip_rewards': None,\n",
      "            'compress_observations': False,\n",
      "            'count_steps_by': 'env_steps',\n",
      "            'create_env_on_driver': False,\n",
      "            'create_local_env_runner': True,\n",
      "            'custom_async_evaluation_function': -1,\n",
      "            'custom_eval_function': None,\n",
      "            'custom_resources_per_env_runner': {},\n",
      "            'custom_resources_per_offline_eval_runner': {},\n",
      "            'dataset_num_iters_per_eval_runner': 1,\n",
      "            'dataset_num_iters_per_learner': None,\n",
      "            'delay_between_env_runner_restarts_s': 60.0,\n",
      "            'disable_env_checking': False,\n",
      "            'eager_max_retraces': 20,\n",
      "            'eager_tracing': True,\n",
      "            'enable_async_evaluation': -1,\n",
      "            'enable_connectors': -1,\n",
      "            'enable_env_runner_and_connector_v2': True,\n",
      "            'enable_rl_module_and_learner': True,\n",
      "            'enable_tf1_exec_eagerly': False,\n",
      "            'entropy_coeff': 0.0,\n",
      "            'entropy_coeff_schedule': None,\n",
      "            'env': 'Pendulum-v1',\n",
      "            'env_config': {},\n",
      "            'env_runner_cls': None,\n",
      "            'env_runner_health_probe_timeout_s': 30.0,\n",
      "            'env_runner_restore_timeout_s': 1800.0,\n",
      "            'env_task_fn': -1,\n",
      "            'episode_lookback_horizon': 1,\n",
      "            'episodes_to_numpy': True,\n",
      "            'evaluation_auto_duration_max_env_steps_per_sample': 2000,\n",
      "            'evaluation_auto_duration_min_env_steps_per_sample': 100,\n",
      "            'evaluation_config': None,\n",
      "            'evaluation_duration': 10,\n",
      "            'evaluation_duration_unit': 'episodes',\n",
      "            'evaluation_force_reset_envs_before_iteration': True,\n",
      "            'evaluation_interval': 1,\n",
      "            'evaluation_num_env_runners': 2,\n",
      "            'evaluation_parallel_to_training': False,\n",
      "            'evaluation_sample_timeout_s': 120.0,\n",
      "            'exploration_config': {},\n",
      "            'explore': True,\n",
      "            'export_native_model_files': False,\n",
      "            'extra_python_environs_for_driver': {},\n",
      "            'extra_python_environs_for_worker': {},\n",
      "            'fake_sampler': False,\n",
      "            'framework': 'torch',\n",
      "            'gamma': 0.99,\n",
      "            'grad_clip': None,\n",
      "            'grad_clip_by': 'global_norm',\n",
      "            'gym_env_vectorize_mode': 'SYNC',\n",
      "            'ignore_env_runner_failures': False,\n",
      "            'ignore_final_observation': False,\n",
      "            'ignore_offline_eval_runner_failures': False,\n",
      "            'in_evaluation': False,\n",
      "            'input': 'sampler',\n",
      "            'input_compress_columns': ['obs', 'new_obs'],\n",
      "            'input_config': {},\n",
      "            'input_filesystem': None,\n",
      "            'input_filesystem_kwargs': {},\n",
      "            'input_read_batch_size': None,\n",
      "            'input_read_episodes': False,\n",
      "            'input_read_method': 'read_parquet',\n",
      "            'input_read_method_kwargs': {},\n",
      "            'input_read_sample_batches': False,\n",
      "            'input_read_schema': {},\n",
      "            'input_spaces_jsonable': True,\n",
      "            'iter_batches_kwargs': {},\n",
      "            'keep_per_episode_custom_metrics': False,\n",
      "            'kl_coeff': 0.2,\n",
      "            'kl_target': 0.01,\n",
      "            'lambda': 1.0,\n",
      "            'learner_config_dict': {},\n",
      "            'local_gpu_idx': 0,\n",
      "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                                      'intra_op_parallelism_threads': 8},\n",
      "            'log_gradients': True,\n",
      "            'log_level': 'WARN',\n",
      "            'log_sys_usage': True,\n",
      "            'logger_config': None,\n",
      "            'logger_creator': None,\n",
      "            'lr': 0.0002,\n",
      "            'lr_schedule': None,\n",
      "            'map_batches_kwargs': {},\n",
      "            'materialize_data': False,\n",
      "            'materialize_mapped_data': True,\n",
      "            'max_num_env_runner_restarts': 1000,\n",
      "            'max_num_offline_eval_runner_restarts': 1000,\n",
      "            'max_requests_in_flight_per_aggregator_actor': 3,\n",
      "            'max_requests_in_flight_per_env_runner': 1,\n",
      "            'max_requests_in_flight_per_learner': 3,\n",
      "            'max_requests_in_flight_per_offline_eval_runner': 1,\n",
      "            'merge_env_runner_states': 'training_only',\n",
      "            'metrics_episode_collection_timeout_s': 60.0,\n",
      "            'metrics_num_episodes_for_smoothing': 100,\n",
      "            'min_sample_timesteps_per_iteration': 0,\n",
      "            'min_time_s_per_iteration': None,\n",
      "            'min_train_timesteps_per_iteration': 0,\n",
      "            'minibatch_size': 128,\n",
      "            'model': {'_disable_action_flattening': False,\n",
      "                      '_disable_preprocessor_api': False,\n",
      "                      '_time_major': False,\n",
      "                      '_use_default_native_models': -1,\n",
      "                      'always_check_shapes': False,\n",
      "                      'attention_dim': 64,\n",
      "                      'attention_head_dim': 32,\n",
      "                      'attention_init_gru_gate_bias': 2.0,\n",
      "                      'attention_memory_inference': 50,\n",
      "                      'attention_memory_training': 50,\n",
      "                      'attention_num_heads': 1,\n",
      "                      'attention_num_transformer_units': 1,\n",
      "                      'attention_position_wise_mlp_dim': 32,\n",
      "                      'attention_use_n_prev_actions': 0,\n",
      "                      'attention_use_n_prev_rewards': 0,\n",
      "                      'conv_activation': 'relu',\n",
      "                      'conv_bias_initializer': None,\n",
      "                      'conv_bias_initializer_config': None,\n",
      "                      'conv_filters': None,\n",
      "                      'conv_kernel_initializer': None,\n",
      "                      'conv_kernel_initializer_config': None,\n",
      "                      'conv_transpose_bias_initializer': None,\n",
      "                      'conv_transpose_bias_initializer_config': None,\n",
      "                      'conv_transpose_kernel_initializer': None,\n",
      "                      'conv_transpose_kernel_initializer_config': None,\n",
      "                      'custom_action_dist': None,\n",
      "                      'custom_model': None,\n",
      "                      'custom_model_config': {},\n",
      "                      'custom_preprocessor': None,\n",
      "                      'dim': 84,\n",
      "                      'encoder_latent_dim': None,\n",
      "                      'fcnet_activation': 'tanh',\n",
      "                      'fcnet_bias_initializer': None,\n",
      "                      'fcnet_bias_initializer_config': None,\n",
      "                      'fcnet_hiddens': [256, 256],\n",
      "                      'fcnet_weights_initializer': None,\n",
      "                      'fcnet_weights_initializer_config': None,\n",
      "                      'framestack': True,\n",
      "                      'free_log_std': False,\n",
      "                      'grayscale': False,\n",
      "                      'log_std_clip_param': 20.0,\n",
      "                      'lstm_bias_initializer': None,\n",
      "                      'lstm_bias_initializer_config': None,\n",
      "                      'lstm_cell_size': 256,\n",
      "                      'lstm_use_prev_action': False,\n",
      "                      'lstm_use_prev_action_reward': -1,\n",
      "                      'lstm_use_prev_reward': False,\n",
      "                      'lstm_weights_initializer': None,\n",
      "                      'lstm_weights_initializer_config': None,\n",
      "                      'max_seq_len': 20,\n",
      "                      'no_final_linear': False,\n",
      "                      'post_fcnet_activation': 'relu',\n",
      "                      'post_fcnet_bias_initializer': None,\n",
      "                      'post_fcnet_bias_initializer_config': None,\n",
      "                      'post_fcnet_hiddens': [],\n",
      "                      'post_fcnet_weights_initializer': None,\n",
      "                      'post_fcnet_weights_initializer_config': None,\n",
      "                      'use_attention': False,\n",
      "                      'use_lstm': False,\n",
      "                      'vf_share_layers': False,\n",
      "                      'zero_mean': True},\n",
      "            'normalize_actions': True,\n",
      "            'num_aggregator_actors_per_learner': 0,\n",
      "            'num_consecutive_env_runner_failures_tolerance': 100,\n",
      "            'num_cpus_for_main_process': 1,\n",
      "            'num_cpus_per_env_runner': 1,\n",
      "            'num_cpus_per_learner': 'auto',\n",
      "            'num_cpus_per_offline_eval_runner': 1,\n",
      "            'num_env_runners': 2,\n",
      "            'num_envs_per_env_runner': 1,\n",
      "            'num_epochs': 10,\n",
      "            'num_gpus': 0,\n",
      "            'num_gpus_per_env_runner': 0,\n",
      "            'num_gpus_per_learner': 0,\n",
      "            'num_gpus_per_offline_eval_runner': 0,\n",
      "            'num_learners': 0,\n",
      "            'num_offline_eval_runners': 0,\n",
      "            'observation_filter': 'NoFilter',\n",
      "            'observation_fn': None,\n",
      "            'observation_space': None,\n",
      "            'off_policy_estimation_methods': {},\n",
      "            'offline_data_class': None,\n",
      "            'offline_eval_batch_size_per_runner': 256,\n",
      "            'offline_eval_rl_module_inference_only': False,\n",
      "            'offline_eval_runner_class': None,\n",
      "            'offline_eval_runner_health_probe_timeout_s': 30.0,\n",
      "            'offline_eval_runner_restore_timeout_s': 1800.0,\n",
      "            'offline_evaluation_duration': 1,\n",
      "            'offline_evaluation_interval': None,\n",
      "            'offline_evaluation_parallel_to_training': False,\n",
      "            'offline_evaluation_timeout_s': 120.0,\n",
      "            'offline_evaluation_type': None,\n",
      "            'offline_loss_for_module_fn': None,\n",
      "            'offline_sampling': False,\n",
      "            'ope_split_batch_by_episode': True,\n",
      "            'optimizer': {},\n",
      "            'output': None,\n",
      "            'output_compress_columns': ['obs', 'new_obs'],\n",
      "            'output_config': {},\n",
      "            'output_filesystem': None,\n",
      "            'output_filesystem_kwargs': {},\n",
      "            'output_max_file_size': 67108864,\n",
      "            'output_max_rows_per_file': None,\n",
      "            'output_write_episodes': True,\n",
      "            'output_write_method': 'write_parquet',\n",
      "            'output_write_method_kwargs': {},\n",
      "            'output_write_remaining_data': False,\n",
      "            'placement_strategy': 'PACK',\n",
      "            'policies': {'default_policy': (None, None, None, None)},\n",
      "            'policies_to_train': None,\n",
      "            'policy_map_cache': -1,\n",
      "            'policy_map_capacity': 100,\n",
      "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x31a3ac900>,\n",
      "            'policy_states_are_swappable': False,\n",
      "            'postprocess_inputs': False,\n",
      "            'prelearner_buffer_class': None,\n",
      "            'prelearner_buffer_kwargs': {},\n",
      "            'prelearner_class': None,\n",
      "            'prelearner_module_synch_period': 10,\n",
      "            'preprocessor_pref': 'deepmind',\n",
      "            'remote_env_batch_wait_ms': 0,\n",
      "            'remote_worker_envs': False,\n",
      "            'render_env': False,\n",
      "            'replay_sequence_length': None,\n",
      "            'restart_failed_env_runners': True,\n",
      "            'restart_failed_offline_eval_runners': True,\n",
      "            'restart_failed_sub_environments': False,\n",
      "            'rollout_fragment_length': 'auto',\n",
      "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      "            'sample_timeout_s': 60.0,\n",
      "            'sampler_perf_stats_ema_coef': None,\n",
      "            'seed': None,\n",
      "            'sgd_minibatch_size': -1,\n",
      "            'shuffle_batch_per_epoch': True,\n",
      "            'shuffle_buffer_size': 0,\n",
      "            'simple_optimizer': False,\n",
      "            'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
      "            'synchronize_filters': -1,\n",
      "            'tf_session_args': {'allow_soft_placement': True,\n",
      "                                'device_count': {'CPU': 1},\n",
      "                                'gpu_options': {'allow_growth': True},\n",
      "                                'inter_op_parallelism_threads': 2,\n",
      "                                'intra_op_parallelism_threads': 2,\n",
      "                                'log_device_placement': False},\n",
      "            'torch_compile_learner': False,\n",
      "            'torch_compile_learner_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_learner_dynamo_mode': None,\n",
      "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
      "            'torch_compile_worker': False,\n",
      "            'torch_compile_worker_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_worker_dynamo_mode': None,\n",
      "            'torch_ddp_kwargs': {},\n",
      "            'torch_skip_nan_gradients': False,\n",
      "            'train_batch_size': 4000,\n",
      "            'update_worker_filter_stats': True,\n",
      "            'use_critic': True,\n",
      "            'use_gae': True,\n",
      "            'use_kl_loss': True,\n",
      "            'use_worker_filter_stats': True,\n",
      "            'validate_env_runners_after_construction': True,\n",
      "            'validate_offline_eval_runners_after_construction': True,\n",
      "            'vf_clip_param': 10.0,\n",
      "            'vf_loss_coeff': 1.0,\n",
      "            'vf_share_layers': -1,\n",
      "            'worker_cls': -1},\n",
      " 'date': '2025-10-06_15-52-32',\n",
      " 'done': False,\n",
      " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
      " 'env_runners': {'agent_episode_return_mean': {'default_agent': -1626.75288058455},\n",
      "                 'env_reset_timer': 0.00019141699885949492,\n",
      "                 'env_step_timer': 2.2262256947891635e-05,\n",
      "                 'env_to_module_connector': {'connector_pipeline_timer': 4.5870806401882e-05,\n",
      "                                             'timers': {'connectors': {'add_observations_from_episodes_to_batch': 2.822262658447174e-06,\n",
      "                                                                       'add_states_from_episodes_to_batch': 8.369753176274436e-07,\n",
      "                                                                       'add_time_dim_to_batch_and_zero_pad': 1.0605674792796381e-06,\n",
      "                                                                       'batch_individual_items': 6.264927857131672e-06,\n",
      "                                                                       'numpy_to_tensor': 1.0038004026702443e-05}}},\n",
      "                 'env_to_module_sum_episodes_length_in': 131.7322456618175,\n",
      "                 'env_to_module_sum_episodes_length_out': 131.7322456618175,\n",
      "                 'episode_duration_sec_mean': 0.05726077069994062,\n",
      "                 'episode_len_max': 200,\n",
      "                 'episode_len_mean': 200.0,\n",
      "                 'episode_len_min': 200,\n",
      "                 'episode_return_max': -1136.9234806319132,\n",
      "                 'episode_return_mean': -1626.75288058455,\n",
      "                 'episode_return_min': -1836.022455251069,\n",
      "                 'module_episode_return_mean': {'default_policy': -1626.75288058455},\n",
      "                 'module_to_env_connector': {'connector_pipeline_timer': 0.00012362412725283036,\n",
      "                                             'timers': {'connectors': {'get_actions': 4.062148283000635e-05,\n",
      "                                                                       'listify_data_for_vector_env': 9.954558787084122e-06,\n",
      "                                                                       'normalize_and_clip_actions': 1.8509444750876808e-05,\n",
      "                                                                       'remove_single_ts_time_rank_from_batch': 7.740465300746552e-07,\n",
      "                                                                       'tensor_to_numpy': 1.6350285942417853e-05,\n",
      "                                                                       'un_batch_to_individual_items': 6.245486042166262e-06}}},\n",
      "                 'num_agent_steps_sampled': {'default_agent': 2000.0},\n",
      "                 'num_agent_steps_sampled_lifetime': {'default_agent': 2000.0},\n",
      "                 'num_env_steps_sampled': 2000.0,\n",
      "                 'num_env_steps_sampled_lifetime': 2000.0,\n",
      "                 'num_env_steps_sampled_lifetime_throughput': 3364.543572687573,\n",
      "                 'num_episodes': 10.0,\n",
      "                 'num_episodes_lifetime': 10.0,\n",
      "                 'num_module_steps_sampled': {'default_policy': 2000.0},\n",
      "                 'num_module_steps_sampled_lifetime': {'default_policy': 2000.0},\n",
      "                 'rlmodule_inference_timer': 4.611820835982922e-05,\n",
      "                 'sample': 0.294552104001923,\n",
      "                 'weights_seq_no': 0.0},\n",
      " 'evaluation': {'actor_manager_num_outstanding_async_reqs': 0,\n",
      "                'env_runners': {'agent_episode_return_mean': {'default_agent': -1461.4299219014943},\n",
      "                                'env_reset_timer': 0.0002958544973807875,\n",
      "                                'env_step_timer': 2.3104501910671368e-05,\n",
      "                                'env_to_module_connector': {'connector_pipeline_timer': 4.5772168294537724e-05,\n",
      "                                                            'timers': {'connectors': {'add_observations_from_episodes_to_batch': 2.9228054587588275e-06,\n",
      "                                                                                      'add_states_from_episodes_to_batch': 8.478918674299853e-07,\n",
      "                                                                                      'add_time_dim_to_batch_and_zero_pad': 1.0490845652433003e-06,\n",
      "                                                                                      'batch_individual_items': 6.28337821743848e-06,\n",
      "                                                                                      'numpy_to_tensor': 9.846136195482955e-06}}},\n",
      "                                'env_to_module_sum_episodes_length_in': 131.7322456618175,\n",
      "                                'env_to_module_sum_episodes_length_out': 131.7322456618175,\n",
      "                                'episode_duration_sec_mean': 0.05680720399686834,\n",
      "                                'episode_len_max': 200,\n",
      "                                'episode_len_mean': 200.0,\n",
      "                                'episode_len_min': 200,\n",
      "                                'episode_return_max': -1058.242003723863,\n",
      "                                'episode_return_mean': -1461.4299219014943,\n",
      "                                'episode_return_min': -1675.4648240159163,\n",
      "                                'module_episode_return_mean': {'default_policy': -1461.4299219014943},\n",
      "                                'module_to_env_connector': {'connector_pipeline_timer': 0.00012370557554615687,\n",
      "                                                            'timers': {'connectors': {'get_actions': 3.980668605922414e-05,\n",
      "                                                                                      'listify_data_for_vector_env': 1.0304592815940351e-05,\n",
      "                                                                                      'normalize_and_clip_actions': 1.849841310757986e-05,\n",
      "                                                                                      'remove_single_ts_time_rank_from_batch': 7.777302240797145e-07,\n",
      "                                                                                      'tensor_to_numpy': 1.6742540870826916e-05,\n",
      "                                                                                      'un_batch_to_individual_items': 6.308726446763263e-06}}},\n",
      "                                'num_agent_steps_sampled': {'default_agent': 2000.0},\n",
      "                                'num_agent_steps_sampled_lifetime': {'default_agent': 2000.0},\n",
      "                                'num_env_steps_sampled': 2000.0,\n",
      "                                'num_env_steps_sampled_lifetime': 2000.0,\n",
      "                                'num_episodes': 10.0,\n",
      "                                'num_episodes_lifetime': 10.0,\n",
      "                                'num_module_steps_sampled': {'default_policy': 2000.0},\n",
      "                                'num_module_steps_sampled_lifetime': {'default_policy': 2000.0},\n",
      "                                'rlmodule_inference_timer': 4.536000675795319e-05,\n",
      "                                'sample': 0.29238654200162273,\n",
      "                                'weights_seq_no': 1.0},\n",
      "                'num_healthy_workers': 2,\n",
      "                'num_remote_worker_restarts': 0},\n",
      " 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0},\n",
      " 'hostname': 'scadsdp25.misc.intern.uni-leipzig.de',\n",
      " 'iterations_since_restore': 1,\n",
      " 'learners': {'__all_modules__': {'learner_connector': {'connector_pipeline_timer': 0.04188595800223993,\n",
      "                                                        'timers': {'connectors': {'add_columns_from_episodes_to_train_batch': 0.01583812500030035,\n",
      "                                                                                  'add_observations_from_episodes_to_batch': 5.2041999879293144e-05,\n",
      "                                                                                  'add_one_ts_to_episodes_and_truncate': 0.0009973749984055758,\n",
      "                                                                                  'add_states_from_episodes_to_batch': 3.1669987947680056e-06,\n",
      "                                                                                  'add_time_dim_to_batch_and_zero_pad': 1.1624993931036443e-05,\n",
      "                                                                                  'batch_individual_items': 0.014034458996320609,\n",
      "                                                                                  'general_advantage_estimation': 0.010716040997067466,\n",
      "                                                                                  'numpy_to_tensor': 9.183300426229835e-05}}},\n",
      "                                  'learner_connector_sum_episodes_length_in': 2000,\n",
      "                                  'learner_connector_sum_episodes_length_out': 2010,\n",
      "                                  'num_env_steps_trained': 317580,\n",
      "                                  'num_env_steps_trained_lifetime': 317580,\n",
      "                                  'num_env_steps_trained_lifetime_throughput': 914334.773855888,\n",
      "                                  'num_module_steps_trained': 20224,\n",
      "                                  'num_module_steps_trained_lifetime': 20224,\n",
      "                                  'num_module_steps_trained_lifetime_throughput': 58224.803319961306,\n",
      "                                  'num_module_steps_trained_throughput': 58224.81659713562,\n",
      "                                  'num_non_trainable_parameters': 0,\n",
      "                                  'num_trainable_parameters': 134403},\n",
      "              'default_policy': {'curr_entropy_coeff': 0.0,\n",
      "                                 'curr_kl_coeff': 0.20000000298023224,\n",
      "                                 'default_optimizer_learning_rate': 0.0002,\n",
      "                                 'diff_num_grad_updates_vs_sampler_policy': 1.0,\n",
      "                                 'entropy': 1.383368,\n",
      "                                 'gradients_default_optimizer_global_norm': 0.92793554,\n",
      "                                 'mean_kl_loss': 0.011569772,\n",
      "                                 'module_train_batch_size_mean': 128.0,\n",
      "                                 'num_module_steps_trained': 20224,\n",
      "                                 'num_module_steps_trained_lifetime': 20224,\n",
      "                                 'num_module_steps_trained_lifetime_throughput': 58223.75103022939,\n",
      "                                 'num_trainable_parameters': 134403,\n",
      "                                 'policy_loss': 0.1490705,\n",
      "                                 'total_loss': 10.151384,\n",
      "                                 'vf_explained_var': -7.259846e-05,\n",
      "                                 'vf_loss': 10.0,\n",
      "                                 'vf_loss_unclipped': 299967.2,\n",
      "                                 'weights_seq_no': 1.0}},\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_env_steps_sampled_lifetime': 2000.0,\n",
      " 'num_training_step_calls_per_iteration': 1,\n",
      " 'perf': {'cpu_util_percent': 11.4, 'ram_util_percent': 70.15},\n",
      " 'pid': 29591,\n",
      " 'time_since_restore': 1.0158488750457764,\n",
      " 'time_this_iter_s': 1.0158488750457764,\n",
      " 'time_total_s': 1.0158488750457764,\n",
      " 'timers': {'env_runner_sampling_timer': 0.30430458299815655,\n",
      "            'learner_update_timer': 0.39584216700313846,\n",
      "            'restore_env_runners': 5.166999471839517e-06,\n",
      "            'synch_weights': 0.0009642500008339994,\n",
      "            'training_iteration': 0.7013571670031524,\n",
      "            'training_step': 0.7012678749961196},\n",
      " 'timestamp': 1759758752,\n",
      " 'training_iteration': 1,\n",
      " 'trial_id': 'default'}\n",
      "{'config': {'_disable_action_flattening': False,\n",
      "            '_disable_execution_plan_api': -1,\n",
      "            '_disable_initialize_loss_from_dummy_batch': False,\n",
      "            '_disable_preprocessor_api': False,\n",
      "            '_dont_auto_sync_env_runner_states': False,\n",
      "            '_enable_rl_module_api': -1,\n",
      "            '_env_to_module_connector': None,\n",
      "            '_fake_gpus': False,\n",
      "            '_is_atari': None,\n",
      "            '_is_online': True,\n",
      "            '_learner_class': None,\n",
      "            '_learner_connector': None,\n",
      "            '_model_config': {},\n",
      "            '_module_to_env_connector': None,\n",
      "            '_per_module_overrides': {},\n",
      "            '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
      "            '_rl_module_spec': None,\n",
      "            '_tf_policy_handles_more_than_one_loss': False,\n",
      "            '_torch_grad_scaler_class': None,\n",
      "            '_torch_lr_scheduler_classes': None,\n",
      "            '_train_batch_size_per_learner': 2000,\n",
      "            '_use_msgpack_checkpoints': False,\n",
      "            '_validate_config': True,\n",
      "            'action_mask_key': 'action_mask',\n",
      "            'action_space': None,\n",
      "            'actions_in_input_normalized': False,\n",
      "            'add_default_connectors_to_env_to_module_pipeline': True,\n",
      "            'add_default_connectors_to_learner_pipeline': True,\n",
      "            'add_default_connectors_to_module_to_env_pipeline': True,\n",
      "            'algorithm_config_overrides_per_module': {},\n",
      "            'always_attach_evaluation_results': -1,\n",
      "            'auto_wrap_old_gym_envs': -1,\n",
      "            'batch_mode': 'truncate_episodes',\n",
      "            'broadcast_env_runner_states': True,\n",
      "            'broadcast_offline_eval_runner_states': False,\n",
      "            'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>,\n",
      "            'callbacks_on_algorithm_init': None,\n",
      "            'callbacks_on_checkpoint_loaded': None,\n",
      "            'callbacks_on_env_runners_recreated': None,\n",
      "            'callbacks_on_environment_created': None,\n",
      "            'callbacks_on_episode_created': None,\n",
      "            'callbacks_on_episode_end': None,\n",
      "            'callbacks_on_episode_start': None,\n",
      "            'callbacks_on_episode_step': None,\n",
      "            'callbacks_on_evaluate_end': None,\n",
      "            'callbacks_on_evaluate_offline_end': None,\n",
      "            'callbacks_on_evaluate_offline_start': None,\n",
      "            'callbacks_on_evaluate_start': None,\n",
      "            'callbacks_on_offline_eval_runners_recreated': None,\n",
      "            'callbacks_on_sample_end': None,\n",
      "            'callbacks_on_train_result': None,\n",
      "            'checkpoint_trainable_policies_only': False,\n",
      "            'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>,\n",
      "            'clip_actions': False,\n",
      "            'clip_param': 0.3,\n",
      "            'clip_rewards': None,\n",
      "            'compress_observations': False,\n",
      "            'count_steps_by': 'env_steps',\n",
      "            'create_env_on_driver': False,\n",
      "            'create_local_env_runner': True,\n",
      "            'custom_async_evaluation_function': -1,\n",
      "            'custom_eval_function': None,\n",
      "            'custom_resources_per_env_runner': {},\n",
      "            'custom_resources_per_offline_eval_runner': {},\n",
      "            'dataset_num_iters_per_eval_runner': 1,\n",
      "            'dataset_num_iters_per_learner': None,\n",
      "            'delay_between_env_runner_restarts_s': 60.0,\n",
      "            'disable_env_checking': False,\n",
      "            'eager_max_retraces': 20,\n",
      "            'eager_tracing': True,\n",
      "            'enable_async_evaluation': -1,\n",
      "            'enable_connectors': -1,\n",
      "            'enable_env_runner_and_connector_v2': True,\n",
      "            'enable_rl_module_and_learner': True,\n",
      "            'enable_tf1_exec_eagerly': False,\n",
      "            'entropy_coeff': 0.0,\n",
      "            'entropy_coeff_schedule': None,\n",
      "            'env': 'Pendulum-v1',\n",
      "            'env_config': {},\n",
      "            'env_runner_cls': None,\n",
      "            'env_runner_health_probe_timeout_s': 30.0,\n",
      "            'env_runner_restore_timeout_s': 1800.0,\n",
      "            'env_task_fn': -1,\n",
      "            'episode_lookback_horizon': 1,\n",
      "            'episodes_to_numpy': True,\n",
      "            'evaluation_auto_duration_max_env_steps_per_sample': 2000,\n",
      "            'evaluation_auto_duration_min_env_steps_per_sample': 100,\n",
      "            'evaluation_config': None,\n",
      "            'evaluation_duration': 10,\n",
      "            'evaluation_duration_unit': 'episodes',\n",
      "            'evaluation_force_reset_envs_before_iteration': True,\n",
      "            'evaluation_interval': 1,\n",
      "            'evaluation_num_env_runners': 2,\n",
      "            'evaluation_parallel_to_training': False,\n",
      "            'evaluation_sample_timeout_s': 120.0,\n",
      "            'exploration_config': {},\n",
      "            'explore': True,\n",
      "            'export_native_model_files': False,\n",
      "            'extra_python_environs_for_driver': {},\n",
      "            'extra_python_environs_for_worker': {},\n",
      "            'fake_sampler': False,\n",
      "            'framework': 'torch',\n",
      "            'gamma': 0.99,\n",
      "            'grad_clip': None,\n",
      "            'grad_clip_by': 'global_norm',\n",
      "            'gym_env_vectorize_mode': 'SYNC',\n",
      "            'ignore_env_runner_failures': False,\n",
      "            'ignore_final_observation': False,\n",
      "            'ignore_offline_eval_runner_failures': False,\n",
      "            'in_evaluation': False,\n",
      "            'input': 'sampler',\n",
      "            'input_compress_columns': ['obs', 'new_obs'],\n",
      "            'input_config': {},\n",
      "            'input_filesystem': None,\n",
      "            'input_filesystem_kwargs': {},\n",
      "            'input_read_batch_size': None,\n",
      "            'input_read_episodes': False,\n",
      "            'input_read_method': 'read_parquet',\n",
      "            'input_read_method_kwargs': {},\n",
      "            'input_read_sample_batches': False,\n",
      "            'input_read_schema': {},\n",
      "            'input_spaces_jsonable': True,\n",
      "            'iter_batches_kwargs': {},\n",
      "            'keep_per_episode_custom_metrics': False,\n",
      "            'kl_coeff': 0.2,\n",
      "            'kl_target': 0.01,\n",
      "            'lambda': 1.0,\n",
      "            'learner_config_dict': {},\n",
      "            'local_gpu_idx': 0,\n",
      "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                                      'intra_op_parallelism_threads': 8},\n",
      "            'log_gradients': True,\n",
      "            'log_level': 'WARN',\n",
      "            'log_sys_usage': True,\n",
      "            'logger_config': None,\n",
      "            'logger_creator': None,\n",
      "            'lr': 0.0002,\n",
      "            'lr_schedule': None,\n",
      "            'map_batches_kwargs': {},\n",
      "            'materialize_data': False,\n",
      "            'materialize_mapped_data': True,\n",
      "            'max_num_env_runner_restarts': 1000,\n",
      "            'max_num_offline_eval_runner_restarts': 1000,\n",
      "            'max_requests_in_flight_per_aggregator_actor': 3,\n",
      "            'max_requests_in_flight_per_env_runner': 1,\n",
      "            'max_requests_in_flight_per_learner': 3,\n",
      "            'max_requests_in_flight_per_offline_eval_runner': 1,\n",
      "            'merge_env_runner_states': 'training_only',\n",
      "            'metrics_episode_collection_timeout_s': 60.0,\n",
      "            'metrics_num_episodes_for_smoothing': 100,\n",
      "            'min_sample_timesteps_per_iteration': 0,\n",
      "            'min_time_s_per_iteration': None,\n",
      "            'min_train_timesteps_per_iteration': 0,\n",
      "            'minibatch_size': 128,\n",
      "            'model': {'_disable_action_flattening': False,\n",
      "                      '_disable_preprocessor_api': False,\n",
      "                      '_time_major': False,\n",
      "                      '_use_default_native_models': -1,\n",
      "                      'always_check_shapes': False,\n",
      "                      'attention_dim': 64,\n",
      "                      'attention_head_dim': 32,\n",
      "                      'attention_init_gru_gate_bias': 2.0,\n",
      "                      'attention_memory_inference': 50,\n",
      "                      'attention_memory_training': 50,\n",
      "                      'attention_num_heads': 1,\n",
      "                      'attention_num_transformer_units': 1,\n",
      "                      'attention_position_wise_mlp_dim': 32,\n",
      "                      'attention_use_n_prev_actions': 0,\n",
      "                      'attention_use_n_prev_rewards': 0,\n",
      "                      'conv_activation': 'relu',\n",
      "                      'conv_bias_initializer': None,\n",
      "                      'conv_bias_initializer_config': None,\n",
      "                      'conv_filters': None,\n",
      "                      'conv_kernel_initializer': None,\n",
      "                      'conv_kernel_initializer_config': None,\n",
      "                      'conv_transpose_bias_initializer': None,\n",
      "                      'conv_transpose_bias_initializer_config': None,\n",
      "                      'conv_transpose_kernel_initializer': None,\n",
      "                      'conv_transpose_kernel_initializer_config': None,\n",
      "                      'custom_action_dist': None,\n",
      "                      'custom_model': None,\n",
      "                      'custom_model_config': {},\n",
      "                      'custom_preprocessor': None,\n",
      "                      'dim': 84,\n",
      "                      'encoder_latent_dim': None,\n",
      "                      'fcnet_activation': 'tanh',\n",
      "                      'fcnet_bias_initializer': None,\n",
      "                      'fcnet_bias_initializer_config': None,\n",
      "                      'fcnet_hiddens': [256, 256],\n",
      "                      'fcnet_weights_initializer': None,\n",
      "                      'fcnet_weights_initializer_config': None,\n",
      "                      'framestack': True,\n",
      "                      'free_log_std': False,\n",
      "                      'grayscale': False,\n",
      "                      'log_std_clip_param': 20.0,\n",
      "                      'lstm_bias_initializer': None,\n",
      "                      'lstm_bias_initializer_config': None,\n",
      "                      'lstm_cell_size': 256,\n",
      "                      'lstm_use_prev_action': False,\n",
      "                      'lstm_use_prev_action_reward': -1,\n",
      "                      'lstm_use_prev_reward': False,\n",
      "                      'lstm_weights_initializer': None,\n",
      "                      'lstm_weights_initializer_config': None,\n",
      "                      'max_seq_len': 20,\n",
      "                      'no_final_linear': False,\n",
      "                      'post_fcnet_activation': 'relu',\n",
      "                      'post_fcnet_bias_initializer': None,\n",
      "                      'post_fcnet_bias_initializer_config': None,\n",
      "                      'post_fcnet_hiddens': [],\n",
      "                      'post_fcnet_weights_initializer': None,\n",
      "                      'post_fcnet_weights_initializer_config': None,\n",
      "                      'use_attention': False,\n",
      "                      'use_lstm': False,\n",
      "                      'vf_share_layers': False,\n",
      "                      'zero_mean': True},\n",
      "            'normalize_actions': True,\n",
      "            'num_aggregator_actors_per_learner': 0,\n",
      "            'num_consecutive_env_runner_failures_tolerance': 100,\n",
      "            'num_cpus_for_main_process': 1,\n",
      "            'num_cpus_per_env_runner': 1,\n",
      "            'num_cpus_per_learner': 'auto',\n",
      "            'num_cpus_per_offline_eval_runner': 1,\n",
      "            'num_env_runners': 2,\n",
      "            'num_envs_per_env_runner': 1,\n",
      "            'num_epochs': 10,\n",
      "            'num_gpus': 0,\n",
      "            'num_gpus_per_env_runner': 0,\n",
      "            'num_gpus_per_learner': 0,\n",
      "            'num_gpus_per_offline_eval_runner': 0,\n",
      "            'num_learners': 0,\n",
      "            'num_offline_eval_runners': 0,\n",
      "            'observation_filter': 'NoFilter',\n",
      "            'observation_fn': None,\n",
      "            'observation_space': None,\n",
      "            'off_policy_estimation_methods': {},\n",
      "            'offline_data_class': None,\n",
      "            'offline_eval_batch_size_per_runner': 256,\n",
      "            'offline_eval_rl_module_inference_only': False,\n",
      "            'offline_eval_runner_class': None,\n",
      "            'offline_eval_runner_health_probe_timeout_s': 30.0,\n",
      "            'offline_eval_runner_restore_timeout_s': 1800.0,\n",
      "            'offline_evaluation_duration': 1,\n",
      "            'offline_evaluation_interval': None,\n",
      "            'offline_evaluation_parallel_to_training': False,\n",
      "            'offline_evaluation_timeout_s': 120.0,\n",
      "            'offline_evaluation_type': None,\n",
      "            'offline_loss_for_module_fn': None,\n",
      "            'offline_sampling': False,\n",
      "            'ope_split_batch_by_episode': True,\n",
      "            'optimizer': {},\n",
      "            'output': None,\n",
      "            'output_compress_columns': ['obs', 'new_obs'],\n",
      "            'output_config': {},\n",
      "            'output_filesystem': None,\n",
      "            'output_filesystem_kwargs': {},\n",
      "            'output_max_file_size': 67108864,\n",
      "            'output_max_rows_per_file': None,\n",
      "            'output_write_episodes': True,\n",
      "            'output_write_method': 'write_parquet',\n",
      "            'output_write_method_kwargs': {},\n",
      "            'output_write_remaining_data': False,\n",
      "            'placement_strategy': 'PACK',\n",
      "            'policies': {'default_policy': (None, None, None, None)},\n",
      "            'policies_to_train': None,\n",
      "            'policy_map_cache': -1,\n",
      "            'policy_map_capacity': 100,\n",
      "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x31a3ac900>,\n",
      "            'policy_states_are_swappable': False,\n",
      "            'postprocess_inputs': False,\n",
      "            'prelearner_buffer_class': None,\n",
      "            'prelearner_buffer_kwargs': {},\n",
      "            'prelearner_class': None,\n",
      "            'prelearner_module_synch_period': 10,\n",
      "            'preprocessor_pref': 'deepmind',\n",
      "            'remote_env_batch_wait_ms': 0,\n",
      "            'remote_worker_envs': False,\n",
      "            'render_env': False,\n",
      "            'replay_sequence_length': None,\n",
      "            'restart_failed_env_runners': True,\n",
      "            'restart_failed_offline_eval_runners': True,\n",
      "            'restart_failed_sub_environments': False,\n",
      "            'rollout_fragment_length': 'auto',\n",
      "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      "            'sample_timeout_s': 60.0,\n",
      "            'sampler_perf_stats_ema_coef': None,\n",
      "            'seed': None,\n",
      "            'sgd_minibatch_size': -1,\n",
      "            'shuffle_batch_per_epoch': True,\n",
      "            'shuffle_buffer_size': 0,\n",
      "            'simple_optimizer': False,\n",
      "            'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
      "            'synchronize_filters': -1,\n",
      "            'tf_session_args': {'allow_soft_placement': True,\n",
      "                                'device_count': {'CPU': 1},\n",
      "                                'gpu_options': {'allow_growth': True},\n",
      "                                'inter_op_parallelism_threads': 2,\n",
      "                                'intra_op_parallelism_threads': 2,\n",
      "                                'log_device_placement': False},\n",
      "            'torch_compile_learner': False,\n",
      "            'torch_compile_learner_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_learner_dynamo_mode': None,\n",
      "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
      "            'torch_compile_worker': False,\n",
      "            'torch_compile_worker_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_worker_dynamo_mode': None,\n",
      "            'torch_ddp_kwargs': {},\n",
      "            'torch_skip_nan_gradients': False,\n",
      "            'train_batch_size': 4000,\n",
      "            'update_worker_filter_stats': True,\n",
      "            'use_critic': True,\n",
      "            'use_gae': True,\n",
      "            'use_kl_loss': True,\n",
      "            'use_worker_filter_stats': True,\n",
      "            'validate_env_runners_after_construction': True,\n",
      "            'validate_offline_eval_runners_after_construction': True,\n",
      "            'vf_clip_param': 10.0,\n",
      "            'vf_loss_coeff': 1.0,\n",
      "            'vf_share_layers': -1,\n",
      "            'worker_cls': -1},\n",
      " 'date': '2025-10-06_15-52-33',\n",
      " 'done': False,\n",
      " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
      " 'env_runners': {'agent_episode_return_mean': {'default_agent': -1541.63545740031},\n",
      "                 'env_reset_timer': 0.00019141699885949492,\n",
      "                 'env_step_timer': 2.282604093681675e-05,\n",
      "                 'env_to_module_connector': {'connector_pipeline_timer': 4.625251390951949e-05,\n",
      "                                             'timers': {'connectors': {'add_observations_from_episodes_to_batch': 2.8533509037853033e-06,\n",
      "                                                                       'add_states_from_episodes_to_batch': 8.522007165351114e-07,\n",
      "                                                                       'add_time_dim_to_batch_and_zero_pad': 1.1013985811110062e-06,\n",
      "                                                                       'batch_individual_items': 6.40700159600742e-06,\n",
      "                                                                       'numpy_to_tensor': 1.0283769329015705e-05}}},\n",
      "                 'env_to_module_sum_episodes_length_in': 131.73765398537716,\n",
      "                 'env_to_module_sum_episodes_length_out': 131.73765398537716,\n",
      "                 'episode_duration_sec_mean': 0.057195376949675844,\n",
      "                 'episode_len_max': 200,\n",
      "                 'episode_len_mean': 200.0,\n",
      "                 'episode_len_min': 200,\n",
      "                 'episode_return_max': -1136.9234806319132,\n",
      "                 'episode_return_mean': -1541.63545740031,\n",
      "                 'episode_return_min': -1836.022455251069,\n",
      "                 'module_episode_return_mean': {'default_policy': -1541.63545740031},\n",
      "                 'module_to_env_connector': {'connector_pipeline_timer': 0.00012578395669141895,\n",
      "                                             'timers': {'connectors': {'get_actions': 4.1135116020149805e-05,\n",
      "                                                                       'listify_data_for_vector_env': 1.0337088431680401e-05,\n",
      "                                                                       'normalize_and_clip_actions': 1.8653210514353507e-05,\n",
      "                                                                       'remove_single_ts_time_rank_from_batch': 7.815394680457883e-07,\n",
      "                                                                       'tensor_to_numpy': 1.699820822101159e-05,\n",
      "                                                                       'un_batch_to_individual_items': 6.438397208683735e-06}}},\n",
      "                 'num_agent_steps_sampled': {'default_agent': 2000.0},\n",
      "                 'num_agent_steps_sampled_lifetime': {'default_agent': 4000.0},\n",
      "                 'num_env_steps_sampled': 2000.0,\n",
      "                 'num_env_steps_sampled_lifetime': 4000.0,\n",
      "                 'num_env_steps_sampled_lifetime_throughput': 3352.8771554115606,\n",
      "                 'num_episodes': 10.0,\n",
      "                 'num_episodes_lifetime': 20.0,\n",
      "                 'num_module_steps_sampled': {'default_policy': 2000.0},\n",
      "                 'num_module_steps_sampled_lifetime': {'default_policy': 4000.0},\n",
      "                 'rlmodule_inference_timer': 4.6917896278644637e-05,\n",
      "                 'sample': 0.2945446819219433,\n",
      "                 'time_between_sampling': 0.7355734164993919,\n",
      "                 'weights_seq_no': 1.0},\n",
      " 'evaluation': {'actor_manager_num_outstanding_async_reqs': 0,\n",
      "                'env_runners': {'agent_episode_return_mean': {'default_agent': -1392.1471240737933},\n",
      "                                'env_reset_timer': 0.0002935874074319145,\n",
      "                                'env_step_timer': 2.252207355095758e-05,\n",
      "                                'env_to_module_connector': {'connector_pipeline_timer': 4.548208767688306e-05,\n",
      "                                                            'timers': {'connectors': {'add_observations_from_episodes_to_batch': 2.8901153033770785e-06,\n",
      "                                                                                      'add_states_from_episodes_to_batch': 8.354106481567781e-07,\n",
      "                                                                                      'add_time_dim_to_batch_and_zero_pad': 1.0282618553130869e-06,\n",
      "                                                                                      'batch_individual_items': 6.264903189738336e-06,\n",
      "                                                                                      'numpy_to_tensor': 9.900772119953332e-06}}},\n",
      "                                'env_to_module_sum_episodes_length_in': 131.73765398537716,\n",
      "                                'env_to_module_sum_episodes_length_out': 131.73765398537716,\n",
      "                                'episode_duration_sec_mean': 0.05668019369841204,\n",
      "                                'episode_len_max': 200,\n",
      "                                'episode_len_mean': 200.0,\n",
      "                                'episode_len_min': 200,\n",
      "                                'episode_return_max': -1058.242003723863,\n",
      "                                'episode_return_mean': -1392.1471240737933,\n",
      "                                'episode_return_min': -1761.8119946513752,\n",
      "                                'module_episode_return_mean': {'default_policy': -1392.1471240737933},\n",
      "                                'module_to_env_connector': {'connector_pipeline_timer': 0.0001233590412352834,\n",
      "                                                            'timers': {'connectors': {'get_actions': 4.012848565029486e-05,\n",
      "                                                                                      'listify_data_for_vector_env': 1.0122988330543908e-05,\n",
      "                                                                                      'normalize_and_clip_actions': 1.8368182305905582e-05,\n",
      "                                                                                      'remove_single_ts_time_rank_from_batch': 7.759464976422541e-07,\n",
      "                                                                                      'tensor_to_numpy': 1.644538258519076e-05,\n",
      "                                                                                      'un_batch_to_individual_items': 6.295075775773954e-06}}},\n",
      "                                'num_agent_steps_sampled': {'default_agent': 2000.0},\n",
      "                                'num_agent_steps_sampled_lifetime': {'default_agent': 4000.0},\n",
      "                                'num_env_steps_sampled': 2000.0,\n",
      "                                'num_env_steps_sampled_lifetime': 2000.0,\n",
      "                                'num_episodes': 10.0,\n",
      "                                'num_episodes_lifetime': 20.0,\n",
      "                                'num_module_steps_sampled': {'default_policy': 2000.0},\n",
      "                                'num_module_steps_sampled_lifetime': {'default_policy': 4000.0},\n",
      "                                'rlmodule_inference_timer': 4.5332313619842585e-05,\n",
      "                                'sample': 0.2923679609565807,\n",
      "                                'time_between_sampling': 0.7292471455002669,\n",
      "                                'weights_seq_no': 2.0},\n",
      "                'num_healthy_workers': 2,\n",
      "                'num_remote_worker_restarts': 0},\n",
      " 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0},\n",
      " 'hostname': 'scadsdp25.misc.intern.uni-leipzig.de',\n",
      " 'iterations_since_restore': 2,\n",
      " 'learners': {'__all_modules__': {'learner_connector': {'connector_pipeline_timer': 0.04180437508221076,\n",
      "                                                        'timers': {'connectors': {'add_columns_from_episodes_to_train_batch': 0.015835343750295577,\n",
      "                                                                                  'add_observations_from_episodes_to_batch': 5.2041159869986585e-05,\n",
      "                                                                                  'add_one_ts_to_episodes_and_truncate': 0.0009973362484015525,\n",
      "                                                                                  'add_states_from_episodes_to_batch': 3.1669888267060742e-06,\n",
      "                                                                                  'add_time_dim_to_batch_and_zero_pad': 1.1609573994064703e-05,\n",
      "                                                                                  'batch_individual_items': 0.014030064406324528,\n",
      "                                                                                  'general_advantage_estimation': 0.010642213917089976,\n",
      "                                                                                  'numpy_to_tensor': 9.180508423014543e-05}}},\n",
      "                                  'learner_connector_sum_episodes_length_in': 2000.0,\n",
      "                                  'learner_connector_sum_episodes_length_out': 2010.0,\n",
      "                                  'num_env_steps_trained': 317580,\n",
      "                                  'num_env_steps_trained_lifetime': 635160,\n",
      "                                  'num_env_steps_trained_lifetime_throughput': 913781.3074589246,\n",
      "                                  'num_module_steps_trained': 20224,\n",
      "                                  'num_module_steps_trained_lifetime': 40448,\n",
      "                                  'num_module_steps_trained_lifetime_throughput': 58189.608934265045,\n",
      "                                  'num_module_steps_trained_throughput': 58189.63788858336,\n",
      "                                  'num_non_trainable_parameters': 0,\n",
      "                                  'num_trainable_parameters': 134403},\n",
      "              'default_policy': {'curr_entropy_coeff': 0.0,\n",
      "                                 'curr_kl_coeff': 0.20000000298023224,\n",
      "                                 'default_optimizer_learning_rate': 0.0002,\n",
      "                                 'diff_num_grad_updates_vs_sampler_policy': 1.0,\n",
      "                                 'entropy': 1.3742793,\n",
      "                                 'gradients_default_optimizer_global_norm': 0.4368031,\n",
      "                                 'mean_kl_loss': 0.007923027,\n",
      "                                 'module_train_batch_size_mean': 128.0,\n",
      "                                 'num_module_steps_trained': 20224,\n",
      "                                 'num_module_steps_trained_lifetime': 40448,\n",
      "                                 'num_module_steps_trained_lifetime_throughput': 58188.62437294183,\n",
      "                                 'num_trainable_parameters': 134403,\n",
      "                                 'policy_loss': 0.050007258,\n",
      "                                 'total_loss': 10.051593,\n",
      "                                 'vf_explained_var': -0.00043427944,\n",
      "                                 'vf_loss': 10.0,\n",
      "                                 'vf_loss_unclipped': 220330.23,\n",
      "                                 'weights_seq_no': 2.0}},\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_env_steps_sampled_lifetime': 4000.0,\n",
      " 'num_training_step_calls_per_iteration': 1,\n",
      " 'perf': {'cpu_util_percent': 29.6, 'ram_util_percent': 70.1},\n",
      " 'pid': 29591,\n",
      " 'time_since_restore': 2.018692970275879,\n",
      " 'time_this_iter_s': 1.0028440952301025,\n",
      " 'time_total_s': 2.018692970275879,\n",
      " 'timers': {'env_runner_sampling_timer': 0.3042735809181613,\n",
      "            'evaluation_iteration': 0.31051954199938336,\n",
      "            'learner_update_timer': 0.39576075617311285,\n",
      "            'restore_env_runners': 5.185329428059049e-06,\n",
      "            'restore_eval_env_runners': 6.583999493159354e-06,\n",
      "            'synch_env_connectors': 0.0023512920015491545,\n",
      "            'synch_eval_env_connectors': 0.00022104199888417497,\n",
      "            'synch_weights': 0.0009624200007965555,\n",
      "            'training_iteration': 0.701243801583114,\n",
      "            'training_step': 0.7011536133261688},\n",
      " 'timestamp': 1759758753,\n",
      " 'training_iteration': 2,\n",
      " 'trial_id': 'default'}\n",
      "{'config': {'_disable_action_flattening': False,\n",
      "            '_disable_execution_plan_api': -1,\n",
      "            '_disable_initialize_loss_from_dummy_batch': False,\n",
      "            '_disable_preprocessor_api': False,\n",
      "            '_dont_auto_sync_env_runner_states': False,\n",
      "            '_enable_rl_module_api': -1,\n",
      "            '_env_to_module_connector': None,\n",
      "            '_fake_gpus': False,\n",
      "            '_is_atari': None,\n",
      "            '_is_online': True,\n",
      "            '_learner_class': None,\n",
      "            '_learner_connector': None,\n",
      "            '_model_config': {},\n",
      "            '_module_to_env_connector': None,\n",
      "            '_per_module_overrides': {},\n",
      "            '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
      "            '_rl_module_spec': None,\n",
      "            '_tf_policy_handles_more_than_one_loss': False,\n",
      "            '_torch_grad_scaler_class': None,\n",
      "            '_torch_lr_scheduler_classes': None,\n",
      "            '_train_batch_size_per_learner': 2000,\n",
      "            '_use_msgpack_checkpoints': False,\n",
      "            '_validate_config': True,\n",
      "            'action_mask_key': 'action_mask',\n",
      "            'action_space': None,\n",
      "            'actions_in_input_normalized': False,\n",
      "            'add_default_connectors_to_env_to_module_pipeline': True,\n",
      "            'add_default_connectors_to_learner_pipeline': True,\n",
      "            'add_default_connectors_to_module_to_env_pipeline': True,\n",
      "            'algorithm_config_overrides_per_module': {},\n",
      "            'always_attach_evaluation_results': -1,\n",
      "            'auto_wrap_old_gym_envs': -1,\n",
      "            'batch_mode': 'truncate_episodes',\n",
      "            'broadcast_env_runner_states': True,\n",
      "            'broadcast_offline_eval_runner_states': False,\n",
      "            'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>,\n",
      "            'callbacks_on_algorithm_init': None,\n",
      "            'callbacks_on_checkpoint_loaded': None,\n",
      "            'callbacks_on_env_runners_recreated': None,\n",
      "            'callbacks_on_environment_created': None,\n",
      "            'callbacks_on_episode_created': None,\n",
      "            'callbacks_on_episode_end': None,\n",
      "            'callbacks_on_episode_start': None,\n",
      "            'callbacks_on_episode_step': None,\n",
      "            'callbacks_on_evaluate_end': None,\n",
      "            'callbacks_on_evaluate_offline_end': None,\n",
      "            'callbacks_on_evaluate_offline_start': None,\n",
      "            'callbacks_on_evaluate_start': None,\n",
      "            'callbacks_on_offline_eval_runners_recreated': None,\n",
      "            'callbacks_on_sample_end': None,\n",
      "            'callbacks_on_train_result': None,\n",
      "            'checkpoint_trainable_policies_only': False,\n",
      "            'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>,\n",
      "            'clip_actions': False,\n",
      "            'clip_param': 0.3,\n",
      "            'clip_rewards': None,\n",
      "            'compress_observations': False,\n",
      "            'count_steps_by': 'env_steps',\n",
      "            'create_env_on_driver': False,\n",
      "            'create_local_env_runner': True,\n",
      "            'custom_async_evaluation_function': -1,\n",
      "            'custom_eval_function': None,\n",
      "            'custom_resources_per_env_runner': {},\n",
      "            'custom_resources_per_offline_eval_runner': {},\n",
      "            'dataset_num_iters_per_eval_runner': 1,\n",
      "            'dataset_num_iters_per_learner': None,\n",
      "            'delay_between_env_runner_restarts_s': 60.0,\n",
      "            'disable_env_checking': False,\n",
      "            'eager_max_retraces': 20,\n",
      "            'eager_tracing': True,\n",
      "            'enable_async_evaluation': -1,\n",
      "            'enable_connectors': -1,\n",
      "            'enable_env_runner_and_connector_v2': True,\n",
      "            'enable_rl_module_and_learner': True,\n",
      "            'enable_tf1_exec_eagerly': False,\n",
      "            'entropy_coeff': 0.0,\n",
      "            'entropy_coeff_schedule': None,\n",
      "            'env': 'Pendulum-v1',\n",
      "            'env_config': {},\n",
      "            'env_runner_cls': None,\n",
      "            'env_runner_health_probe_timeout_s': 30.0,\n",
      "            'env_runner_restore_timeout_s': 1800.0,\n",
      "            'env_task_fn': -1,\n",
      "            'episode_lookback_horizon': 1,\n",
      "            'episodes_to_numpy': True,\n",
      "            'evaluation_auto_duration_max_env_steps_per_sample': 2000,\n",
      "            'evaluation_auto_duration_min_env_steps_per_sample': 100,\n",
      "            'evaluation_config': None,\n",
      "            'evaluation_duration': 10,\n",
      "            'evaluation_duration_unit': 'episodes',\n",
      "            'evaluation_force_reset_envs_before_iteration': True,\n",
      "            'evaluation_interval': 1,\n",
      "            'evaluation_num_env_runners': 2,\n",
      "            'evaluation_parallel_to_training': False,\n",
      "            'evaluation_sample_timeout_s': 120.0,\n",
      "            'exploration_config': {},\n",
      "            'explore': True,\n",
      "            'export_native_model_files': False,\n",
      "            'extra_python_environs_for_driver': {},\n",
      "            'extra_python_environs_for_worker': {},\n",
      "            'fake_sampler': False,\n",
      "            'framework': 'torch',\n",
      "            'gamma': 0.99,\n",
      "            'grad_clip': None,\n",
      "            'grad_clip_by': 'global_norm',\n",
      "            'gym_env_vectorize_mode': 'SYNC',\n",
      "            'ignore_env_runner_failures': False,\n",
      "            'ignore_final_observation': False,\n",
      "            'ignore_offline_eval_runner_failures': False,\n",
      "            'in_evaluation': False,\n",
      "            'input': 'sampler',\n",
      "            'input_compress_columns': ['obs', 'new_obs'],\n",
      "            'input_config': {},\n",
      "            'input_filesystem': None,\n",
      "            'input_filesystem_kwargs': {},\n",
      "            'input_read_batch_size': None,\n",
      "            'input_read_episodes': False,\n",
      "            'input_read_method': 'read_parquet',\n",
      "            'input_read_method_kwargs': {},\n",
      "            'input_read_sample_batches': False,\n",
      "            'input_read_schema': {},\n",
      "            'input_spaces_jsonable': True,\n",
      "            'iter_batches_kwargs': {},\n",
      "            'keep_per_episode_custom_metrics': False,\n",
      "            'kl_coeff': 0.2,\n",
      "            'kl_target': 0.01,\n",
      "            'lambda': 1.0,\n",
      "            'learner_config_dict': {},\n",
      "            'local_gpu_idx': 0,\n",
      "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                                      'intra_op_parallelism_threads': 8},\n",
      "            'log_gradients': True,\n",
      "            'log_level': 'WARN',\n",
      "            'log_sys_usage': True,\n",
      "            'logger_config': None,\n",
      "            'logger_creator': None,\n",
      "            'lr': 0.0002,\n",
      "            'lr_schedule': None,\n",
      "            'map_batches_kwargs': {},\n",
      "            'materialize_data': False,\n",
      "            'materialize_mapped_data': True,\n",
      "            'max_num_env_runner_restarts': 1000,\n",
      "            'max_num_offline_eval_runner_restarts': 1000,\n",
      "            'max_requests_in_flight_per_aggregator_actor': 3,\n",
      "            'max_requests_in_flight_per_env_runner': 1,\n",
      "            'max_requests_in_flight_per_learner': 3,\n",
      "            'max_requests_in_flight_per_offline_eval_runner': 1,\n",
      "            'merge_env_runner_states': 'training_only',\n",
      "            'metrics_episode_collection_timeout_s': 60.0,\n",
      "            'metrics_num_episodes_for_smoothing': 100,\n",
      "            'min_sample_timesteps_per_iteration': 0,\n",
      "            'min_time_s_per_iteration': None,\n",
      "            'min_train_timesteps_per_iteration': 0,\n",
      "            'minibatch_size': 128,\n",
      "            'model': {'_disable_action_flattening': False,\n",
      "                      '_disable_preprocessor_api': False,\n",
      "                      '_time_major': False,\n",
      "                      '_use_default_native_models': -1,\n",
      "                      'always_check_shapes': False,\n",
      "                      'attention_dim': 64,\n",
      "                      'attention_head_dim': 32,\n",
      "                      'attention_init_gru_gate_bias': 2.0,\n",
      "                      'attention_memory_inference': 50,\n",
      "                      'attention_memory_training': 50,\n",
      "                      'attention_num_heads': 1,\n",
      "                      'attention_num_transformer_units': 1,\n",
      "                      'attention_position_wise_mlp_dim': 32,\n",
      "                      'attention_use_n_prev_actions': 0,\n",
      "                      'attention_use_n_prev_rewards': 0,\n",
      "                      'conv_activation': 'relu',\n",
      "                      'conv_bias_initializer': None,\n",
      "                      'conv_bias_initializer_config': None,\n",
      "                      'conv_filters': None,\n",
      "                      'conv_kernel_initializer': None,\n",
      "                      'conv_kernel_initializer_config': None,\n",
      "                      'conv_transpose_bias_initializer': None,\n",
      "                      'conv_transpose_bias_initializer_config': None,\n",
      "                      'conv_transpose_kernel_initializer': None,\n",
      "                      'conv_transpose_kernel_initializer_config': None,\n",
      "                      'custom_action_dist': None,\n",
      "                      'custom_model': None,\n",
      "                      'custom_model_config': {},\n",
      "                      'custom_preprocessor': None,\n",
      "                      'dim': 84,\n",
      "                      'encoder_latent_dim': None,\n",
      "                      'fcnet_activation': 'tanh',\n",
      "                      'fcnet_bias_initializer': None,\n",
      "                      'fcnet_bias_initializer_config': None,\n",
      "                      'fcnet_hiddens': [256, 256],\n",
      "                      'fcnet_weights_initializer': None,\n",
      "                      'fcnet_weights_initializer_config': None,\n",
      "                      'framestack': True,\n",
      "                      'free_log_std': False,\n",
      "                      'grayscale': False,\n",
      "                      'log_std_clip_param': 20.0,\n",
      "                      'lstm_bias_initializer': None,\n",
      "                      'lstm_bias_initializer_config': None,\n",
      "                      'lstm_cell_size': 256,\n",
      "                      'lstm_use_prev_action': False,\n",
      "                      'lstm_use_prev_action_reward': -1,\n",
      "                      'lstm_use_prev_reward': False,\n",
      "                      'lstm_weights_initializer': None,\n",
      "                      'lstm_weights_initializer_config': None,\n",
      "                      'max_seq_len': 20,\n",
      "                      'no_final_linear': False,\n",
      "                      'post_fcnet_activation': 'relu',\n",
      "                      'post_fcnet_bias_initializer': None,\n",
      "                      'post_fcnet_bias_initializer_config': None,\n",
      "                      'post_fcnet_hiddens': [],\n",
      "                      'post_fcnet_weights_initializer': None,\n",
      "                      'post_fcnet_weights_initializer_config': None,\n",
      "                      'use_attention': False,\n",
      "                      'use_lstm': False,\n",
      "                      'vf_share_layers': False,\n",
      "                      'zero_mean': True},\n",
      "            'normalize_actions': True,\n",
      "            'num_aggregator_actors_per_learner': 0,\n",
      "            'num_consecutive_env_runner_failures_tolerance': 100,\n",
      "            'num_cpus_for_main_process': 1,\n",
      "            'num_cpus_per_env_runner': 1,\n",
      "            'num_cpus_per_learner': 'auto',\n",
      "            'num_cpus_per_offline_eval_runner': 1,\n",
      "            'num_env_runners': 2,\n",
      "            'num_envs_per_env_runner': 1,\n",
      "            'num_epochs': 10,\n",
      "            'num_gpus': 0,\n",
      "            'num_gpus_per_env_runner': 0,\n",
      "            'num_gpus_per_learner': 0,\n",
      "            'num_gpus_per_offline_eval_runner': 0,\n",
      "            'num_learners': 0,\n",
      "            'num_offline_eval_runners': 0,\n",
      "            'observation_filter': 'NoFilter',\n",
      "            'observation_fn': None,\n",
      "            'observation_space': None,\n",
      "            'off_policy_estimation_methods': {},\n",
      "            'offline_data_class': None,\n",
      "            'offline_eval_batch_size_per_runner': 256,\n",
      "            'offline_eval_rl_module_inference_only': False,\n",
      "            'offline_eval_runner_class': None,\n",
      "            'offline_eval_runner_health_probe_timeout_s': 30.0,\n",
      "            'offline_eval_runner_restore_timeout_s': 1800.0,\n",
      "            'offline_evaluation_duration': 1,\n",
      "            'offline_evaluation_interval': None,\n",
      "            'offline_evaluation_parallel_to_training': False,\n",
      "            'offline_evaluation_timeout_s': 120.0,\n",
      "            'offline_evaluation_type': None,\n",
      "            'offline_loss_for_module_fn': None,\n",
      "            'offline_sampling': False,\n",
      "            'ope_split_batch_by_episode': True,\n",
      "            'optimizer': {},\n",
      "            'output': None,\n",
      "            'output_compress_columns': ['obs', 'new_obs'],\n",
      "            'output_config': {},\n",
      "            'output_filesystem': None,\n",
      "            'output_filesystem_kwargs': {},\n",
      "            'output_max_file_size': 67108864,\n",
      "            'output_max_rows_per_file': None,\n",
      "            'output_write_episodes': True,\n",
      "            'output_write_method': 'write_parquet',\n",
      "            'output_write_method_kwargs': {},\n",
      "            'output_write_remaining_data': False,\n",
      "            'placement_strategy': 'PACK',\n",
      "            'policies': {'default_policy': (None, None, None, None)},\n",
      "            'policies_to_train': None,\n",
      "            'policy_map_cache': -1,\n",
      "            'policy_map_capacity': 100,\n",
      "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x31a3ac900>,\n",
      "            'policy_states_are_swappable': False,\n",
      "            'postprocess_inputs': False,\n",
      "            'prelearner_buffer_class': None,\n",
      "            'prelearner_buffer_kwargs': {},\n",
      "            'prelearner_class': None,\n",
      "            'prelearner_module_synch_period': 10,\n",
      "            'preprocessor_pref': 'deepmind',\n",
      "            'remote_env_batch_wait_ms': 0,\n",
      "            'remote_worker_envs': False,\n",
      "            'render_env': False,\n",
      "            'replay_sequence_length': None,\n",
      "            'restart_failed_env_runners': True,\n",
      "            'restart_failed_offline_eval_runners': True,\n",
      "            'restart_failed_sub_environments': False,\n",
      "            'rollout_fragment_length': 'auto',\n",
      "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      "            'sample_timeout_s': 60.0,\n",
      "            'sampler_perf_stats_ema_coef': None,\n",
      "            'seed': None,\n",
      "            'sgd_minibatch_size': -1,\n",
      "            'shuffle_batch_per_epoch': True,\n",
      "            'shuffle_buffer_size': 0,\n",
      "            'simple_optimizer': False,\n",
      "            'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
      "            'synchronize_filters': -1,\n",
      "            'tf_session_args': {'allow_soft_placement': True,\n",
      "                                'device_count': {'CPU': 1},\n",
      "                                'gpu_options': {'allow_growth': True},\n",
      "                                'inter_op_parallelism_threads': 2,\n",
      "                                'intra_op_parallelism_threads': 2,\n",
      "                                'log_device_placement': False},\n",
      "            'torch_compile_learner': False,\n",
      "            'torch_compile_learner_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_learner_dynamo_mode': None,\n",
      "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
      "            'torch_compile_worker': False,\n",
      "            'torch_compile_worker_dynamo_backend': 'aot_eager',\n",
      "            'torch_compile_worker_dynamo_mode': None,\n",
      "            'torch_ddp_kwargs': {},\n",
      "            'torch_skip_nan_gradients': False,\n",
      "            'train_batch_size': 4000,\n",
      "            'update_worker_filter_stats': True,\n",
      "            'use_critic': True,\n",
      "            'use_gae': True,\n",
      "            'use_kl_loss': True,\n",
      "            'use_worker_filter_stats': True,\n",
      "            'validate_env_runners_after_construction': True,\n",
      "            'validate_offline_eval_runners_after_construction': True,\n",
      "            'vf_clip_param': 10.0,\n",
      "            'vf_loss_coeff': 1.0,\n",
      "            'vf_share_layers': -1,\n",
      "            'worker_cls': -1},\n",
      " 'date': '2025-10-06_15-52-34',\n",
      " 'done': False,\n",
      " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
      " 'env_runners': {'agent_episode_return_mean': {'default_agent': -1509.6038962180849},\n",
      "                 'env_reset_timer': 0.00019141699885949492,\n",
      "                 'env_step_timer': 2.2742488323443456e-05,\n",
      "                 'env_to_module_connector': {'connector_pipeline_timer': 4.499647102277855e-05,\n",
      "                                             'timers': {'connectors': {'add_observations_from_episodes_to_batch': 2.7604662602456926e-06,\n",
      "                                                                       'add_states_from_episodes_to_batch': 8.099581797707789e-07,\n",
      "                                                                       'add_time_dim_to_batch_and_zero_pad': 1.0458025654715092e-06,\n",
      "                                                                       'batch_individual_items': 6.199786095128435e-06,\n",
      "                                                                       'numpy_to_tensor': 9.947434449293789e-06}}},\n",
      "                 'env_to_module_sum_episodes_length_in': 131.73765420741816,\n",
      "                 'env_to_module_sum_episodes_length_out': 131.73765420741816,\n",
      "                 'episode_duration_sec_mean': 0.05710392633385102,\n",
      "                 'episode_len_max': 200,\n",
      "                 'episode_len_mean': 200.0,\n",
      "                 'episode_len_min': 200,\n",
      "                 'episode_return_max': -1136.9234806319132,\n",
      "                 'episode_return_mean': -1509.6038962180849,\n",
      "                 'episode_return_min': -1836.022455251069,\n",
      "                 'module_episode_return_mean': {'default_policy': -1509.6038962180849},\n",
      "                 'module_to_env_connector': {'connector_pipeline_timer': 0.00012359556885580098,\n",
      "                                             'timers': {'connectors': {'get_actions': 4.050879863941264e-05,\n",
      "                                                                       'listify_data_for_vector_env': 1.0314519541681176e-05,\n",
      "                                                                       'normalize_and_clip_actions': 1.843072008090121e-05,\n",
      "                                                                       'remove_single_ts_time_rank_from_batch': 7.604976539633958e-07,\n",
      "                                                                       'tensor_to_numpy': 1.6411487648733578e-05,\n",
      "                                                                       'un_batch_to_individual_items': 6.3035235293817425e-06}}},\n",
      "                 'num_agent_steps_sampled': {'default_agent': 2000.0},\n",
      "                 'num_agent_steps_sampled_lifetime': {'default_agent': 6000.0},\n",
      "                 'num_env_steps_sampled': 2000.0,\n",
      "                 'num_env_steps_sampled_lifetime': 6000.0,\n",
      "                 'num_env_steps_sampled_lifetime_throughput': 3331.3264315027286,\n",
      "                 'num_episodes': 10.0,\n",
      "                 'num_episodes_lifetime': 30.0,\n",
      "                 'num_module_steps_sampled': {'default_policy': 2000.0},\n",
      "                 'num_module_steps_sampled_lifetime': {'default_policy': 6000.0},\n",
      "                 'rlmodule_inference_timer': 4.581095156495908e-05,\n",
      "                 'sample': 0.2945297980177133,\n",
      "                 'time_between_sampling': 0.7354493456694036,\n",
      "                 'weights_seq_no': 2.0},\n",
      " 'evaluation': {'actor_manager_num_outstanding_async_reqs': 0,\n",
      "                'env_runners': {'agent_episode_return_mean': {'default_agent': -1462.4811483673834},\n",
      "                                'env_reset_timer': 0.0002912669533885492,\n",
      "                                'env_step_timer': 2.2140390892194462e-05,\n",
      "                                'env_to_module_connector': {'connector_pipeline_timer': 4.568141612422022e-05,\n",
      "                                                            'timers': {'connectors': {'add_observations_from_episodes_to_batch': 2.9074461618300164e-06,\n",
      "                                                                                      'add_states_from_episodes_to_batch': 8.491295466442336e-07,\n",
      "                                                                                      'add_time_dim_to_batch_and_zero_pad': 1.055512718838903e-06,\n",
      "                                                                                      'batch_individual_items': 6.297226096686404e-06,\n",
      "                                                                                      'numpy_to_tensor': 9.909507727862325e-06}}},\n",
      "                                'env_to_module_sum_episodes_length_in': 131.73765420741816,\n",
      "                                'env_to_module_sum_episodes_length_out': 131.73765420741816,\n",
      "                                'episode_duration_sec_mean': 0.05659340276639947,\n",
      "                                'episode_len_max': 200,\n",
      "                                'episode_len_mean': 200.0,\n",
      "                                'episode_len_min': 200,\n",
      "                                'episode_return_max': -1058.242003723863,\n",
      "                                'episode_return_mean': -1462.4811483673834,\n",
      "                                'episode_return_min': -1771.2211691842735,\n",
      "                                'module_episode_return_mean': {'default_policy': -1462.4811483673834},\n",
      "                                'module_to_env_connector': {'connector_pipeline_timer': 0.00012203799447149374,\n",
      "                                                            'timers': {'connectors': {'get_actions': 3.970031048517612e-05,\n",
      "                                                                                      'listify_data_for_vector_env': 9.946726459523457e-06,\n",
      "                                                                                      'normalize_and_clip_actions': 1.809664555830651e-05,\n",
      "                                                                                      'remove_single_ts_time_rank_from_batch': 7.93920310386646e-07,\n",
      "                                                                                      'tensor_to_numpy': 1.6167050469150495e-05,\n",
      "                                                                                      'un_batch_to_individual_items': 6.260882522078957e-06}}},\n",
      "                                'num_agent_steps_sampled': {'default_agent': 2000.0},\n",
      "                                'num_agent_steps_sampled_lifetime': {'default_agent': 6000.0},\n",
      "                                'num_env_steps_sampled': 2000.0,\n",
      "                                'num_env_steps_sampled_lifetime': 2000.0,\n",
      "                                'num_episodes': 10.0,\n",
      "                                'num_episodes_lifetime': 30.0,\n",
      "                                'num_module_steps_sampled': {'default_policy': 2000.0},\n",
      "                                'num_module_steps_sampled_lifetime': {'default_policy': 6000.0},\n",
      "                                'rlmodule_inference_timer': 4.491545392701842e-05,\n",
      "                                'sample': 0.2923417084270393,\n",
      "                                'time_between_sampling': 0.7292359380052585,\n",
      "                                'weights_seq_no': 3.0},\n",
      "                'num_healthy_workers': 2,\n",
      "                'num_remote_worker_restarts': 0},\n",
      " 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0},\n",
      " 'hostname': 'scadsdp25.misc.intern.uni-leipzig.de',\n",
      " 'iterations_since_restore': 3,\n",
      " 'learners': {'__all_modules__': {'learner_connector': {'connector_pipeline_timer': 0.04172449133134433,\n",
      "                                                        'timers': {'connectors': {'add_columns_from_episodes_to_train_batch': 0.015831892402790978,\n",
      "                                                                                  'add_observations_from_episodes_to_batch': 5.2040748301806156e-05,\n",
      "                                                                                  'add_one_ts_to_episodes_and_truncate': 0.0009971207959286403,\n",
      "                                                                                  'add_states_from_episodes_to_batch': 3.1661589579016436e-06,\n",
      "                                                                                  'add_time_dim_to_batch_and_zero_pad': 1.158930829114979e-05,\n",
      "                                                                                  'batch_individual_items': 0.01402985501232979,\n",
      "                                                                                  'general_advantage_estimation': 0.010566756357919803,\n",
      "                                                                                  'numpy_to_tensor': 9.178578339196973e-05}}},\n",
      "                                  'learner_connector_sum_episodes_length_in': 2000.0,\n",
      "                                  'learner_connector_sum_episodes_length_out': 2010.0,\n",
      "                                  'num_env_steps_trained': 317580,\n",
      "                                  'num_env_steps_trained_lifetime': 952740,\n",
      "                                  'num_env_steps_trained_lifetime_throughput': 913279.2599263878,\n",
      "                                  'num_module_steps_trained': 20224,\n",
      "                                  'num_module_steps_trained_lifetime': 60672,\n",
      "                                  'num_module_steps_trained_lifetime_throughput': 58157.69451740904,\n",
      "                                  'num_module_steps_trained_throughput': 58157.71230608468,\n",
      "                                  'num_non_trainable_parameters': 0,\n",
      "                                  'num_trainable_parameters': 134403},\n",
      "              'default_policy': {'curr_entropy_coeff': 0.0,\n",
      "                                 'curr_kl_coeff': 0.20000000298023224,\n",
      "                                 'default_optimizer_learning_rate': 0.0002,\n",
      "                                 'diff_num_grad_updates_vs_sampler_policy': 1.0,\n",
      "                                 'entropy': 1.4605995,\n",
      "                                 'gradients_default_optimizer_global_norm': 0.7793488,\n",
      "                                 'mean_kl_loss': 0.018958181,\n",
      "                                 'module_train_batch_size_mean': 128.0,\n",
      "                                 'num_module_steps_trained': 20224,\n",
      "                                 'num_module_steps_trained_lifetime': 60672,\n",
      "                                 'num_module_steps_trained_lifetime_throughput': 58156.77383383661,\n",
      "                                 'num_trainable_parameters': 134403,\n",
      "                                 'policy_loss': -0.16778141,\n",
      "                                 'total_loss': 9.836011,\n",
      "                                 'vf_explained_var': -0.00023126602,\n",
      "                                 'vf_loss': 10.0,\n",
      "                                 'vf_loss_unclipped': 196338.6,\n",
      "                                 'weights_seq_no': 3.0}},\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_env_steps_sampled_lifetime': 6000.0,\n",
      " 'num_training_step_calls_per_iteration': 1,\n",
      " 'perf': {'cpu_util_percent': 13.4, 'ram_util_percent': 70.05},\n",
      " 'pid': 29591,\n",
      " 'time_since_restore': 3.0230319499969482,\n",
      " 'time_this_iter_s': 1.0043389797210693,\n",
      " 'time_total_s': 3.0230319499969482,\n",
      " 'timers': {'env_runner_sampling_timer': 0.3042443667789797,\n",
      "            'evaluation_iteration': 0.3104990365793492,\n",
      "            'learner_update_timer': 0.39569097986133994,\n",
      "            'restore_env_runners': 5.183066150493687e-06,\n",
      "            'restore_eval_env_runners': 6.58565950288903e-06,\n",
      "            'synch_env_connectors': 0.002349803671531845,\n",
      "            'synch_eval_env_connectors': 0.00022096698892710265,\n",
      "            'synch_weights': 0.0009613737108367786,\n",
      "            'training_iteration': 0.701144498567278,\n",
      "            'training_step': 0.7010536538629341},\n",
      " 'timestamp': 1759758754,\n",
      " 'training_iteration': 3,\n",
      " 'trial_id': 'default'}\n"
     ]
    }
   ],
   "source": [
    "config.evaluation(\n",
    "    # Run one evaluation round every iteration.\n",
    "    evaluation_interval=1,\n",
    "\n",
    "    # Create 2 eval EnvRunners in the extra EnvRunnerGroup.\n",
    "    evaluation_num_env_runners=2,\n",
    "\n",
    "    # Run evaluation for exactly 10 episodes. Note that because you have\n",
    "    # 2 EnvRunners, each one runs through 5 episodes.\n",
    "    evaluation_duration_unit=\"episodes\",\n",
    "    evaluation_duration=10,\n",
    ")\n",
    "\n",
    "# Rebuild the PPO, but with the extra evaluation EnvRunnerGroup\n",
    "ppo_with_evaluation = config.build_algo()\n",
    "\n",
    "for _ in range(3):\n",
    "    pprint(ppo_with_evaluation.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dd6c254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-10-06 16:21:26</td></tr>\n",
       "<tr><td>Running for: </td><td>00:10:36.66        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.0/18.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/11 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_42f45_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.001 </td></tr>\n",
       "<tr><td>PPO_Pendulum-v1_42f45_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0005</td></tr>\n",
       "<tr><td>PPO_Pendulum-v1_42f45_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 16:21:26,590\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-10-06 16:21:26,593\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/paula/ray_results/PPO_2025-10-06_16-10-49' in 0.0023s.\n",
      "2025-10-06 16:21:26,597\tINFO tune.py:1041 -- Total run time: 636.67 seconds (636.66 seconds for the tuning loop).\n",
      "2025-10-06 16:21:26,597\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/Users/paula/ray_results/PPO_2025-10-06_16-10-49\", trainable=...)\n",
      "2025-10-06 16:21:26,601\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 3 trial(s):\n",
      "- PPO_Pendulum-v1_42f45_00000: FileNotFoundError('Could not fetch metrics for PPO_Pendulum-v1_42f45_00000: both result.json and progress.csv were not found at /Users/paula/ray_results/PPO_2025-10-06_16-10-49/PPO_Pendulum-v1_42f45_00000_0_lr=0.0010_2025-10-06_16-10-49')\n",
      "- PPO_Pendulum-v1_42f45_00001: FileNotFoundError('Could not fetch metrics for PPO_Pendulum-v1_42f45_00001: both result.json and progress.csv were not found at /Users/paula/ray_results/PPO_2025-10-06_16-10-49/PPO_Pendulum-v1_42f45_00001_1_lr=0.0005_2025-10-06_16-10-49')\n",
      "- PPO_Pendulum-v1_42f45_00002: FileNotFoundError('Could not fetch metrics for PPO_Pendulum-v1_42f45_00002: both result.json and progress.csv were not found at /Users/paula/ray_results/PPO_2025-10-06_16-10-49/PPO_Pendulum-v1_42f45_00002_2_lr=0.0001_2025-10-06_16-10-49')\n"
     ]
    }
   ],
   "source": [
    "from ray import train, tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"Pendulum-v1\")\n",
    "    # Specify a simple tune hyperparameter sweep.\n",
    "    .training(\n",
    "        lr=tune.grid_search([0.001, 0.0005, 0.0001]),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a Tuner instance to manage the trials.\n",
    "tuner = tune.Tuner(\n",
    "    config.algo_class,\n",
    "    param_space=config,\n",
    "    # Specify a stopping criterion. Note that the criterion has to match one of the\n",
    "    # pretty printed result metrics from the results returned previously by\n",
    "    # ``.train()``. Also note that -1100 is not a good episode return for\n",
    "    # Pendulum-v1, we are using it here to shorten the experiment time.\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"env_runners/episode_return_mean\": -1100.0},\n",
    "    ),\n",
    ")\n",
    "# Run the Tuner and capture the results.\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best checkpoint corresponding to the best result\n",
    "# from the preceding experiment.\n",
    "best_checkpoint = best_result.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce224dd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrl_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RLModule\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Create only the neural network (RLModule) from our algorithm checkpoint.\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# See here (https://docs.ray.io/en/master/rllib/checkpoints.html)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# to learn more about checkpointing and the specific \"path\" used.\u001b[39;00m\n\u001b[32m     10\u001b[39m rl_module = RLModule.from_checkpoint(\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     Path(\u001b[43mbest_checkpoint\u001b[49m.path)\n\u001b[32m     12\u001b[39m     / \u001b[33m\"\u001b[39m\u001b[33mlearner_group\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m     / \u001b[33m\"\u001b[39m\u001b[33mlearner\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m     / \u001b[33m\"\u001b[39m\u001b[33mrl_module\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m     / \u001b[33m\"\u001b[39m\u001b[33mdefault_policy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Create the RL environment to test against (same as was used for training earlier).\u001b[39;00m\n\u001b[32m     19\u001b[39m env = gym.make(\u001b[33m\"\u001b[39m\u001b[33mPendulum-v1\u001b[39m\u001b[33m\"\u001b[39m, render_mode=\u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'best_checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "# Use tune with ray rllib to run hyperparameter tuning experiments\n",
    "\n",
    "from pathlib import Path\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from ray.rllib.core.rl_module import RLModule\n",
    "\n",
    "# Create only the neural network (RLModule) from our algorithm checkpoint.\n",
    "# See here (https://docs.ray.io/en/master/rllib/checkpoints.html)\n",
    "# to learn more about checkpointing and the specific \"path\" used.\n",
    "rl_module = RLModule.from_checkpoint(\n",
    "    Path(best_checkpoint.path)\n",
    "    / \"learner_group\"\n",
    "    / \"learner\"\n",
    "    / \"rl_module\"\n",
    "    / \"default_policy\"\n",
    ")\n",
    "\n",
    "# Create the RL environment to test against (same as was used for training earlier).\n",
    "env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "\n",
    "episode_return = 0.0\n",
    "done = False\n",
    "\n",
    "# Reset the env to get the initial observation.\n",
    "obs, info = env.reset()\n",
    "\n",
    "while not done:\n",
    "    # Uncomment this line to render the env.\n",
    "    # env.render()\n",
    "\n",
    "    # Compute the next action from a batch (B=1) of observations.\n",
    "    obs_batch = torch.from_numpy(obs).unsqueeze(0)  # add batch B=1 dimension\n",
    "    model_outputs = rl_module.forward_inference({\"obs\": obs_batch})\n",
    "\n",
    "    # Extract the action distribution parameters from the output and dissolve batch dim.\n",
    "    action_dist_params = model_outputs[\"action_dist_inputs\"][0].numpy()\n",
    "\n",
    "    # We have continuous actions -> take the mean (max likelihood).\n",
    "    greedy_action = np.clip(\n",
    "        action_dist_params[0:1],  # 0=mean, 1=log(stddev), [0:1]=use mean, but keep shape=(1,)\n",
    "        a_min=env.action_space.low[0],\n",
    "        a_max=env.action_space.high[0],\n",
    "    )\n",
    "    # For discrete actions, you should take the argmax over the logits:\n",
    "    # greedy_action = np.argmax(action_dist_params)\n",
    "\n",
    "    # Send the action to the environment for the next step.\n",
    "    obs, reward, terminated, truncated, info = env.step(greedy_action)\n",
    "\n",
    "    # Perform env-loop bookkeeping.\n",
    "    episode_return += reward\n",
    "    done = terminated or truncated\n",
    "\n",
    "print(f\"Reached episode return of {episode_return}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033065f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "part-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
