{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a442a56",
   "metadata": {},
   "source": [
    "# Trying out MPE2 simple (base model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525da123",
   "metadata": {},
   "source": [
    "First, let's start with the general framework for running the experiment. The simple environment has only one agent and one landmark. First, let's try a random action.\n",
    "It should open a window with one moving particle and a \"landmark\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea19e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAACuCAYAAACm9LxMAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAABBJJREFUeJzt2D1qVFEYgOEvMw5iRlMqIiKIYGNvncrKPjtwFy7EDYiVjZX7sLJJJWgnIUEdnesOxDT+8D5PfS4czoGP99yDZVmWAQAgY/W3NwAAwJ8lAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQc+V3F76883Bqtt+vzvGnR1Pz8frM86eTszmbuf9mco7ufZuTt6dT82Fuz7N5MTVfPp/Ou1dPpmZ9tJ1bJ8dTsz49mptPTqbm7oNr8/r947+9jX+aP4C/sN43j+frZpJWu0nabPdTdDGHU7TfnU/RarOeooPz5kA/vNG878toFg4AQJgABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgQgAECMAAQAiBGAAAAxAhAAIEYAAgDECEAAgBgBCAAQIwABAGIEIABAjAAEAIgRgAAAMQIQACBGAAIAxAhAAIAYAQgAECMAAQBiBCAAQIwABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEAYgTgL/xY7afo6m6S9ptJ2p03x8DhXEzRarOdov3uxxQt2+ZAvzhr3vdlHCzLslzqCwAA/mvNpz8AQJgABACIEYAAADECEAAgRgACAMQIQACAGAEIABAjAAEApuUnRLQ2VUhziKIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = [np.array([0.0, 0.0, 0.0]),\n",
    "          np.array([0.95, 0.0, 0.35]),\n",
    "          np.array([0.35, 0.35, 0.35]), \n",
    "          np.array([0.5, 0.0, 0.35]), \n",
    "          np.array([0.9, 0.85, 0.2]),\n",
    "          np.array([0.2, 0.85, 0.9]),\n",
    "          np.array([0.0, 0.45, 0.05]),\n",
    "          np.array([0.1, 0.85, 0.35]),\n",
    "          np.array([0.2, 0.0, 0.75]),\n",
    "          np.array([1.0, 1.0, 1.0])]\n",
    "\n",
    "# Show the colors as patches\n",
    "plt.figure(figsize=(8, 2))\n",
    "for i, c in enumerate(colors):\n",
    "    plt.fill_between([i, i+1], 0, 1, color=c)\n",
    "plt.xlim(0, len(colors))\n",
    "plt.ylim(0, 1)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c040766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist  0.028284271247461926\n",
      "penetration 0.07171572875253808\n",
      "force [-5.07106781 -5.07106781]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "contact_margin = 1e-3\n",
    "contact_force = 1e2\n",
    "dist_min = 0.1\n",
    "delta_pos = np.array([0.98,0.98]) - np.array([1.0,1.0])\n",
    "# compute Euclidean distance\n",
    "dist = np.sqrt(np.sum(np.square(delta_pos)))\n",
    "penetration = np.logaddexp(0, -(dist - dist_min) / contact_margin) * contact_margin\n",
    "force = contact_force * delta_pos / dist * penetration\n",
    "print(\"dist \", dist)\n",
    "print(\"penetration\", penetration)\n",
    "print(\"force\", force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9070f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, -1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,0]) + np.array([1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5b159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "# Using the \"simple\" environment from the multi-agent-particle-envs (mpe2) package as a template\n",
    "from mpe2 import simple_v3\n",
    "env = simple_v3.parallel_env(max_cycles=100, render_mode='human')\n",
    "\n",
    "env.reset() # Do seed=42 for reproducibility\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        action = env.action_space(agent).sample() # this is where you would insert your policy\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f74da7",
   "metadata": {},
   "source": [
    "We will now use PPO to train the agents. First, we will try with the simple_v3 environment again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d10085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 12:00:32,543\tINFO worker.py:1951 -- Started a local Ray instance.\n",
      "2025-10-08 12:00:34,349\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "2025-10-08 12:00:34,351\tWARNING algorithm_config.py:5045 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "[2025-10-08 12:00:34,426 E 71676 980401] core_worker.cc:2246: Actor with class name: 'MultiAgentEnvRunner' and ID: 'c4b2d99e53e31f298b9fe96f01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=71748)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=71748)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "2025-10-08 12:00:36,781\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=71748)\u001b[0m 2025-10-08 12:00:36,705\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from mpe2 import simple_v3\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "import ray\n",
    "import particle_v1\n",
    "\n",
    "ray.init()#ignore_reinit_error=True) # don't re-init if already running\n",
    "def make_env(env_config=None):\n",
    "    return particle_v1.parallel_env(max_cycles=25, render_mode=None)\n",
    "\n",
    "register_env(\"particle_v1\", lambda config: PettingZooEnv(make_env(config)))\n",
    "\n",
    "# Temporary env instance just to grab one agent's space\n",
    "tmp_env = PettingZooEnv(make_env())\n",
    "first_agent = list(tmp_env.observation_space.keys())[0]\n",
    "obs_space = tmp_env.observation_space[first_agent]\n",
    "act_space = tmp_env.action_space[first_agent]\n",
    "tmp_env.close()\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"particle_v1\")\n",
    "    .training(\n",
    "        train_batch_size_per_learner=2000,\n",
    "        lr=0.0004,\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\"shared_policy\": (None, obs_space, act_space, {})},\n",
    "        policy_mapping_fn=lambda agent_id, *args, **kwargs: \"shared_policy\",\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "#print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "673d48c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date: 2025-10-08_11-49-56\n",
      "done: false\n",
      "env_runner_group:\n",
      "  actor_manager_num_outstanding_async_reqs: 0\n",
      "env_runners:\n",
      "  agent_episode_returns_mean:\n",
      "    agent_0: -16.77413089470129\n",
      "    agent_1: -16.424280441626824\n",
      "    agent_2: -16.46938126045687\n",
      "    agent_3: -27.307958670859485\n",
      "    agent_4: -30.092221446710646\n",
      "    agent_5: -10.026442260182389\n",
      "    agent_6: -20.60375932543154\n",
      "    agent_7: -16.28701155006773\n",
      "    agent_8: -30.907787797650197\n",
      "    agent_9: -38.806422195142204\n",
      "  agent_steps:\n",
      "    agent_0: 25.0\n",
      "    agent_1: 25.0\n",
      "    agent_2: 25.0\n",
      "    agent_3: 25.0\n",
      "    agent_4: 25.0\n",
      "    agent_5: 25.0\n",
      "    agent_6: 25.0\n",
      "    agent_7: 25.0\n",
      "    agent_8: 25.0\n",
      "    agent_9: 25.0\n",
      "  connector_pipeline_timer: 0.0004748544997710269\n",
      "  env_reset_timer: 0.0001611045008758083\n",
      "  env_step_timer: 0.0001164026690279647\n",
      "  env_to_module_connector:\n",
      "    connector_pipeline_timer: 6.887857190600966e-05\n",
      "    timers:\n",
      "      connectors:\n",
      "        add_observations_from_episodes_to_batch: 1.4704572538911084e-05\n",
      "        add_states_from_episodes_to_batch: 2.0197284080037448e-06\n",
      "        add_time_dim_to_batch_and_zero_pad: 2.7835176585786497e-06\n",
      "        agent_to_module_mapping: 2.2579832659832163e-06\n",
      "        batch_individual_items: 6.749198398319976e-06\n",
      "        numpy_to_tensor: 1.0332435812103531e-05\n",
      "  env_to_module_sum_episodes_length_in: 170.17444947649753\n",
      "  env_to_module_sum_episodes_length_out: 170.17444947649753\n",
      "  episode_duration_sec_mean: 0.1253420833754717\n",
      "  episode_len_max: 250\n",
      "  episode_len_mean: 250.0\n",
      "  episode_len_min: 250\n",
      "  episode_return_max: -145.4000720239673\n",
      "  episode_return_mean: -223.69939584282918\n",
      "  episode_return_min: -329.1754791133242\n",
      "  module_episode_returns_mean:\n",
      "    shared_policy: -38.806422195142204\n",
      "  module_to_env_connector:\n",
      "    connector_pipeline_timer: 0.0001298152131420445\n",
      "    timers:\n",
      "      connectors:\n",
      "        get_actions: 4.804768326915171e-05\n",
      "        listify_data_for_vector_env: 4.530004949167176e-06\n",
      "        module_to_agent_unmapping: 1.9050909969471926e-06\n",
      "        normalize_and_clip_actions: 1.096194057321756e-05\n",
      "        remove_single_ts_time_rank_from_batch: 7.487551891891987e-07\n",
      "        tensor_to_numpy: 1.922588787956583e-05\n",
      "        un_batch_to_individual_items: 8.689388771252835e-06\n",
      "  num_agent_steps_sampled:\n",
      "    agent_0: 200.0\n",
      "    agent_1: 208.0\n",
      "    agent_2: 208.0\n",
      "    agent_3: 208.0\n",
      "    agent_4: 208.0\n",
      "    agent_5: 208.0\n",
      "    agent_6: 208.0\n",
      "    agent_7: 208.0\n",
      "    agent_8: 208.0\n",
      "    agent_9: 208.0\n",
      "  num_agent_steps_sampled_lifetime:\n",
      "    agent_0: 200.0\n",
      "    agent_1: 208.0\n",
      "    agent_2: 208.0\n",
      "    agent_3: 208.0\n",
      "    agent_4: 208.0\n",
      "    agent_5: 208.0\n",
      "    agent_6: 208.0\n",
      "    agent_7: 208.0\n",
      "    agent_8: 208.0\n",
      "    agent_9: 208.0\n",
      "  num_env_steps_sampled: 2000.0\n",
      "  num_env_steps_sampled_lifetime: 2000.0\n",
      "  num_env_steps_sampled_lifetime_throughput: 2315.8304624663447\n",
      "  num_episodes: 8.0\n",
      "  num_episodes_lifetime: 8.0\n",
      "  num_module_steps_sampled:\n",
      "    shared_policy: 2072.0\n",
      "  num_module_steps_sampled_lifetime:\n",
      "    shared_policy: 2072.0\n",
      "  rlmodule_inference_timer: 3.7839153526291237e-05\n",
      "  sample: 0.5107271454999136\n",
      "  timers:\n",
      "    connectors:\n",
      "      add_observations_from_episodes_to_batch: 4.943800013279542e-05\n",
      "      add_states_from_episodes_to_batch: 7.020999873930123e-06\n",
      "      add_time_dim_to_batch_and_zero_pad: 2.3457999304810073e-05\n",
      "      agent_to_module_mapping: 7.916999493318144e-06\n",
      "      batch_individual_items: 3.4438000511727296e-05\n",
      "      numpy_to_tensor: 6.291600038821343e-05\n",
      "  weights_seq_no: 0.0\n",
      "fault_tolerance:\n",
      "  num_healthy_workers: 2\n",
      "  num_remote_worker_restarts: 0\n",
      "hostname: scadsdp25.misc.intern.uni-leipzig.de\n",
      "iterations_since_restore: 1\n",
      "learners:\n",
      "  __all_modules__:\n",
      "    learner_connector:\n",
      "      connector_pipeline_timer: 0.05031391699958476\n",
      "      timers:\n",
      "        connectors:\n",
      "          add_columns_from_episodes_to_train_batch: 0.017312041998593486\n",
      "          add_observations_from_episodes_to_batch: 0.0003563329992175568\n",
      "          add_one_ts_to_episodes_and_truncate: 0.003262374999394524\n",
      "          add_states_from_episodes_to_batch: 3.3750002330634743e-06\n",
      "          add_time_dim_to_batch_and_zero_pad: 3.0125000193947926e-05\n",
      "          agent_to_module_mapping: 0.0009189579996018438\n",
      "          batch_individual_items: 0.014192750000802334\n",
      "          general_advantage_estimation: 0.013983916000142926\n",
      "          numpy_to_tensor: 0.00010637500054144766\n",
      "    learner_connector_sum_episodes_length_in: 2000\n",
      "    learner_connector_sum_episodes_length_out: 2000\n",
      "    num_env_steps_trained: 976000\n",
      "    num_env_steps_trained_lifetime: 976000\n",
      "    num_env_steps_trained_lifetime_throughput: 879486.541591912\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 62464\n",
      "    num_module_steps_trained_lifetime_throughput: 56285.508496176\n",
      "    num_module_steps_trained_throughput: 56284.006193617024\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 135686\n",
      "  shared_policy:\n",
      "    curr_entropy_coeff: 0.0\n",
      "    curr_kl_coeff: 0.20000000298023224\n",
      "    default_optimizer_learning_rate: 0.0004\n",
      "    diff_num_grad_updates_vs_sampler_policy: 1.0\n",
      "    entropy: 1.5852469205856323\n",
      "    gradients_default_optimizer_global_norm: 1.7780988216400146\n",
      "    mean_kl_loss: 0.013795716688036919\n",
      "    module_train_batch_size_mean: 128.0\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 62464\n",
      "    num_module_steps_trained_lifetime_throughput: 56282.97440510027\n",
      "    num_trainable_parameters: 135686\n",
      "    policy_loss: -0.04737553000450134\n",
      "    total_loss: 7.072839736938477\n",
      "    vf_explained_var: -0.004670143127441406\n",
      "    vf_loss: 7.117456436157227\n",
      "    vf_loss_unclipped: 164.33517456054688\n",
      "    weights_seq_no: 1.0\n",
      "node_ip: 127.0.0.1\n",
      "num_env_steps_sampled_lifetime: 2000.0\n",
      "num_training_step_calls_per_iteration: 1\n",
      "perf:\n",
      "  cpu_util_percent: 8.606976744186046\n",
      "  ram_util_percent: 63.77441860465115\n",
      "pid: 39523\n",
      "time_since_restore: 1.7330279350280762\n",
      "time_this_iter_s: 1.7330279350280762\n",
      "time_total_s: 1.7330279350280762\n",
      "timers:\n",
      "  env_runner_sampling_timer: 0.5313151249993098\n",
      "  learner_update_timer: 1.1939642079996702\n",
      "  restore_env_runners: 4.600000102072954e-05\n",
      "  synch_weights: 0.0013729160000366392\n",
      "  training_iteration: 1.727722083000117\n",
      "  training_step: 1.727427792000526\n",
      "timestamp: 1759916996\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "date: 2025-10-08_11-49-57\n",
      "done: false\n",
      "env_runner_group:\n",
      "  actor_manager_num_outstanding_async_reqs: 0\n",
      "env_runners:\n",
      "  agent_episode_returns_mean:\n",
      "    agent_0: -25.342863404637903\n",
      "    agent_1: -19.846800990837593\n",
      "    agent_2: -25.04325156500364\n",
      "    agent_3: -30.912010430854952\n",
      "    agent_4: -22.64056350506437\n",
      "    agent_5: -20.418116556418095\n",
      "    agent_6: -27.025931443052663\n",
      "    agent_7: -22.82492692036172\n",
      "    agent_8: -29.207139533564725\n",
      "    agent_9: -41.16773325117897\n",
      "  agent_steps:\n",
      "    agent_0: 25.0\n",
      "    agent_1: 25.0\n",
      "    agent_2: 25.0\n",
      "    agent_3: 25.0\n",
      "    agent_4: 25.0\n",
      "    agent_5: 25.0\n",
      "    agent_6: 25.0\n",
      "    agent_7: 25.0\n",
      "    agent_8: 25.0\n",
      "    agent_9: 25.0\n",
      "  connector_pipeline_timer: 0.0004748544997710269\n",
      "  env_reset_timer: 0.0001611045008758083\n",
      "  env_step_timer: 0.00011697599649381419\n",
      "  env_to_module_connector:\n",
      "    connector_pipeline_timer: 6.879243333322174e-05\n",
      "    timers:\n",
      "      connectors:\n",
      "        add_observations_from_episodes_to_batch: 1.4685087889969068e-05\n",
      "        add_states_from_episodes_to_batch: 2.0449330371332882e-06\n",
      "        add_time_dim_to_batch_and_zero_pad: 2.953737866443522e-06\n",
      "        agent_to_module_mapping: 2.2722892603103413e-06\n",
      "        batch_individual_items: 6.820176116281154e-06\n",
      "        numpy_to_tensor: 1.0334521717778513e-05\n",
      "  env_to_module_sum_episodes_length_in: 170.18150663270126\n",
      "  env_to_module_sum_episodes_length_out: 170.18150663270126\n",
      "  episode_duration_sec_mean: 0.12309169793763886\n",
      "  episode_len_max: 250\n",
      "  episode_len_mean: 250.0\n",
      "  episode_len_min: 250\n",
      "  episode_return_max: -145.4000720239673\n",
      "  episode_return_mean: -264.4293376009746\n",
      "  episode_return_min: -457.35961946546286\n",
      "  module_episode_returns_mean:\n",
      "    shared_policy: -41.16773325117897\n",
      "  module_to_env_connector:\n",
      "    connector_pipeline_timer: 0.0001306439595223067\n",
      "    timers:\n",
      "      connectors:\n",
      "        get_actions: 4.846295957318793e-05\n",
      "        listify_data_for_vector_env: 4.736284879631273e-06\n",
      "        module_to_agent_unmapping: 1.9233336401288845e-06\n",
      "        normalize_and_clip_actions: 1.1019592219042685e-05\n",
      "        remove_single_ts_time_rank_from_batch: 7.54669402732064e-07\n",
      "        tensor_to_numpy: 1.9323088905005378e-05\n",
      "        un_batch_to_individual_items: 8.783042162166892e-06\n",
      "  num_agent_steps_sampled:\n",
      "    agent_0: 200.0\n",
      "    agent_1: 208.0\n",
      "    agent_2: 208.0\n",
      "    agent_3: 208.0\n",
      "    agent_4: 208.0\n",
      "    agent_5: 208.0\n",
      "    agent_6: 208.0\n",
      "    agent_7: 208.0\n",
      "    agent_8: 208.0\n",
      "    agent_9: 208.0\n",
      "  num_agent_steps_sampled_lifetime:\n",
      "    agent_0: 400.0\n",
      "    agent_1: 416.0\n",
      "    agent_2: 416.0\n",
      "    agent_3: 416.0\n",
      "    agent_4: 416.0\n",
      "    agent_5: 416.0\n",
      "    agent_6: 416.0\n",
      "    agent_7: 416.0\n",
      "    agent_8: 416.0\n",
      "    agent_9: 416.0\n",
      "  num_env_steps_sampled: 2000.0\n",
      "  num_env_steps_sampled_lifetime: 4000.0\n",
      "  num_env_steps_sampled_lifetime_throughput: 2310.1081753843437\n",
      "  num_episodes: 8.0\n",
      "  num_episodes_lifetime: 16.0\n",
      "  num_module_steps_sampled:\n",
      "    shared_policy: 2072.0\n",
      "  num_module_steps_sampled_lifetime:\n",
      "    shared_policy: 4144.0\n",
      "  rlmodule_inference_timer: 3.8868317643661714e-05\n",
      "  sample: 0.5105417271699115\n",
      "  time_between_sampling: 1.2339727504995608\n",
      "  timers:\n",
      "    connectors:\n",
      "      add_observations_from_episodes_to_batch: 4.943800013279542e-05\n",
      "      add_states_from_episodes_to_batch: 7.020999873930123e-06\n",
      "      add_time_dim_to_batch_and_zero_pad: 2.3457999304810073e-05\n",
      "      agent_to_module_mapping: 7.916999493318144e-06\n",
      "      batch_individual_items: 3.4438000511727296e-05\n",
      "      numpy_to_tensor: 6.291600038821343e-05\n",
      "  weights_seq_no: 1.0\n",
      "fault_tolerance:\n",
      "  num_healthy_workers: 2\n",
      "  num_remote_worker_restarts: 0\n",
      "hostname: scadsdp25.misc.intern.uni-leipzig.de\n",
      "iterations_since_restore: 2\n",
      "learners:\n",
      "  __all_modules__:\n",
      "    learner_connector:\n",
      "      connector_pipeline_timer: 0.05021608449958876\n",
      "      timers:\n",
      "        connectors:\n",
      "          add_columns_from_episodes_to_train_batch: 0.017311741998619255\n",
      "          add_observations_from_episodes_to_batch: 0.0003564392492262414\n",
      "          add_one_ts_to_episodes_and_truncate: 0.0032634074994166437\n",
      "          add_states_from_episodes_to_batch: 3.371250240888912e-06\n",
      "          add_time_dim_to_batch_and_zero_pad: 3.0126250203466045e-05\n",
      "          agent_to_module_mapping: 0.0009193571696050639\n",
      "          batch_individual_items: 0.014200972080798239\n",
      "          general_advantage_estimation: 0.01387652143013838\n",
      "          numpy_to_tensor: 0.00010652292054146529\n",
      "    learner_connector_sum_episodes_length_in: 2000.0\n",
      "    learner_connector_sum_episodes_length_out: 2000.0\n",
      "    num_env_steps_trained: 976000\n",
      "    num_env_steps_trained_lifetime: 1952000\n",
      "    num_env_steps_trained_lifetime_throughput: 880372.6591834931\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 124928\n",
      "    num_module_steps_trained_lifetime_throughput: 56342.22579049101\n",
      "    num_module_steps_trained_throughput: 56340.77039809417\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 135686\n",
      "  shared_policy:\n",
      "    curr_entropy_coeff: 0.0\n",
      "    curr_kl_coeff: 0.20000000298023224\n",
      "    default_optimizer_learning_rate: 0.0004\n",
      "    diff_num_grad_updates_vs_sampler_policy: 1.0\n",
      "    entropy: 1.5365861654281616\n",
      "    gradients_default_optimizer_global_norm: 0.9368252754211426\n",
      "    mean_kl_loss: 0.019106650725007057\n",
      "    module_train_batch_size_mean: 128.0\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 124928\n",
      "    num_module_steps_trained_lifetime_throughput: 56339.74164796929\n",
      "    num_trainable_parameters: 135686\n",
      "    policy_loss: 0.10155880451202393\n",
      "    total_loss: 7.654353141784668\n",
      "    vf_explained_var: 0.03500849008560181\n",
      "    vf_loss: 7.548973083496094\n",
      "    vf_loss_unclipped: 302.76019287109375\n",
      "    weights_seq_no: 2.0\n",
      "node_ip: 127.0.0.1\n",
      "num_env_steps_sampled_lifetime: 4000.0\n",
      "num_training_step_calls_per_iteration: 1\n",
      "perf:\n",
      "  cpu_util_percent: 20.7\n",
      "  ram_util_percent: 63.36666666666667\n",
      "pid: 39523\n",
      "time_since_restore: 3.4103758335113525\n",
      "time_this_iter_s: 1.6773478984832764\n",
      "time_total_s: 3.4103758335113525\n",
      "timers:\n",
      "  env_runner_sampling_timer: 0.5311217395893073\n",
      "  learner_update_timer: 1.193600202589678\n",
      "  restore_env_runners: 4.562167101539671e-05\n",
      "  synch_env_connectors: 0.0030953330006013857\n",
      "  synch_weights: 0.001373807250038226\n",
      "  training_iteration: 1.7271659913401163\n",
      "  training_step: 1.7268716274205145\n",
      "timestamp: 1759916997\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "\n",
      "date: 2025-10-08_11-49-59\n",
      "done: false\n",
      "env_runner_group:\n",
      "  actor_manager_num_outstanding_async_reqs: 0\n",
      "env_runners:\n",
      "  agent_episode_returns_mean:\n",
      "    agent_0: -25.473397189874333\n",
      "    agent_1: -19.761670008050313\n",
      "    agent_2: -22.262519018486064\n",
      "    agent_3: -28.33945644248904\n",
      "    agent_4: -24.887322704920148\n",
      "    agent_5: -21.83324736167548\n",
      "    agent_6: -21.203490458721348\n",
      "    agent_7: -21.10371718372338\n",
      "    agent_8: -24.756516433100614\n",
      "    agent_9: -40.696797486046954\n",
      "  agent_steps:\n",
      "    agent_0: 25.0\n",
      "    agent_1: 25.0\n",
      "    agent_2: 25.0\n",
      "    agent_3: 25.0\n",
      "    agent_4: 25.0\n",
      "    agent_5: 25.0\n",
      "    agent_6: 25.0\n",
      "    agent_7: 25.0\n",
      "    agent_8: 25.0\n",
      "    agent_9: 25.0\n",
      "  connector_pipeline_timer: 0.0004748544997710269\n",
      "  env_reset_timer: 0.0001611045008758083\n",
      "  env_step_timer: 0.00011581738509624762\n",
      "  env_to_module_connector:\n",
      "    connector_pipeline_timer: 6.855906144420426e-05\n",
      "    timers:\n",
      "      connectors:\n",
      "        add_observations_from_episodes_to_batch: 1.4599963350179371e-05\n",
      "        add_states_from_episodes_to_batch: 2.0609836581447266e-06\n",
      "        add_time_dim_to_batch_and_zero_pad: 2.845719409853522e-06\n",
      "        agent_to_module_mapping: 2.2899965557878867e-06\n",
      "        batch_individual_items: 6.819645084487169e-06\n",
      "        numpy_to_tensor: 1.038572430069392e-05\n",
      "  env_to_module_sum_episodes_length_in: 170.18150692536236\n",
      "  env_to_module_sum_episodes_length_out: 170.18150692536236\n",
      "  episode_duration_sec_mean: 0.12277839070839036\n",
      "  episode_len_max: 250\n",
      "  episode_len_mean: 250.0\n",
      "  episode_len_min: 250\n",
      "  episode_return_max: -143.50404182199946\n",
      "  episode_return_mean: -250.31813428708764\n",
      "  episode_return_min: -457.35961946546286\n",
      "  module_episode_returns_mean:\n",
      "    shared_policy: -40.696797486046954\n",
      "  module_to_env_connector:\n",
      "    connector_pipeline_timer: 0.00012934028945555412\n",
      "    timers:\n",
      "      connectors:\n",
      "        get_actions: 4.772523372343121e-05\n",
      "        listify_data_for_vector_env: 4.583981779361476e-06\n",
      "        module_to_agent_unmapping: 1.910599478254013e-06\n",
      "        normalize_and_clip_actions: 1.0976466921260455e-05\n",
      "        remove_single_ts_time_rank_from_batch: 7.463541984033091e-07\n",
      "        tensor_to_numpy: 1.9379163725412736e-05\n",
      "        un_batch_to_individual_items: 8.714212826847945e-06\n",
      "  num_agent_steps_sampled:\n",
      "    agent_0: 200.0\n",
      "    agent_1: 208.0\n",
      "    agent_2: 208.0\n",
      "    agent_3: 208.0\n",
      "    agent_4: 208.0\n",
      "    agent_5: 208.0\n",
      "    agent_6: 208.0\n",
      "    agent_7: 208.0\n",
      "    agent_8: 208.0\n",
      "    agent_9: 208.0\n",
      "  num_agent_steps_sampled_lifetime:\n",
      "    agent_0: 600.0\n",
      "    agent_1: 624.0\n",
      "    agent_2: 624.0\n",
      "    agent_3: 624.0\n",
      "    agent_4: 624.0\n",
      "    agent_5: 624.0\n",
      "    agent_6: 624.0\n",
      "    agent_7: 624.0\n",
      "    agent_8: 624.0\n",
      "    agent_9: 624.0\n",
      "  num_env_steps_sampled: 2000.0\n",
      "  num_env_steps_sampled_lifetime: 6000.0\n",
      "  num_env_steps_sampled_lifetime_throughput: 2309.523559789042\n",
      "  num_episodes: 8.0\n",
      "  num_episodes_lifetime: 24.0\n",
      "  num_module_steps_sampled:\n",
      "    shared_policy: 2072.0\n",
      "  num_module_steps_sampled_lifetime:\n",
      "    shared_policy: 6216.0\n",
      "  rlmodule_inference_timer: 3.8360348814242205e-05\n",
      "  sample: 0.5104119415632142\n",
      "  time_between_sampling: 1.2335979704945568\n",
      "  timers:\n",
      "    connectors:\n",
      "      add_observations_from_episodes_to_batch: 4.943800013279542e-05\n",
      "      add_states_from_episodes_to_batch: 7.020999873930123e-06\n",
      "      add_time_dim_to_batch_and_zero_pad: 2.3457999304810073e-05\n",
      "      agent_to_module_mapping: 7.916999493318144e-06\n",
      "      batch_individual_items: 3.4438000511727296e-05\n",
      "      numpy_to_tensor: 6.291600038821343e-05\n",
      "  weights_seq_no: 2.0\n",
      "fault_tolerance:\n",
      "  num_healthy_workers: 2\n",
      "  num_remote_worker_restarts: 0\n",
      "hostname: scadsdp25.misc.intern.uni-leipzig.de\n",
      "iterations_since_restore: 3\n",
      "learners:\n",
      "  __all_modules__:\n",
      "    learner_connector:\n",
      "      connector_pipeline_timer: 0.05011384281460159\n",
      "      timers:\n",
      "        connectors:\n",
      "          add_columns_from_episodes_to_train_batch: 0.017308121248638238\n",
      "          add_observations_from_episodes_to_batch: 0.00035670110672872396\n",
      "          add_one_ts_to_episodes_and_truncate: 0.0032634413344223502\n",
      "          add_states_from_episodes_to_batch: 3.3675377304462015e-06\n",
      "          add_time_dim_to_batch_and_zero_pad: 3.0090407686657272e-05\n",
      "          agent_to_module_mapping: 0.000919984847914202\n",
      "          batch_individual_items: 0.014205774859978556\n",
      "          general_advantage_estimation: 0.013772334965837943\n",
      "          numpy_to_tensor: 0.00010657227134506685\n",
      "    learner_connector_sum_episodes_length_in: 2000.0\n",
      "    learner_connector_sum_episodes_length_out: 2000.0\n",
      "    num_env_steps_trained: 976000\n",
      "    num_env_steps_trained_lifetime: 2928000\n",
      "    num_env_steps_trained_lifetime_throughput: 881498.8019980463\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 187392\n",
      "    num_module_steps_trained_lifetime_throughput: 56414.29970894589\n",
      "    num_module_steps_trained_throughput: 56412.916066157246\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 135686\n",
      "  shared_policy:\n",
      "    curr_entropy_coeff: 0.0\n",
      "    curr_kl_coeff: 0.30000001192092896\n",
      "    default_optimizer_learning_rate: 0.0004\n",
      "    diff_num_grad_updates_vs_sampler_policy: 1.0\n",
      "    entropy: 1.5046857595443726\n",
      "    gradients_default_optimizer_global_norm: 1.3932387828826904\n",
      "    mean_kl_loss: 0.02189597114920616\n",
      "    module_train_batch_size_mean: 128.0\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 187392\n",
      "    num_module_steps_trained_lifetime_throughput: 56411.93111928864\n",
      "    num_trainable_parameters: 135686\n",
      "    policy_loss: 0.05918320268392563\n",
      "    total_loss: 5.565495014190674\n",
      "    vf_explained_var: 0.1700848937034607\n",
      "    vf_loss: 5.501932621002197\n",
      "    vf_loss_unclipped: 137.7261962890625\n",
      "    weights_seq_no: 3.0\n",
      "node_ip: 127.0.0.1\n",
      "num_env_steps_sampled_lifetime: 6000.0\n",
      "num_training_step_calls_per_iteration: 1\n",
      "perf:\n",
      "  cpu_util_percent: 21.9\n",
      "  ram_util_percent: 63.599999999999994\n",
      "pid: 39523\n",
      "time_since_restore: 5.135758876800537\n",
      "time_this_iter_s: 1.7253830432891846\n",
      "time_total_s: 5.135758876800537\n",
      "timers:\n",
      "  env_runner_sampling_timer: 0.5309906709434193\n",
      "  learner_update_timer: 1.1936598513937942\n",
      "  restore_env_runners: 4.524003430778975e-05\n",
      "  synch_env_connectors: 0.003095716330590221\n",
      "  synch_weights: 0.0013732683475269369\n",
      "  training_iteration: 1.7270939830867271\n",
      "  training_step: 1.7267998094763124\n",
      "timestamp: 1759916999\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "date: 2025-10-08_11-50-01\n",
      "done: false\n",
      "env_runner_group:\n",
      "  actor_manager_num_outstanding_async_reqs: 0\n",
      "env_runners:\n",
      "  agent_episode_returns_mean:\n",
      "    agent_0: -20.9753746020436\n",
      "    agent_1: -18.638577086604222\n",
      "    agent_2: -22.66832285963288\n",
      "    agent_3: -27.641778627033336\n",
      "    agent_4: -26.1321926856168\n",
      "    agent_5: -24.07048653136667\n",
      "    agent_6: -17.976253198163427\n",
      "    agent_7: -21.01727098598817\n",
      "    agent_8: -22.039728446554847\n",
      "    agent_9: -32.98326300786895\n",
      "  agent_steps:\n",
      "    agent_0: 25.0\n",
      "    agent_1: 25.0\n",
      "    agent_2: 25.0\n",
      "    agent_3: 25.0\n",
      "    agent_4: 25.0\n",
      "    agent_5: 25.0\n",
      "    agent_6: 25.0\n",
      "    agent_7: 25.0\n",
      "    agent_8: 25.0\n",
      "    agent_9: 25.0\n",
      "  connector_pipeline_timer: 0.0004748544997710269\n",
      "  env_reset_timer: 0.0001611045008758083\n",
      "  env_step_timer: 0.00011565363013366878\n",
      "  env_to_module_connector:\n",
      "    connector_pipeline_timer: 6.810344115716852e-05\n",
      "    timers:\n",
      "      connectors:\n",
      "        add_observations_from_episodes_to_batch: 1.4587188647090553e-05\n",
      "        add_states_from_episodes_to_batch: 2.081289903662311e-06\n",
      "        add_time_dim_to_batch_and_zero_pad: 2.7785818214016435e-06\n",
      "        agent_to_module_mapping: 2.246988142956086e-06\n",
      "        batch_individual_items: 6.684674171238647e-06\n",
      "        numpy_to_tensor: 1.0099197919541578e-05\n",
      "  env_to_module_sum_episodes_length_in: 170.18150692537458\n",
      "  env_to_module_sum_episodes_length_out: 170.18150692537458\n",
      "  episode_duration_sec_mean: 0.12246283468761021\n",
      "  episode_len_max: 250\n",
      "  episode_len_mean: 250.0\n",
      "  episode_len_min: 250\n",
      "  episode_return_max: -137.15806760421722\n",
      "  episode_return_mean: -234.14324803087288\n",
      "  episode_return_min: -457.35961946546286\n",
      "  module_episode_returns_mean:\n",
      "    shared_policy: -32.98326300786895\n",
      "  module_to_env_connector:\n",
      "    connector_pipeline_timer: 0.00012838562569284276\n",
      "    timers:\n",
      "      connectors:\n",
      "        get_actions: 4.747210907517323e-05\n",
      "        listify_data_for_vector_env: 4.55603580072172e-06\n",
      "        module_to_agent_unmapping: 1.9021832372065651e-06\n",
      "        normalize_and_clip_actions: 1.0883999693657242e-05\n",
      "        remove_single_ts_time_rank_from_batch: 7.482077571238343e-07\n",
      "        tensor_to_numpy: 1.8921061480103676e-05\n",
      "        un_batch_to_individual_items: 8.560268469871096e-06\n",
      "  num_agent_steps_sampled:\n",
      "    agent_0: 200.0\n",
      "    agent_1: 208.0\n",
      "    agent_2: 208.0\n",
      "    agent_3: 208.0\n",
      "    agent_4: 208.0\n",
      "    agent_5: 208.0\n",
      "    agent_6: 208.0\n",
      "    agent_7: 208.0\n",
      "    agent_8: 208.0\n",
      "    agent_9: 208.0\n",
      "  num_agent_steps_sampled_lifetime:\n",
      "    agent_0: 800.0\n",
      "    agent_1: 832.0\n",
      "    agent_2: 832.0\n",
      "    agent_3: 832.0\n",
      "    agent_4: 832.0\n",
      "    agent_5: 832.0\n",
      "    agent_6: 832.0\n",
      "    agent_7: 832.0\n",
      "    agent_8: 832.0\n",
      "    agent_9: 832.0\n",
      "  num_env_steps_sampled: 2000.0\n",
      "  num_env_steps_sampled_lifetime: 8000.0\n",
      "  num_env_steps_sampled_lifetime_throughput: 2309.7259822622127\n",
      "  num_episodes: 8.0\n",
      "  num_episodes_lifetime: 32.0\n",
      "  num_module_steps_sampled:\n",
      "    shared_policy: 2072.0\n",
      "  num_module_steps_sampled_lifetime:\n",
      "    shared_policy: 8288.0\n",
      "  rlmodule_inference_timer: 3.775452653118395e-05\n",
      "  sample: 0.5102553898575783\n",
      "  time_between_sampling: 1.233661255999608\n",
      "  timers:\n",
      "    connectors:\n",
      "      add_observations_from_episodes_to_batch: 4.943800013279542e-05\n",
      "      add_states_from_episodes_to_batch: 7.020999873930123e-06\n",
      "      add_time_dim_to_batch_and_zero_pad: 2.3457999304810073e-05\n",
      "      agent_to_module_mapping: 7.916999493318144e-06\n",
      "      batch_individual_items: 3.4438000511727296e-05\n",
      "      numpy_to_tensor: 6.291600038821343e-05\n",
      "  weights_seq_no: 3.0\n",
      "fault_tolerance:\n",
      "  num_healthy_workers: 2\n",
      "  num_remote_worker_restarts: 0\n",
      "hostname: scadsdp25.misc.intern.uni-leipzig.de\n",
      "iterations_since_restore: 4\n",
      "learners:\n",
      "  __all_modules__:\n",
      "    learner_connector:\n",
      "      connector_pipeline_timer: 0.050013720216467175\n",
      "      timers:\n",
      "        connectors:\n",
      "          add_columns_from_episodes_to_train_batch: 0.017306552956139292\n",
      "          add_observations_from_episodes_to_batch: 0.0003565782556542326\n",
      "          add_one_ts_to_episodes_and_truncate: 0.003263183171079234\n",
      "          add_states_from_episodes_to_batch: 3.3601123529333563e-06\n",
      "          add_time_dim_to_batch_and_zero_pad: 2.998158361573587e-05\n",
      "          agent_to_module_mapping: 0.0009201204194349029\n",
      "          batch_individual_items: 0.014211170031388299\n",
      "          general_advantage_estimation: 0.013668698276177702\n",
      "          numpy_to_tensor: 0.00010696154863305857\n",
      "    learner_connector_sum_episodes_length_in: 2000.0\n",
      "    learner_connector_sum_episodes_length_out: 2000.0\n",
      "    num_env_steps_trained: 976000\n",
      "    num_env_steps_trained_lifetime: 3904000\n",
      "    num_env_steps_trained_lifetime_throughput: 883399.4068259608\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 249856\n",
      "    num_module_steps_trained_lifetime_throughput: 56536.00588374801\n",
      "    num_module_steps_trained_throughput: 56534.69212648693\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 135686\n",
      "  shared_policy:\n",
      "    curr_entropy_coeff: 0.0\n",
      "    curr_kl_coeff: 0.30000001192092896\n",
      "    default_optimizer_learning_rate: 0.0004\n",
      "    diff_num_grad_updates_vs_sampler_policy: 1.0\n",
      "    entropy: 1.4988166093826294\n",
      "    gradients_default_optimizer_global_norm: 2.9669911861419678\n",
      "    mean_kl_loss: 0.015066522173583508\n",
      "    module_train_batch_size_mean: 128.0\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 249856\n",
      "    num_module_steps_trained_lifetime_throughput: 56533.73078450116\n",
      "    num_trainable_parameters: 135686\n",
      "    policy_loss: 0.3047134578227997\n",
      "    total_loss: 5.852443695068359\n",
      "    vf_explained_var: 0.14864426851272583\n",
      "    vf_loss: 5.543210029602051\n",
      "    vf_loss_unclipped: 144.26295471191406\n",
      "    weights_seq_no: 4.0\n",
      "node_ip: 127.0.0.1\n",
      "num_env_steps_sampled_lifetime: 8000.0\n",
      "num_training_step_calls_per_iteration: 1\n",
      "perf:\n",
      "  cpu_util_percent: 21.35\n",
      "  ram_util_percent: 63.8\n",
      "pid: 39523\n",
      "time_since_restore: 6.822256803512573\n",
      "time_this_iter_s: 1.6864979267120361\n",
      "time_total_s: 6.822256803512573\n",
      "timers:\n",
      "  env_runner_sampling_timer: 0.5308382300639829\n",
      "  learner_update_timer: 1.1933559870398498\n",
      "  restore_env_runners: 4.485596397423796e-05\n",
      "  synch_env_connectors: 0.0030989399972915633\n",
      "  synch_weights: 0.001373815664065636\n",
      "  training_iteration: 1.726637869085848\n",
      "  training_step: 1.7263439472215587\n",
      "timestamp: 1759917001\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "\n",
      "date: 2025-10-08_11-50-03\n",
      "done: false\n",
      "env_runner_group:\n",
      "  actor_manager_num_outstanding_async_reqs: 0\n",
      "env_runners:\n",
      "  agent_episode_returns_mean:\n",
      "    agent_0: -20.0412173220948\n",
      "    agent_1: -18.878366158266413\n",
      "    agent_2: -22.35321716242519\n",
      "    agent_3: -26.472151312652443\n",
      "    agent_4: -26.30232105165925\n",
      "    agent_5: -25.43366755019756\n",
      "    agent_6: -17.276397088424297\n",
      "    agent_7: -19.242378297850273\n",
      "    agent_8: -21.165611161602435\n",
      "    agent_9: -29.504406374227443\n",
      "  agent_steps:\n",
      "    agent_0: 25.0\n",
      "    agent_1: 25.0\n",
      "    agent_2: 25.0\n",
      "    agent_3: 25.0\n",
      "    agent_4: 25.0\n",
      "    agent_5: 25.0\n",
      "    agent_6: 25.0\n",
      "    agent_7: 25.0\n",
      "    agent_8: 25.0\n",
      "    agent_9: 25.0\n",
      "  connector_pipeline_timer: 0.0004748544997710269\n",
      "  env_reset_timer: 0.0001611045008758083\n",
      "  env_step_timer: 0.00011747517795394811\n",
      "  env_to_module_connector:\n",
      "    connector_pipeline_timer: 6.997882470773784e-05\n",
      "    timers:\n",
      "      connectors:\n",
      "        add_observations_from_episodes_to_batch: 1.495711196574883e-05\n",
      "        add_states_from_episodes_to_batch: 2.097625057401168e-06\n",
      "        add_time_dim_to_batch_and_zero_pad: 2.8575946128925177e-06\n",
      "        agent_to_module_mapping: 2.3022799008965042e-06\n",
      "        batch_individual_items: 6.830093336879938e-06\n",
      "        numpy_to_tensor: 1.0288754244936127e-05\n",
      "  env_to_module_sum_episodes_length_in: 170.18150692537458\n",
      "  env_to_module_sum_episodes_length_out: 170.18150692537458\n",
      "  episode_duration_sec_mean: 0.12245728130010321\n",
      "  episode_len_max: 250\n",
      "  episode_len_mean: 250.0\n",
      "  episode_len_min: 250\n",
      "  episode_return_max: -137.15806760421722\n",
      "  episode_return_mean: -226.66973347940007\n",
      "  episode_return_min: -457.35961946546286\n",
      "  module_episode_returns_mean:\n",
      "    shared_policy: -29.504406374227443\n",
      "  module_to_env_connector:\n",
      "    connector_pipeline_timer: 0.00013056386456119007\n",
      "    timers:\n",
      "      connectors:\n",
      "        get_actions: 4.7855019016528175e-05\n",
      "        listify_data_for_vector_env: 4.72370341599215e-06\n",
      "        module_to_agent_unmapping: 1.9627533332725897e-06\n",
      "        normalize_and_clip_actions: 1.1087216009776449e-05\n",
      "        remove_single_ts_time_rank_from_batch: 7.646637455274689e-07\n",
      "        tensor_to_numpy: 1.929392002073516e-05\n",
      "        un_batch_to_individual_items: 8.813940282232252e-06\n",
      "  num_agent_steps_sampled:\n",
      "    agent_0: 200.0\n",
      "    agent_1: 208.0\n",
      "    agent_2: 208.0\n",
      "    agent_3: 208.0\n",
      "    agent_4: 208.0\n",
      "    agent_5: 208.0\n",
      "    agent_6: 208.0\n",
      "    agent_7: 208.0\n",
      "    agent_8: 208.0\n",
      "    agent_9: 208.0\n",
      "  num_agent_steps_sampled_lifetime:\n",
      "    agent_0: 1000.0\n",
      "    agent_1: 1040.0\n",
      "    agent_2: 1040.0\n",
      "    agent_3: 1040.0\n",
      "    agent_4: 1040.0\n",
      "    agent_5: 1040.0\n",
      "    agent_6: 1040.0\n",
      "    agent_7: 1040.0\n",
      "    agent_8: 1040.0\n",
      "    agent_9: 1040.0\n",
      "  num_env_steps_sampled: 2000.0\n",
      "  num_env_steps_sampled_lifetime: 10000.0\n",
      "  num_env_steps_sampled_lifetime_throughput: 2304.285923761962\n",
      "  num_episodes: 8.0\n",
      "  num_episodes_lifetime: 40.0\n",
      "  num_module_steps_sampled:\n",
      "    shared_policy: 2072.0\n",
      "  num_module_steps_sampled_lifetime:\n",
      "    shared_policy: 10360.0\n",
      "  rlmodule_inference_timer: 3.800673580928269e-05\n",
      "  sample: 0.510135478664002\n",
      "  time_between_sampling: 1.2333604334396209\n",
      "  timers:\n",
      "    connectors:\n",
      "      add_observations_from_episodes_to_batch: 4.943800013279542e-05\n",
      "      add_states_from_episodes_to_batch: 7.020999873930123e-06\n",
      "      add_time_dim_to_batch_and_zero_pad: 2.3457999304810073e-05\n",
      "      agent_to_module_mapping: 7.916999493318144e-06\n",
      "      batch_individual_items: 3.4438000511727296e-05\n",
      "      numpy_to_tensor: 6.291600038821343e-05\n",
      "  weights_seq_no: 4.0\n",
      "fault_tolerance:\n",
      "  num_healthy_workers: 2\n",
      "  num_remote_worker_restarts: 0\n",
      "hostname: scadsdp25.misc.intern.uni-leipzig.de\n",
      "iterations_since_restore: 5\n",
      "learners:\n",
      "  __all_modules__:\n",
      "    learner_connector:\n",
      "      connector_pipeline_timer: 0.04997762051430787\n",
      "      timers:\n",
      "        connectors:\n",
      "          add_columns_from_episodes_to_train_batch: 0.017306138266573082\n",
      "          add_observations_from_episodes_to_batch: 0.00035754914310098037\n",
      "          add_one_ts_to_episodes_and_truncate: 0.0033157725893606152\n",
      "          add_states_from_episodes_to_batch: 3.358591217563262e-06\n",
      "          add_time_dim_to_batch_and_zero_pad: 2.996593778024689e-05\n",
      "          agent_to_module_mapping: 0.0009201642152416551\n",
      "          batch_individual_items: 0.014223102501087055\n",
      "          general_advantage_estimation: 0.013566200873417037\n",
      "          numpy_to_tensor: 0.00010723609315619457\n",
      "    learner_connector_sum_episodes_length_in: 2000.0\n",
      "    learner_connector_sum_episodes_length_out: 2000.0\n",
      "    num_env_steps_trained: 976000\n",
      "    num_env_steps_trained_lifetime: 4880000\n",
      "    num_env_steps_trained_lifetime_throughput: 882370.7814950696\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 312320\n",
      "    num_module_steps_trained_lifetime_throughput: 56470.218090976305\n",
      "    num_module_steps_trained_throughput: 56468.922233038444\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 135686\n",
      "  shared_policy:\n",
      "    curr_entropy_coeff: 0.0\n",
      "    curr_kl_coeff: 0.30000001192092896\n",
      "    default_optimizer_learning_rate: 0.0004\n",
      "    diff_num_grad_updates_vs_sampler_policy: 1.0\n",
      "    entropy: 1.446930170059204\n",
      "    gradients_default_optimizer_global_norm: 1.8251296281814575\n",
      "    mean_kl_loss: 0.016237832605838776\n",
      "    module_train_batch_size_mean: 128.0\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 312320\n",
      "    num_module_steps_trained_lifetime_throughput: 56467.94402552997\n",
      "    num_trainable_parameters: 135686\n",
      "    policy_loss: 0.005334796849638224\n",
      "    total_loss: 5.23384428024292\n",
      "    vf_explained_var: 0.24912339448928833\n",
      "    vf_loss: 5.223638534545898\n",
      "    vf_loss_unclipped: 63.529659271240234\n",
      "    weights_seq_no: 5.0\n",
      "node_ip: 127.0.0.1\n",
      "num_env_steps_sampled_lifetime: 10000.0\n",
      "num_training_step_calls_per_iteration: 1\n",
      "perf:\n",
      "  cpu_util_percent: 20.0\n",
      "  ram_util_percent: 63.53333333333333\n",
      "pid: 39523\n",
      "time_since_restore: 8.518202781677246\n",
      "time_this_iter_s: 1.6959459781646729\n",
      "time_total_s: 8.518202781677246\n",
      "timers:\n",
      "  env_runner_sampling_timer: 0.5307379027633459\n",
      "  learner_update_timer: 1.1930922717494434\n",
      "  restore_env_runners: 4.4484064333235655e-05\n",
      "  synch_env_connectors: 0.003097219767327422\n",
      "  synch_weights: 0.001376390847426923\n",
      "  training_iteration: 1.7262795758149896\n",
      "  training_step: 1.7259855469093535\n",
      "timestamp: 1759917003\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "date: 2025-10-08_11-50-04\n",
      "done: false\n",
      "env_runner_group:\n",
      "  actor_manager_num_outstanding_async_reqs: 0\n",
      "env_runners:\n",
      "  agent_episode_returns_mean:\n",
      "    agent_0: -19.1061519613322\n",
      "    agent_1: -18.93892789706638\n",
      "    agent_2: -21.70234143275178\n",
      "    agent_3: -27.14318077377835\n",
      "    agent_4: -24.8963323294077\n",
      "    agent_5: -23.78300440997079\n",
      "    agent_6: -17.259270548158856\n",
      "    agent_7: -18.721819469467675\n",
      "    agent_8: -20.524703463483707\n",
      "    agent_9: -28.049087860764228\n",
      "  agent_steps:\n",
      "    agent_0: 25.0\n",
      "    agent_1: 25.0\n",
      "    agent_2: 25.0\n",
      "    agent_3: 25.0\n",
      "    agent_4: 25.0\n",
      "    agent_5: 25.0\n",
      "    agent_6: 25.0\n",
      "    agent_7: 25.0\n",
      "    agent_8: 25.0\n",
      "    agent_9: 25.0\n",
      "  connector_pipeline_timer: 0.0004748544997710269\n",
      "  env_reset_timer: 0.0001611045008758083\n",
      "  env_step_timer: 0.00011947587252510215\n",
      "  env_to_module_connector:\n",
      "    connector_pipeline_timer: 7.105429959308969e-05\n",
      "    timers:\n",
      "      connectors:\n",
      "        add_observations_from_episodes_to_batch: 1.5136452417372481e-05\n",
      "        add_states_from_episodes_to_batch: 2.1097541792354574e-06\n",
      "        add_time_dim_to_batch_and_zero_pad: 2.948255428613629e-06\n",
      "        agent_to_module_mapping: 2.355628287132334e-06\n",
      "        batch_individual_items: 7.1147244429299975e-06\n",
      "        numpy_to_tensor: 1.0727812537646921e-05\n",
      "  env_to_module_sum_episodes_length_in: 170.18150692537458\n",
      "  env_to_module_sum_episodes_length_out: 170.18150692537458\n",
      "  episode_duration_sec_mean: 0.12282204433339909\n",
      "  episode_len_max: 250\n",
      "  episode_len_mean: 250.0\n",
      "  episode_len_min: 250\n",
      "  episode_return_max: -115.42038194004243\n",
      "  episode_return_mean: -220.1248201461817\n",
      "  episode_return_min: -457.35961946546286\n",
      "  module_episode_returns_mean:\n",
      "    shared_policy: -28.049087860764228\n",
      "  module_to_env_connector:\n",
      "    connector_pipeline_timer: 0.00013309126011649687\n",
      "    timers:\n",
      "      connectors:\n",
      "        get_actions: 4.890321083980273e-05\n",
      "        listify_data_for_vector_env: 4.7874897187417955e-06\n",
      "        module_to_agent_unmapping: 1.9783510053908486e-06\n",
      "        normalize_and_clip_actions: 1.1416293675006792e-05\n",
      "        remove_single_ts_time_rank_from_batch: 7.682242652657504e-07\n",
      "        tensor_to_numpy: 2.001065957191894e-05\n",
      "        un_batch_to_individual_items: 8.931133546124254e-06\n",
      "  num_agent_steps_sampled:\n",
      "    agent_0: 200.0\n",
      "    agent_1: 208.0\n",
      "    agent_2: 208.0\n",
      "    agent_3: 208.0\n",
      "    agent_4: 208.0\n",
      "    agent_5: 208.0\n",
      "    agent_6: 208.0\n",
      "    agent_7: 208.0\n",
      "    agent_8: 208.0\n",
      "    agent_9: 208.0\n",
      "  num_agent_steps_sampled_lifetime:\n",
      "    agent_0: 1200.0\n",
      "    agent_1: 1248.0\n",
      "    agent_2: 1248.0\n",
      "    agent_3: 1248.0\n",
      "    agent_4: 1248.0\n",
      "    agent_5: 1248.0\n",
      "    agent_6: 1248.0\n",
      "    agent_7: 1248.0\n",
      "    agent_8: 1248.0\n",
      "    agent_9: 1248.0\n",
      "  num_env_steps_sampled: 2000.0\n",
      "  num_env_steps_sampled_lifetime: 12000.0\n",
      "  num_env_steps_sampled_lifetime_throughput: 2295.8805331720687\n",
      "  num_episodes: 8.0\n",
      "  num_episodes_lifetime: 48.0\n",
      "  num_module_steps_sampled:\n",
      "    shared_policy: 2072.0\n",
      "  num_module_steps_sampled_lifetime:\n",
      "    shared_policy: 12432.0\n",
      "  rlmodule_inference_timer: 3.8997439842670145e-05\n",
      "  sample: 0.510110136167363\n",
      "  time_between_sampling: 1.233118742850232\n",
      "  timers:\n",
      "    connectors:\n",
      "      add_observations_from_episodes_to_batch: 4.943800013279542e-05\n",
      "      add_states_from_episodes_to_batch: 7.020999873930123e-06\n",
      "      add_time_dim_to_batch_and_zero_pad: 2.3457999304810073e-05\n",
      "      agent_to_module_mapping: 7.916999493318144e-06\n",
      "      batch_individual_items: 3.4438000511727296e-05\n",
      "      numpy_to_tensor: 6.291600038821343e-05\n",
      "  weights_seq_no: 5.0\n",
      "fault_tolerance:\n",
      "  num_healthy_workers: 2\n",
      "  num_remote_worker_restarts: 0\n",
      "hostname: scadsdp25.misc.intern.uni-leipzig.de\n",
      "iterations_since_restore: 6\n",
      "learners:\n",
      "  __all_modules__:\n",
      "    learner_connector:\n",
      "      connector_pipeline_timer: 0.04989331722915929\n",
      "      timers:\n",
      "        connectors:\n",
      "          add_columns_from_episodes_to_train_batch: 0.01730939188389705\n",
      "          add_observations_from_episodes_to_batch: 0.00035790365167255383\n",
      "          add_one_ts_to_episodes_and_truncate: 0.0033175319534641885\n",
      "          add_states_from_episodes_to_batch: 3.3562553068719245e-06\n",
      "          add_time_dim_to_batch_and_zero_pad: 3.0119608396787645e-05\n",
      "          agent_to_module_mapping: 0.000920245073103035\n",
      "          batch_individual_items: 0.01423351564606962\n",
      "          general_advantage_estimation: 0.013465476364668837\n",
      "          numpy_to_tensor: 0.00010742623223366644\n",
      "    learner_connector_sum_episodes_length_in: 2000.0\n",
      "    learner_connector_sum_episodes_length_out: 2000.0\n",
      "    num_env_steps_trained: 976000\n",
      "    num_env_steps_trained_lifetime: 5856000\n",
      "    num_env_steps_trained_lifetime_throughput: 880467.2586045531\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 374784\n",
      "    num_module_steps_trained_lifetime_throughput: 56348.482461130785\n",
      "    num_module_steps_trained_throughput: 56347.26914530876\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 135686\n",
      "  shared_policy:\n",
      "    curr_entropy_coeff: 0.0\n",
      "    curr_kl_coeff: 0.30000001192092896\n",
      "    default_optimizer_learning_rate: 0.0004\n",
      "    diff_num_grad_updates_vs_sampler_policy: 1.0\n",
      "    entropy: 1.4348578453063965\n",
      "    gradients_default_optimizer_global_norm: 3.178487777709961\n",
      "    mean_kl_loss: 0.01273680292069912\n",
      "    module_train_batch_size_mean: 128.0\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 374784\n",
      "    num_module_steps_trained_lifetime_throughput: 56346.325296319555\n",
      "    num_trainable_parameters: 135686\n",
      "    policy_loss: -0.1274234503507614\n",
      "    total_loss: 5.408777713775635\n",
      "    vf_explained_var: 0.41987526416778564\n",
      "    vf_loss: 5.532380104064941\n",
      "    vf_loss_unclipped: 24.16583251953125\n",
      "    weights_seq_no: 6.0\n",
      "node_ip: 127.0.0.1\n",
      "num_env_steps_sampled_lifetime: 12000.0\n",
      "num_training_step_calls_per_iteration: 1\n",
      "perf:\n",
      "  cpu_util_percent: 26.8\n",
      "  ram_util_percent: 63.8\n",
      "pid: 39523\n",
      "time_since_restore: 10.318701028823853\n",
      "time_this_iter_s: 1.8004982471466064\n",
      "time_total_s: 10.318701028823853\n",
      "timers:\n",
      "  env_runner_sampling_timer: 0.5307423845657189\n",
      "  learner_update_timer: 1.193777119861947\n",
      "  restore_env_runners: 4.4109643681874295e-05\n",
      "  synch_env_connectors: 0.0030962175696501036\n",
      "  synch_weights: 0.0013761910989517315\n",
      "  training_iteration: 1.7269686763068366\n",
      "  training_step: 1.7266746489402534\n",
      "timestamp: 1759917004\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "\n",
      "date: 2025-10-08_11-50-06\n",
      "done: false\n",
      "env_runner_group:\n",
      "  actor_manager_num_outstanding_async_reqs: 0\n",
      "env_runners:\n",
      "  agent_episode_returns_mean:\n",
      "    agent_0: -18.476286731346896\n",
      "    agent_1: -18.668831869751735\n",
      "    agent_2: -22.06021593413342\n",
      "    agent_3: -23.980471962961243\n",
      "    agent_4: -23.148782041055966\n",
      "    agent_5: -23.905156090445054\n",
      "    agent_6: -18.402681069534726\n",
      "    agent_7: -17.265303328796495\n",
      "    agent_8: -20.196597840928213\n",
      "    agent_9: -27.951408068059873\n",
      "  agent_steps:\n",
      "    agent_0: 25.0\n",
      "    agent_1: 25.0\n",
      "    agent_2: 25.0\n",
      "    agent_3: 25.0\n",
      "    agent_4: 25.0\n",
      "    agent_5: 25.0\n",
      "    agent_6: 25.0\n",
      "    agent_7: 25.0\n",
      "    agent_8: 25.0\n",
      "    agent_9: 25.0\n",
      "  connector_pipeline_timer: 0.0004748544997710269\n",
      "  env_reset_timer: 0.0001611045008758083\n",
      "  env_step_timer: 0.00011782148394674732\n",
      "  env_to_module_connector:\n",
      "    connector_pipeline_timer: 6.92620380187994e-05\n",
      "    timers:\n",
      "      connectors:\n",
      "        add_observations_from_episodes_to_batch: 1.4829289234175906e-05\n",
      "        add_states_from_episodes_to_batch: 2.0743991977073203e-06\n",
      "        add_time_dim_to_batch_and_zero_pad: 2.853267761100985e-06\n",
      "        agent_to_module_mapping: 2.291270718462699e-06\n",
      "        batch_individual_items: 6.7675729904205085e-06\n",
      "        numpy_to_tensor: 1.031398037212864e-05\n",
      "  env_to_module_sum_episodes_length_in: 170.18150692537458\n",
      "  env_to_module_sum_episodes_length_out: 170.18150692537458\n",
      "  episode_duration_sec_mean: 0.12231701755998074\n",
      "  episode_len_max: 250\n",
      "  episode_len_mean: 250.0\n",
      "  episode_len_min: 250\n",
      "  episode_return_max: -115.42038194004243\n",
      "  episode_return_mean: -214.05573493701363\n",
      "  episode_return_min: -457.35961946546286\n",
      "  module_episode_returns_mean:\n",
      "    shared_policy: -27.951408068059873\n",
      "  module_to_env_connector:\n",
      "    connector_pipeline_timer: 0.00013020047011965706\n",
      "    timers:\n",
      "      connectors:\n",
      "        get_actions: 4.808330186840547e-05\n",
      "        listify_data_for_vector_env: 4.580539142404844e-06\n",
      "        module_to_agent_unmapping: 1.9226242315627923e-06\n",
      "        normalize_and_clip_actions: 1.1050560829812597e-05\n",
      "        remove_single_ts_time_rank_from_batch: 7.571508080155795e-07\n",
      "        tensor_to_numpy: 1.912216746461653e-05\n",
      "        un_batch_to_individual_items: 8.694018794816701e-06\n",
      "  num_agent_steps_sampled:\n",
      "    agent_0: 200.0\n",
      "    agent_1: 208.0\n",
      "    agent_2: 208.0\n",
      "    agent_3: 208.0\n",
      "    agent_4: 208.0\n",
      "    agent_5: 208.0\n",
      "    agent_6: 208.0\n",
      "    agent_7: 208.0\n",
      "    agent_8: 208.0\n",
      "    agent_9: 208.0\n",
      "  num_agent_steps_sampled_lifetime:\n",
      "    agent_0: 1400.0\n",
      "    agent_1: 1456.0\n",
      "    agent_2: 1456.0\n",
      "    agent_3: 1456.0\n",
      "    agent_4: 1456.0\n",
      "    agent_5: 1456.0\n",
      "    agent_6: 1456.0\n",
      "    agent_7: 1456.0\n",
      "    agent_8: 1456.0\n",
      "    agent_9: 1456.0\n",
      "  num_env_steps_sampled: 2000.0\n",
      "  num_env_steps_sampled_lifetime: 14000.0\n",
      "  num_env_steps_sampled_lifetime_throughput: 2297.063645041377\n",
      "  num_episodes: 8.0\n",
      "  num_episodes_lifetime: 56.0\n",
      "  num_module_steps_sampled:\n",
      "    shared_policy: 2072.0\n",
      "  num_module_steps_sampled_lifetime:\n",
      "    shared_policy: 14504.0\n",
      "  rlmodule_inference_timer: 3.8769494246229895e-05\n",
      "  sample: 0.5099852810556877\n",
      "  time_between_sampling: 1.2338482316717272\n",
      "  timers:\n",
      "    connectors:\n",
      "      add_observations_from_episodes_to_batch: 4.943800013279542e-05\n",
      "      add_states_from_episodes_to_batch: 7.020999873930123e-06\n",
      "      add_time_dim_to_batch_and_zero_pad: 2.3457999304810073e-05\n",
      "      agent_to_module_mapping: 7.916999493318144e-06\n",
      "      batch_individual_items: 3.4438000511727296e-05\n",
      "      numpy_to_tensor: 6.291600038821343e-05\n",
      "  weights_seq_no: 6.0\n",
      "fault_tolerance:\n",
      "  num_healthy_workers: 2\n",
      "  num_remote_worker_restarts: 0\n",
      "hostname: scadsdp25.misc.intern.uni-leipzig.de\n",
      "iterations_since_restore: 7\n",
      "learners:\n",
      "  __all_modules__:\n",
      "    learner_connector:\n",
      "      connector_pipeline_timer: 0.04980143572687621\n",
      "      timers:\n",
      "        connectors:\n",
      "          add_columns_from_episodes_to_train_batch: 0.017316130465056236\n",
      "          add_observations_from_episodes_to_batch: 0.0003580883651476047\n",
      "          add_one_ts_to_episodes_and_truncate: 0.0033187028839245427\n",
      "          add_states_from_episodes_to_batch: 3.357272760998678e-06\n",
      "          add_time_dim_to_batch_and_zero_pad: 3.00471623084053e-05\n",
      "          agent_to_module_mapping: 0.000920380542372694\n",
      "          batch_individual_items: 0.014237094659597752\n",
      "          general_advantage_estimation: 0.013362385351021043\n",
      "          numpy_to_tensor: 0.0001073169699105987\n",
      "    learner_connector_sum_episodes_length_in: 2000.0\n",
      "    learner_connector_sum_episodes_length_out: 2000.0\n",
      "    num_env_steps_trained: 976000\n",
      "    num_env_steps_trained_lifetime: 6832000\n",
      "    num_env_steps_trained_lifetime_throughput: 881928.7218526568\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 437248\n",
      "    num_module_steps_trained_lifetime_throughput: 56442.019647939\n",
      "    num_module_steps_trained_throughput: 56440.81784046676\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 135686\n",
      "  shared_policy:\n",
      "    curr_entropy_coeff: 0.0\n",
      "    curr_kl_coeff: 0.30000001192092896\n",
      "    default_optimizer_learning_rate: 0.0004\n",
      "    diff_num_grad_updates_vs_sampler_policy: 1.0\n",
      "    entropy: 1.3868207931518555\n",
      "    gradients_default_optimizer_global_norm: 12.240942001342773\n",
      "    mean_kl_loss: 0.018343785777688026\n",
      "    module_train_batch_size_mean: 128.0\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 437248\n",
      "    num_module_steps_trained_lifetime_throughput: 56439.9267682982\n",
      "    num_trainable_parameters: 135686\n",
      "    policy_loss: -0.1609179973602295\n",
      "    total_loss: 3.8956544399261475\n",
      "    vf_explained_var: 0.7390017509460449\n",
      "    vf_loss: 4.051069259643555\n",
      "    vf_loss_unclipped: 8.229652404785156\n",
      "    weights_seq_no: 7.0\n",
      "node_ip: 127.0.0.1\n",
      "num_env_steps_sampled_lifetime: 14000.0\n",
      "num_training_step_calls_per_iteration: 1\n",
      "perf:\n",
      "  cpu_util_percent: 21.433333333333334\n",
      "  ram_util_percent: 63.833333333333336\n",
      "pid: 39523\n",
      "time_since_restore: 12.038672924041748\n",
      "time_this_iter_s: 1.7199718952178955\n",
      "time_total_s: 12.038672924041748\n",
      "timers:\n",
      "  env_runner_sampling_timer: 0.5306702715600601\n",
      "  learner_update_timer: 1.1937169528333313\n",
      "  restore_env_runners: 4.3737717245698325e-05\n",
      "  synch_env_connectors: 0.0030967291439584093\n",
      "  synch_weights: 0.0013766350179556575\n",
      "  training_iteration: 1.7268374574537584\n",
      "  training_step: 1.7265433174508442\n",
      "timestamp: 1759917006\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "date: 2025-10-08_11-50-08\n",
      "done: false\n",
      "env_runner_group:\n",
      "  actor_manager_num_outstanding_async_reqs: 0\n",
      "env_runners:\n",
      "  agent_episode_returns_mean:\n",
      "    agent_0: -15.15413768944909\n",
      "    agent_1: -19.26992821216639\n",
      "    agent_2: -18.189997947958638\n",
      "    agent_3: -24.70411938391305\n",
      "    agent_4: -22.491782950400758\n",
      "    agent_5: -22.661289314678857\n",
      "    agent_6: -15.445557741368738\n",
      "    agent_7: -17.506023585558616\n",
      "    agent_8: -16.846157034842868\n",
      "    agent_9: -23.605008469336486\n",
      "  agent_steps:\n",
      "    agent_0: 25.0\n",
      "    agent_1: 25.0\n",
      "    agent_2: 25.0\n",
      "    agent_3: 25.0\n",
      "    agent_4: 25.0\n",
      "    agent_5: 25.0\n",
      "    agent_6: 25.0\n",
      "    agent_7: 25.0\n",
      "    agent_8: 25.0\n",
      "    agent_9: 25.0\n",
      "  connector_pipeline_timer: 0.0004748544997710269\n",
      "  env_reset_timer: 0.0001611045008758083\n",
      "  env_step_timer: 0.00011867798803045788\n",
      "  env_to_module_connector:\n",
      "    connector_pipeline_timer: 7.084296674609049e-05\n",
      "    timers:\n",
      "      connectors:\n",
      "        add_observations_from_episodes_to_batch: 1.54101552250887e-05\n",
      "        add_states_from_episodes_to_batch: 2.123390285443653e-06\n",
      "        add_time_dim_to_batch_and_zero_pad: 2.91033721764891e-06\n",
      "        agent_to_module_mapping: 2.328551752840656e-06\n",
      "        batch_individual_items: 6.878333367046306e-06\n",
      "        numpy_to_tensor: 1.0394888831064479e-05\n",
      "  env_to_module_sum_episodes_length_in: 170.18150692537458\n",
      "  env_to_module_sum_episodes_length_out: 170.18150692537458\n",
      "  episode_duration_sec_mean: 0.12269331585994223\n",
      "  episode_len_max: 250\n",
      "  episode_len_mean: 250.0\n",
      "  episode_len_min: 250\n",
      "  episode_return_max: -115.42038194004243\n",
      "  episode_return_mean: -195.8740023296735\n",
      "  episode_return_min: -378.41681460181104\n",
      "  module_episode_returns_mean:\n",
      "    shared_policy: -23.605008469336486\n",
      "  module_to_env_connector:\n",
      "    connector_pipeline_timer: 0.00013107680605748288\n",
      "    timers:\n",
      "      connectors:\n",
      "        get_actions: 4.772039162721702e-05\n",
      "        listify_data_for_vector_env: 4.728949993507606e-06\n",
      "        module_to_agent_unmapping: 2.0113972496810556e-06\n",
      "        normalize_and_clip_actions: 1.1371386374580823e-05\n",
      "        remove_single_ts_time_rank_from_batch: 7.782365646583027e-07\n",
      "        tensor_to_numpy: 1.9278024581579665e-05\n",
      "        un_batch_to_individual_items: 8.839930830005229e-06\n",
      "  num_agent_steps_sampled:\n",
      "    agent_0: 200.0\n",
      "    agent_1: 208.0\n",
      "    agent_2: 208.0\n",
      "    agent_3: 208.0\n",
      "    agent_4: 208.0\n",
      "    agent_5: 208.0\n",
      "    agent_6: 208.0\n",
      "    agent_7: 208.0\n",
      "    agent_8: 208.0\n",
      "    agent_9: 208.0\n",
      "  num_agent_steps_sampled_lifetime:\n",
      "    agent_0: 1600.0\n",
      "    agent_1: 1664.0\n",
      "    agent_2: 1664.0\n",
      "    agent_3: 1664.0\n",
      "    agent_4: 1664.0\n",
      "    agent_5: 1664.0\n",
      "    agent_6: 1664.0\n",
      "    agent_7: 1664.0\n",
      "    agent_8: 1664.0\n",
      "    agent_9: 1664.0\n",
      "  num_env_steps_sampled: 2000.0\n",
      "  num_env_steps_sampled_lifetime: 16000.0\n",
      "  num_env_steps_sampled_lifetime_throughput: 2292.2447761467834\n",
      "  num_episodes: 8.0\n",
      "  num_episodes_lifetime: 64.0\n",
      "  num_module_steps_sampled:\n",
      "    shared_policy: 2072.0\n",
      "  num_module_steps_sampled_lifetime:\n",
      "    shared_policy: 16576.0\n",
      "  rlmodule_inference_timer: 3.826287039061273e-05\n",
      "  sample: 0.5099094461601332\n",
      "  time_between_sampling: 1.2338435131100054\n",
      "  timers:\n",
      "    connectors:\n",
      "      add_observations_from_episodes_to_batch: 4.943800013279542e-05\n",
      "      add_states_from_episodes_to_batch: 7.020999873930123e-06\n",
      "      add_time_dim_to_batch_and_zero_pad: 2.3457999304810073e-05\n",
      "      agent_to_module_mapping: 7.916999493318144e-06\n",
      "      batch_individual_items: 3.4438000511727296e-05\n",
      "      numpy_to_tensor: 6.291600038821343e-05\n",
      "  weights_seq_no: 7.0\n",
      "fault_tolerance:\n",
      "  num_healthy_workers: 2\n",
      "  num_remote_worker_restarts: 0\n",
      "hostname: scadsdp25.misc.intern.uni-leipzig.de\n",
      "iterations_since_restore: 8\n",
      "learners:\n",
      "  __all_modules__:\n",
      "    learner_connector:\n",
      "      connector_pipeline_timer: 0.04970783428960486\n",
      "      timers:\n",
      "        connectors:\n",
      "          add_columns_from_episodes_to_train_batch: 0.017316005410404078\n",
      "          add_observations_from_episodes_to_batch: 0.0003578124814872159\n",
      "          add_one_ts_to_episodes_and_truncate: 0.003318621275074012\n",
      "          add_states_from_episodes_to_batch: 3.3520300475632626e-06\n",
      "          add_time_dim_to_batch_and_zero_pad: 2.9894190680685592e-05\n",
      "          agent_to_module_mapping: 0.0009203617369479461\n",
      "          batch_individual_items: 0.014243796632995843\n",
      "          general_advantage_estimation: 0.013263133167518946\n",
      "          numpy_to_tensor: 0.0001072888002014647\n",
      "    learner_connector_sum_episodes_length_in: 2000.0\n",
      "    learner_connector_sum_episodes_length_out: 2000.0\n",
      "    num_env_steps_trained: 976000\n",
      "    num_env_steps_trained_lifetime: 7808000\n",
      "    num_env_steps_trained_lifetime_throughput: 882490.8408715382\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 499712\n",
      "    num_module_steps_trained_lifetime_throughput: 56478.09599106338\n",
      "    num_module_steps_trained_throughput: 56476.96209610982\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 135686\n",
      "  shared_policy:\n",
      "    curr_entropy_coeff: 0.0\n",
      "    curr_kl_coeff: 0.45000001788139343\n",
      "    default_optimizer_learning_rate: 0.0004\n",
      "    diff_num_grad_updates_vs_sampler_policy: 1.0\n",
      "    entropy: 1.411752462387085\n",
      "    gradients_default_optimizer_global_norm: 3.373366594314575\n",
      "    mean_kl_loss: 0.02201024815440178\n",
      "    module_train_batch_size_mean: 128.0\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 499712\n",
      "    num_module_steps_trained_lifetime_throughput: 56476.15193497215\n",
      "    num_trainable_parameters: 135686\n",
      "    policy_loss: 0.08587440103292465\n",
      "    total_loss: 5.600189208984375\n",
      "    vf_explained_var: 0.42464733123779297\n",
      "    vf_loss: 5.507710933685303\n",
      "    vf_loss_unclipped: 64.66014862060547\n",
      "    weights_seq_no: 8.0\n",
      "node_ip: 127.0.0.1\n",
      "num_env_steps_sampled_lifetime: 16000.0\n",
      "num_training_step_calls_per_iteration: 1\n",
      "perf:\n",
      "  cpu_util_percent: 22.55\n",
      "  ram_util_percent: 63.3\n",
      "pid: 39523\n",
      "time_since_restore: 13.88507080078125\n",
      "time_this_iter_s: 1.846397876739502\n",
      "time_total_s: 13.88507080078125\n",
      "timers:\n",
      "  env_runner_sampling_timer: 0.5317532159344593\n",
      "  learner_update_timer: 1.1937740428850039\n",
      "  restore_env_runners: 4.337826006775135e-05\n",
      "  synch_env_connectors: 0.003106659772526235\n",
      "  synch_weights: 0.0013770244977889974\n",
      "  training_iteration: 1.7279785528792266\n",
      "  training_step: 1.7276843301063414\n",
      "timestamp: 1759917008\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "\n",
      "date: 2025-10-08_11-50-10\n",
      "done: false\n",
      "env_runner_group:\n",
      "  actor_manager_num_outstanding_async_reqs: 0\n",
      "env_runners:\n",
      "  agent_episode_returns_mean:\n",
      "    agent_0: -14.204007488356101\n",
      "    agent_1: -19.87141712980666\n",
      "    agent_2: -19.43796242654857\n",
      "    agent_3: -22.957366670012817\n",
      "    agent_4: -20.813844915514053\n",
      "    agent_5: -18.97688197251197\n",
      "    agent_6: -16.36336267444493\n",
      "    agent_7: -16.354033775970358\n",
      "    agent_8: -17.926246863648053\n",
      "    agent_9: -19.522604177246944\n",
      "  agent_steps:\n",
      "    agent_0: 25.0\n",
      "    agent_1: 25.0\n",
      "    agent_2: 25.0\n",
      "    agent_3: 25.0\n",
      "    agent_4: 25.0\n",
      "    agent_5: 25.0\n",
      "    agent_6: 25.0\n",
      "    agent_7: 25.0\n",
      "    agent_8: 25.0\n",
      "    agent_9: 25.0\n",
      "  connector_pipeline_timer: 0.0004748544997710269\n",
      "  env_reset_timer: 0.0001611045008758083\n",
      "  env_step_timer: 0.00011798744382767831\n",
      "  env_to_module_connector:\n",
      "    connector_pipeline_timer: 7.03534630490648e-05\n",
      "    timers:\n",
      "      connectors:\n",
      "        add_observations_from_episodes_to_batch: 1.4989760300771605e-05\n",
      "        add_states_from_episodes_to_batch: 2.1005154442136837e-06\n",
      "        add_time_dim_to_batch_and_zero_pad: 2.911348256736602e-06\n",
      "        agent_to_module_mapping: 2.3607968644696213e-06\n",
      "        batch_individual_items: 6.926492936938941e-06\n",
      "        numpy_to_tensor: 1.0449435556449058e-05\n",
      "  env_to_module_sum_episodes_length_in: 170.18150692537458\n",
      "  env_to_module_sum_episodes_length_out: 170.18150692537458\n",
      "  episode_duration_sec_mean: 0.12298201670004345\n",
      "  episode_len_max: 250\n",
      "  episode_len_mean: 250.0\n",
      "  episode_len_min: 250\n",
      "  episode_return_max: -100.63492481928171\n",
      "  episode_return_mean: -186.42772809406043\n",
      "  episode_return_min: -378.41681460181104\n",
      "  module_episode_returns_mean:\n",
      "    shared_policy: -19.522604177246944\n",
      "  module_to_env_connector:\n",
      "    connector_pipeline_timer: 0.00013085027931573104\n",
      "    timers:\n",
      "      connectors:\n",
      "        get_actions: 4.8019499103004316e-05\n",
      "        listify_data_for_vector_env: 4.597797542708682e-06\n",
      "        module_to_agent_unmapping: 1.974695441265037e-06\n",
      "        normalize_and_clip_actions: 1.1143555414742757e-05\n",
      "        remove_single_ts_time_rank_from_batch: 7.746915376014074e-07\n",
      "        tensor_to_numpy: 1.9484684707303224e-05\n",
      "        un_batch_to_individual_items: 8.88288307605294e-06\n",
      "  num_agent_steps_sampled:\n",
      "    agent_0: 200.0\n",
      "    agent_1: 208.0\n",
      "    agent_2: 208.0\n",
      "    agent_3: 208.0\n",
      "    agent_4: 208.0\n",
      "    agent_5: 208.0\n",
      "    agent_6: 208.0\n",
      "    agent_7: 208.0\n",
      "    agent_8: 208.0\n",
      "    agent_9: 208.0\n",
      "  num_agent_steps_sampled_lifetime:\n",
      "    agent_0: 1800.0\n",
      "    agent_1: 1872.0\n",
      "    agent_2: 1872.0\n",
      "    agent_3: 1872.0\n",
      "    agent_4: 1872.0\n",
      "    agent_5: 1872.0\n",
      "    agent_6: 1872.0\n",
      "    agent_7: 1872.0\n",
      "    agent_8: 1872.0\n",
      "    agent_9: 1872.0\n",
      "  num_env_steps_sampled: 2000.0\n",
      "  num_env_steps_sampled_lifetime: 18000.0\n",
      "  num_env_steps_sampled_lifetime_throughput: 2288.6745780513074\n",
      "  num_episodes: 8.0\n",
      "  num_episodes_lifetime: 72.0\n",
      "  num_module_steps_sampled:\n",
      "    shared_policy: 2072.0\n",
      "  num_module_steps_sampled_lifetime:\n",
      "    shared_policy: 18648.0\n",
      "  rlmodule_inference_timer: 3.840785382442128e-05\n",
      "  sample: 0.5098531031585369\n",
      "  time_between_sampling: 1.2350648046439128\n",
      "  timers:\n",
      "    connectors:\n",
      "      add_observations_from_episodes_to_batch: 4.943800013279542e-05\n",
      "      add_states_from_episodes_to_batch: 7.020999873930123e-06\n",
      "      add_time_dim_to_batch_and_zero_pad: 2.3457999304810073e-05\n",
      "      agent_to_module_mapping: 7.916999493318144e-06\n",
      "      batch_individual_items: 3.4438000511727296e-05\n",
      "      numpy_to_tensor: 6.291600038821343e-05\n",
      "  weights_seq_no: 8.0\n",
      "fault_tolerance:\n",
      "  num_healthy_workers: 2\n",
      "  num_remote_worker_restarts: 0\n",
      "hostname: scadsdp25.misc.intern.uni-leipzig.de\n",
      "iterations_since_restore: 9\n",
      "learners:\n",
      "  __all_modules__:\n",
      "    learner_connector:\n",
      "      connector_pipeline_timer: 0.04969154760670178\n",
      "      timers:\n",
      "        connectors:\n",
      "          add_columns_from_episodes_to_train_batch: 0.017325437436300214\n",
      "          add_observations_from_episodes_to_batch: 0.0003575472766706233\n",
      "          add_one_ts_to_episodes_and_truncate: 0.0033183688123130236\n",
      "          add_states_from_episodes_to_batch: 3.351839744764986e-06\n",
      "          add_time_dim_to_batch_and_zero_pad: 2.983982877940714e-05\n",
      "          agent_to_module_mapping: 0.0009207143695848927\n",
      "          batch_individual_items: 0.014317084916673602\n",
      "          general_advantage_estimation: 0.013163618505827184\n",
      "          numpy_to_tensor: 0.00010750300219854582\n",
      "    learner_connector_sum_episodes_length_in: 2000.0\n",
      "    learner_connector_sum_episodes_length_out: 2000.0\n",
      "    num_env_steps_trained: 976000\n",
      "    num_env_steps_trained_lifetime: 8784000\n",
      "    num_env_steps_trained_lifetime_throughput: 883004.6483764342\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 562176\n",
      "    num_module_steps_trained_lifetime_throughput: 56511.10811121326\n",
      "    num_module_steps_trained_throughput: 56510.01297646123\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 135686\n",
      "  shared_policy:\n",
      "    curr_entropy_coeff: 0.0\n",
      "    curr_kl_coeff: 0.45000001788139343\n",
      "    default_optimizer_learning_rate: 0.0004\n",
      "    diff_num_grad_updates_vs_sampler_policy: 1.0\n",
      "    entropy: 1.3844494819641113\n",
      "    gradients_default_optimizer_global_norm: 4.507065773010254\n",
      "    mean_kl_loss: 0.01441385131329298\n",
      "    module_train_batch_size_mean: 128.0\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 562176\n",
      "    num_module_steps_trained_lifetime_throughput: 56509.193814371894\n",
      "    num_trainable_parameters: 135686\n",
      "    policy_loss: 0.0349593386054039\n",
      "    total_loss: 4.763106346130371\n",
      "    vf_explained_var: 0.5627608895301819\n",
      "    vf_loss: 4.721660614013672\n",
      "    vf_loss_unclipped: 23.942222595214844\n",
      "    weights_seq_no: 9.0\n",
      "node_ip: 127.0.0.1\n",
      "num_env_steps_sampled_lifetime: 18000.0\n",
      "num_training_step_calls_per_iteration: 1\n",
      "perf:\n",
      "  cpu_util_percent: 20.7\n",
      "  ram_util_percent: 63.73333333333333\n",
      "pid: 39523\n",
      "time_since_restore: 15.618751525878906\n",
      "time_this_iter_s: 1.7336807250976562\n",
      "time_total_s: 15.618751525878906\n",
      "timers:\n",
      "  env_runner_sampling_timer: 0.5317206004451036\n",
      "  learner_update_timer: 1.1938109978761484\n",
      "  restore_env_runners: 4.3024897462430184e-05\n",
      "  synch_env_connectors: 0.003109536924787038\n",
      "  synch_weights: 0.0013779971728044033\n",
      "  training_iteration: 1.7279869523504319\n",
      "  training_step: 1.727692443055283\n",
      "timestamp: 1759917010\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n",
      "date: 2025-10-08_11-50-12\n",
      "done: false\n",
      "env_runner_group:\n",
      "  actor_manager_num_outstanding_async_reqs: 0\n",
      "env_runners:\n",
      "  agent_episode_returns_mean:\n",
      "    agent_0: -14.51048533583311\n",
      "    agent_1: -18.617531873965362\n",
      "    agent_2: -17.699912006828395\n",
      "    agent_3: -20.926881239056748\n",
      "    agent_4: -18.697226413244255\n",
      "    agent_5: -16.90041797129805\n",
      "    agent_6: -16.72454641540746\n",
      "    agent_7: -16.523153296043244\n",
      "    agent_8: -17.09939250165034\n",
      "    agent_9: -19.882553958159328\n",
      "  agent_steps:\n",
      "    agent_0: 25.0\n",
      "    agent_1: 25.0\n",
      "    agent_2: 25.0\n",
      "    agent_3: 25.0\n",
      "    agent_4: 25.0\n",
      "    agent_5: 25.0\n",
      "    agent_6: 25.0\n",
      "    agent_7: 25.0\n",
      "    agent_8: 25.0\n",
      "    agent_9: 25.0\n",
      "  connector_pipeline_timer: 0.0004748544997710269\n",
      "  env_reset_timer: 0.0001611045008758083\n",
      "  env_step_timer: 0.00011576899347034091\n",
      "  env_to_module_connector:\n",
      "    connector_pipeline_timer: 6.978735647851385e-05\n",
      "    timers:\n",
      "      connectors:\n",
      "        add_observations_from_episodes_to_batch: 1.4849080504981556e-05\n",
      "        add_states_from_episodes_to_batch: 2.0731631900984206e-06\n",
      "        add_time_dim_to_batch_and_zero_pad: 2.841692676237993e-06\n",
      "        agent_to_module_mapping: 2.3063367559791403e-06\n",
      "        batch_individual_items: 6.900599099384195e-06\n",
      "        numpy_to_tensor: 1.0443521055121858e-05\n",
      "  env_to_module_sum_episodes_length_in: 170.18150692537458\n",
      "  env_to_module_sum_episodes_length_out: 170.18150692537458\n",
      "  episode_duration_sec_mean: 0.12330872500002442\n",
      "  episode_len_max: 250\n",
      "  episode_len_mean: 250.0\n",
      "  episode_len_min: 250\n",
      "  episode_return_max: -93.98729315602716\n",
      "  episode_return_mean: -177.58210101148632\n",
      "  episode_return_min: -378.41681460181104\n",
      "  module_episode_returns_mean:\n",
      "    shared_policy: -19.882553958159328\n",
      "  module_to_env_connector:\n",
      "    connector_pipeline_timer: 0.0001309638955914959\n",
      "    timers:\n",
      "      connectors:\n",
      "        get_actions: 4.790183591663397e-05\n",
      "        listify_data_for_vector_env: 4.6073497174237415e-06\n",
      "        module_to_agent_unmapping: 1.9426192861882384e-06\n",
      "        normalize_and_clip_actions: 1.1014961064996316e-05\n",
      "        remove_single_ts_time_rank_from_batch: 7.600955512207228e-07\n",
      "        tensor_to_numpy: 1.971528512751449e-05\n",
      "        un_batch_to_individual_items: 8.839722140623534e-06\n",
      "  num_agent_steps_sampled:\n",
      "    agent_0: 200.0\n",
      "    agent_1: 208.0\n",
      "    agent_2: 208.0\n",
      "    agent_3: 208.0\n",
      "    agent_4: 208.0\n",
      "    agent_5: 208.0\n",
      "    agent_6: 208.0\n",
      "    agent_7: 208.0\n",
      "    agent_8: 208.0\n",
      "    agent_9: 208.0\n",
      "  num_agent_steps_sampled_lifetime:\n",
      "    agent_0: 2000.0\n",
      "    agent_1: 2080.0\n",
      "    agent_2: 2080.0\n",
      "    agent_3: 2080.0\n",
      "    agent_4: 2080.0\n",
      "    agent_5: 2080.0\n",
      "    agent_6: 2080.0\n",
      "    agent_7: 2080.0\n",
      "    agent_8: 2080.0\n",
      "    agent_9: 2080.0\n",
      "  num_env_steps_sampled: 2000.0\n",
      "  num_env_steps_sampled_lifetime: 20000.0\n",
      "  num_env_steps_sampled_lifetime_throughput: 2287.932619361862\n",
      "  num_episodes: 8.0\n",
      "  num_episodes_lifetime: 80.0\n",
      "  num_module_steps_sampled:\n",
      "    shared_policy: 2072.0\n",
      "  num_module_steps_sampled_lifetime:\n",
      "    shared_policy: 20720.0\n",
      "  rlmodule_inference_timer: 3.8324315434107925e-05\n",
      "  sample: 0.5097882402519489\n",
      "  time_between_sampling: 1.2351207707674687\n",
      "  timers:\n",
      "    connectors:\n",
      "      add_observations_from_episodes_to_batch: 4.943800013279542e-05\n",
      "      add_states_from_episodes_to_batch: 7.020999873930123e-06\n",
      "      add_time_dim_to_batch_and_zero_pad: 2.3457999304810073e-05\n",
      "      agent_to_module_mapping: 7.916999493318144e-06\n",
      "      batch_individual_items: 3.4438000511727296e-05\n",
      "      numpy_to_tensor: 6.291600038821343e-05\n",
      "  weights_seq_no: 9.0\n",
      "fault_tolerance:\n",
      "  num_healthy_workers: 2\n",
      "  num_remote_worker_restarts: 0\n",
      "hostname: scadsdp25.misc.intern.uni-leipzig.de\n",
      "iterations_since_restore: 10\n",
      "learners:\n",
      "  __all_modules__:\n",
      "    learner_connector:\n",
      "      connector_pipeline_timer: 0.04960728713063485\n",
      "      timers:\n",
      "        connectors:\n",
      "          add_columns_from_episodes_to_train_batch: 0.017325591401934178\n",
      "          add_observations_from_episodes_to_batch: 0.00035736513391130116\n",
      "          add_one_ts_to_episodes_and_truncate: 0.003318579294194506\n",
      "          add_states_from_episodes_to_batch: 3.3483213392835144e-06\n",
      "          add_time_dim_to_batch_and_zero_pad: 2.9794760491008715e-05\n",
      "          agent_to_module_mapping: 0.0009207526458766084\n",
      "          batch_individual_items: 0.014329915737509163\n",
      "          general_advantage_estimation: 0.013066585660759921\n",
      "          numpy_to_tensor: 0.00010758672217775595\n",
      "    learner_connector_sum_episodes_length_in: 2000.0\n",
      "    learner_connector_sum_episodes_length_out: 2000.0\n",
      "    num_env_steps_trained: 976000\n",
      "    num_env_steps_trained_lifetime: 9760000\n",
      "    num_env_steps_trained_lifetime_throughput: 881699.3565331\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 624640\n",
      "    num_module_steps_trained_lifetime_throughput: 56427.77618045542\n",
      "    num_module_steps_trained_throughput: 56426.72845774345\n",
      "    num_non_trainable_parameters: 0\n",
      "    num_trainable_parameters: 135686\n",
      "  shared_policy:\n",
      "    curr_entropy_coeff: 0.0\n",
      "    curr_kl_coeff: 0.45000001788139343\n",
      "    default_optimizer_learning_rate: 0.0004\n",
      "    diff_num_grad_updates_vs_sampler_policy: 1.0\n",
      "    entropy: 1.3892606496810913\n",
      "    gradients_default_optimizer_global_norm: 2.1072659492492676\n",
      "    mean_kl_loss: 0.01617416739463806\n",
      "    module_train_batch_size_mean: 128.0\n",
      "    num_module_steps_trained: 62464\n",
      "    num_module_steps_trained_lifetime: 624640\n",
      "    num_module_steps_trained_lifetime_throughput: 56425.889022975665\n",
      "    num_trainable_parameters: 135686\n",
      "    policy_loss: -0.09808310866355896\n",
      "    total_loss: 4.2677507400512695\n",
      "    vf_explained_var: 0.27217161655426025\n",
      "    vf_loss: 4.358555316925049\n",
      "    vf_loss_unclipped: 10.942136764526367\n",
      "    weights_seq_no: 10.0\n",
      "node_ip: 127.0.0.1\n",
      "num_env_steps_sampled_lifetime: 20000.0\n",
      "num_training_step_calls_per_iteration: 1\n",
      "perf:\n",
      "  cpu_util_percent: 20.4\n",
      "  ram_util_percent: 63.45\n",
      "pid: 39523\n",
      "time_since_restore: 17.363606452941895\n",
      "time_this_iter_s: 1.7448549270629883\n",
      "time_total_s: 17.363606452941895\n",
      "timers:\n",
      "  env_runner_sampling_timer: 0.5316941490206534\n",
      "  learner_update_timer: 1.1939602224874015\n",
      "  restore_env_runners: 4.267214847766261e-05\n",
      "  synch_env_connectors: 0.0031060886355445765\n",
      "  synch_weights: 0.0013790063710762176\n",
      "  training_iteration: 1.7281124494869142\n",
      "  training_step: 1.727817963624737\n",
      "timestamp: 1759917012\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/Users/paula/part-sim/ppo_model_checkpoint), metrics={'timers': {'training_iteration': 1.7281124494869142, 'restore_env_runners': 4.267214847766261e-05, 'training_step': 1.727817963624737, 'env_runner_sampling_timer': 0.5316941490206534, 'learner_update_timer': 1.1939602224874015, 'synch_weights': 0.0013790063710762176, 'synch_env_connectors': 0.0031060886355445765}, 'env_runners': {'num_agent_steps_sampled': {'agent_9': 208.0, 'agent_6': 208.0, 'agent_3': 208.0, 'agent_4': 208.0, 'agent_2': 208.0, 'agent_5': 208.0, 'agent_0': 200.0, 'agent_7': 208.0, 'agent_1': 208.0, 'agent_8': 208.0}, 'num_episodes_lifetime': 80.0, 'num_episodes': 8.0, 'env_to_module_connector': {'timers': {'connectors': {'add_time_dim_to_batch_and_zero_pad': 2.841692676237993e-06, 'numpy_to_tensor': 1.0443521055121858e-05, 'add_states_from_episodes_to_batch': 2.0731631900984206e-06, 'add_observations_from_episodes_to_batch': 1.4849080504981556e-05, 'agent_to_module_mapping': 2.3063367559791403e-06, 'batch_individual_items': 6.900599099384195e-06}}, 'connector_pipeline_timer': 6.978735647851385e-05}, 'module_episode_returns_mean': {'shared_policy': -19.882553958159328}, 'timers': {'connectors': {'batch_individual_items': 3.4438000511727296e-05, 'numpy_to_tensor': 6.291600038821343e-05, 'add_time_dim_to_batch_and_zero_pad': 2.3457999304810073e-05, 'add_states_from_episodes_to_batch': 7.020999873930123e-06, 'add_observations_from_episodes_to_batch': 4.943800013279542e-05, 'agent_to_module_mapping': 7.916999493318144e-06}}, 'env_to_module_sum_episodes_length_in': 170.18150692537458, 'num_agent_steps_sampled_lifetime': {'agent_7': 2080.0, 'agent_1': 2080.0, 'agent_3': 2080.0, 'agent_8': 2080.0, 'agent_9': 2080.0, 'agent_6': 2080.0, 'agent_2': 2080.0, 'agent_4': 2080.0, 'agent_5': 2080.0, 'agent_0': 2000.0}, 'connector_pipeline_timer': 0.0004748544997710269, 'agent_episode_returns_mean': {'agent_7': -16.523153296043244, 'agent_6': -16.72454641540746, 'agent_1': -18.617531873965362, 'agent_3': -20.926881239056748, 'agent_8': -17.09939250165034, 'agent_9': -19.882553958159328, 'agent_2': -17.699912006828395, 'agent_5': -16.90041797129805, 'agent_0': -14.51048533583311, 'agent_4': -18.697226413244255}, 'module_to_env_connector': {'timers': {'connectors': {'normalize_and_clip_actions': 1.1014961064996316e-05, 'tensor_to_numpy': 1.971528512751449e-05, 'get_actions': 4.790183591663397e-05, 'un_batch_to_individual_items': 8.839722140623534e-06, 'remove_single_ts_time_rank_from_batch': 7.600955512207228e-07, 'listify_data_for_vector_env': 4.6073497174237415e-06, 'module_to_agent_unmapping': 1.9426192861882384e-06}}, 'connector_pipeline_timer': 0.0001309638955914959}, 'agent_steps': {'agent_2': 25.0, 'agent_5': 25.0, 'agent_0': 25.0, 'agent_4': 25.0, 'agent_7': 25.0, 'agent_6': 25.0, 'agent_1': 25.0, 'agent_3': 25.0, 'agent_8': 25.0, 'agent_9': 25.0}, 'episode_return_mean': -177.58210101148632, 'env_reset_timer': 0.0001611045008758083, 'weights_seq_no': 9.0, 'num_module_steps_sampled': {'shared_policy': 2072.0}, 'episode_return_min': -378.41681460181104, 'sample': 0.5097882402519489, 'env_to_module_sum_episodes_length_out': 170.18150692537458, 'episode_len_max': 250, 'rlmodule_inference_timer': 3.8324315434107925e-05, 'num_module_steps_sampled_lifetime': {'shared_policy': 20720.0}, 'episode_duration_sec_mean': 0.12330872500002442, 'num_env_steps_sampled_lifetime': 20000.0, 'num_env_steps_sampled': 2000.0, 'episode_return_max': -93.98729315602716, 'episode_len_mean': 250.0, 'episode_len_min': 250, 'env_step_timer': 0.00011576899347034091, 'time_between_sampling': 1.2351207707674687, 'num_env_steps_sampled_lifetime_throughput': 2287.932619361862}, 'learners': {'shared_policy': {'policy_loss': -0.09808311, 'module_train_batch_size_mean': 128.0, 'entropy': 1.3892606, 'num_trainable_parameters': 135686, 'curr_entropy_coeff': 0.0, 'default_optimizer_learning_rate': 0.0004, 'num_module_steps_trained': 62464, 'total_loss': 4.2677507, 'curr_kl_coeff': 0.45000001788139343, 'mean_kl_loss': 0.016174167, 'vf_explained_var': 0.27217162, 'vf_loss_unclipped': 10.942137, 'vf_loss': 4.3585553, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'gradients_default_optimizer_global_norm': 2.107266, 'weights_seq_no': 10.0, 'num_module_steps_trained_lifetime': 624640, 'num_module_steps_trained_lifetime_throughput': 56425.889022975665}, '__all_modules__': {'num_trainable_parameters': 135686, 'num_env_steps_trained': 976000, 'num_env_steps_trained_lifetime': 9760000, 'learner_connector': {'timers': {'connectors': {'add_time_dim_to_batch_and_zero_pad': 2.9794760491008715e-05, 'general_advantage_estimation': 0.013066585660759921, 'add_states_from_episodes_to_batch': 3.3483213392835144e-06, 'add_observations_from_episodes_to_batch': 0.00035736513391130116, 'agent_to_module_mapping': 0.0009207526458766084, 'add_one_ts_to_episodes_and_truncate': 0.003318579294194506, 'batch_individual_items': 0.014329915737509163, 'numpy_to_tensor': 0.00010758672217775595, 'add_columns_from_episodes_to_train_batch': 0.017325591401934178}}, 'connector_pipeline_timer': 0.04960728713063485}, 'num_module_steps_trained': 62464, 'learner_connector_sum_episodes_length_out': 2000.0, 'learner_connector_sum_episodes_length_in': 2000.0, 'num_module_steps_trained_lifetime': 624640, 'num_non_trainable_parameters': 0, 'num_env_steps_trained_lifetime_throughput': 881699.3565331, 'num_module_steps_trained_throughput': 56426.72845774345, 'num_module_steps_trained_lifetime_throughput': 56427.77618045542}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 20000.0, 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'done': False, 'training_iteration': 10, 'trial_id': 'default', 'date': '2025-10-08_11-50-12', 'timestamp': 1759917012, 'time_this_iter_s': 1.7448549270629883, 'time_total_s': 17.363606452941895, 'pid': 39523, 'hostname': 'scadsdp25.misc.intern.uni-leipzig.de', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'particle_v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 0.0004, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': 2000, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function <lambda> at 0x33547efc0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'shared_policy': (None, Box(-inf, inf, (4,), float32), Discrete(5), {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 17.363606452941895, 'iterations_since_restore': 10, 'perf': {'cpu_util_percent': 20.4, 'ram_util_percent': 63.45}})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))\n",
    "#algo.stop()\n",
    "algo.save(\"/Users/paula/part-sim/ppo_model_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7be4319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ray.rllib.core import (\n",
    "    COMPONENT_ENV_RUNNER,\n",
    "    COMPONENT_ENV_TO_MODULE_CONNECTOR,\n",
    "    COMPONENT_MODULE_TO_ENV_CONNECTOR,\n",
    "    COMPONENT_LEARNER_GROUP,\n",
    "    COMPONENT_LEARNER,\n",
    "    COMPONENT_RL_MODULE,\n",
    "    DEFAULT_MODULE_ID, # this is \"default_policy\" by default but in multi-agent it is \"shared_policy\"\n",
    ")\n",
    "from ray.rllib.core.rl_module.rl_module import RLModule\n",
    "\n",
    "\n",
    "# Create RLModule from a checkpoint.\n",
    "rl_module = RLModule.from_checkpoint(\n",
    "    os.path.join(\n",
    "        \"/Users/paula/part-sim/ppo_model_checkpoint\", # checkpoint.path (or directly a string path to the checkpoint dir)\n",
    "        COMPONENT_LEARNER_GROUP,\n",
    "        COMPONENT_LEARNER,\n",
    "        COMPONENT_RL_MODULE,\n",
    "        \"shared_policy\", # DEFAULT_MODULE_ID,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "784c75c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'DefaultPPOTorchRLModule' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m num_episodes < max_episodes:\n\u001b[32m     49\u001b[39m     shared_data = {}\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     input_dict = \u001b[43menv_to_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrl_module\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrl_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable exploration for inference\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshared_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshared_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Compute actions using the RLModule\u001b[39;00m\n\u001b[32m     58\u001b[39m     rl_module_out = rl_module.forward_inference(input_dict)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/connectors/env_to_module/env_to_module_pipeline.py:38\u001b[39m, in \u001b[36mEnvToModulePipeline.__call__\u001b[39m\u001b[34m(self, rl_module, batch, episodes, explore, shared_data, metrics, **kwargs)\u001b[39m\n\u001b[32m     31\u001b[39m     metrics.log_value(\n\u001b[32m     32\u001b[39m         ENV_TO_MODULE_SUM_EPISODES_LENGTH_IN,\n\u001b[32m     33\u001b[39m         \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlen\u001b[39m, episodes)),\n\u001b[32m     34\u001b[39m     )\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Make sure user does not necessarily send initial input into this pipeline.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Might just be empty and to be populated from `episodes`.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m ret = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrl_module\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrl_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshared_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshared_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshared_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Log the sum of lengths of all episodes outgoing.\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/connectors/connector_pipeline_v2.py:123\u001b[39m, in \u001b[36mConnectorPipelineV2.__call__\u001b[39m\u001b[34m(self, rl_module, batch, episodes, explore, shared_data, metrics, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m     stats = metrics.log_time(\n\u001b[32m    114\u001b[39m         kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mmetrics_prefix_key\u001b[39m\u001b[33m\"\u001b[39m, ())\n\u001b[32m    115\u001b[39m         + (\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m         )\n\u001b[32m    120\u001b[39m     )\n\u001b[32m    121\u001b[39m     stats.\u001b[34m__enter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m batch = \u001b[43mconnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrl_module\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrl_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshared_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshared_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Deprecated arg.\u001b[39;49;00m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metrics:\n\u001b[32m    136\u001b[39m     stats.\u001b[34m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/connectors/common/agent_to_module_mapping.py:180\u001b[39m, in \u001b[36mAgentToModuleMapping.__call__\u001b[39m\u001b[34m(self, rl_module, batch, episodes, explore, shared_data, **kwargs)\u001b[39m\n\u001b[32m    178\u001b[39m memorized_map_structure = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m column, agent_data \u001b[38;5;129;01min\u001b[39;00m batch.items():\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m rl_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mcolumn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrl_module\u001b[49m:\n\u001b[32m    181\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m eps_id, agent_id, module_id \u001b[38;5;129;01min\u001b[39;00m agent_data.keys():\n",
      "\u001b[31mTypeError\u001b[39m: argument of type 'DefaultPPOTorchRLModule' is not iterable"
     ]
    }
   ],
   "source": [
    "# from chat\n",
    "from ray.rllib.connectors.env_to_module import EnvToModulePipeline\n",
    "from ray.rllib.connectors.module_to_env import ModuleToEnvPipeline\n",
    "from ray.rllib.core.rl_module.rl_module import RLModule\n",
    "from ray.rllib.env.single_agent_episode import SingleAgentEpisode\n",
    "from ray.rllib.core.columns import Columns\n",
    "import os\n",
    "\n",
    "# Restore components from the checkpoint\n",
    "checkpoint_path = \"/Users/paula/part-sim/ppo_model_checkpoint\"\n",
    "\n",
    "env_to_module = EnvToModulePipeline.from_checkpoint(\n",
    "    os.path.join(checkpoint_path, \"env_runner\", \"env_to_module_connector\")\n",
    ")\n",
    "\n",
    "rl_module = RLModule.from_checkpoint(\n",
    "    os.path.join(checkpoint_path, \"learner_group\", \"learner\", \"rl_module\", \"shared_policy\")\n",
    ")\n",
    "\n",
    "module_to_env = ModuleToEnvPipeline.from_checkpoint(\n",
    "    os.path.join(checkpoint_path, \"env_runner\", \"module_to_env_connector\")\n",
    ")\n",
    "\n",
    "# Set up the environment\n",
    "from mpe2 import simple_v3\n",
    "env = simple_v3.env(max_cycles=100, render_mode='human')\n",
    "\n",
    "# Reset the environment and initialize the episode\n",
    "# Temporary env instance just to grab one agent's space\n",
    "tmp_env = PettingZooEnv(env)\n",
    "first_agent = list(tmp_env.observation_space.keys())[0]\n",
    "obs_space = tmp_env.observation_space[first_agent]\n",
    "act_space = tmp_env.action_space[first_agent]\n",
    "observations1, _ = tmp_env.reset()\n",
    "obs = observations1[first_agent] # returns global state, which is the same as the observations for the single agent, here\n",
    "tmp_env.close()\n",
    "\n",
    "episode = SingleAgentEpisode(\n",
    "    observations=[obs],\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    ")\n",
    "\n",
    "# Inference loop\n",
    "num_episodes = 0\n",
    "max_episodes = 10\n",
    "\n",
    "while num_episodes < max_episodes:\n",
    "    shared_data = {}\n",
    "    input_dict = env_to_module(\n",
    "        episodes=[episode],\n",
    "        rl_module=rl_module,\n",
    "        explore=False,  # Disable exploration for inference\n",
    "        shared_data=shared_data,\n",
    "    )\n",
    "\n",
    "    # Compute actions using the RLModule\n",
    "    rl_module_out = rl_module.forward_inference(input_dict)\n",
    "\n",
    "    # Process the RLModule output and send actions to the environment\n",
    "    to_env = module_to_env(\n",
    "        batch=rl_module_out,\n",
    "        episodes=[episode],\n",
    "        rl_module=rl_module,\n",
    "        explore=False,\n",
    "        shared_data=shared_data,\n",
    "    )\n",
    "    action = to_env.pop(Columns.ACTIONS)[0]\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    # Update the episode\n",
    "    episode.add_env_step(\n",
    "        obs,\n",
    "        action,\n",
    "        reward,\n",
    "        terminated=terminated,\n",
    "        truncated=truncated,\n",
    "        extra_model_outputs={k: v[0] for k, v in to_env.items()},\n",
    "    )\n",
    "\n",
    "    # Check if the episode is done\n",
    "    if episode.is_done:\n",
    "        print(f\"Episode done: Total reward = {episode.get_return()}\")\n",
    "        obs, _ = env.reset()\n",
    "        episode = SingleAgentEpisode(\n",
    "            observations=[obs],\n",
    "            observation_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "        )\n",
    "        num_episodes += 1\n",
    "\n",
    "env.close()\n",
    "print(f\"Completed {num_episodes} episodes of inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50304e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-10-07 13:10:53,976\tERROR actor_manager.py:873 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=98435, ip=127.0.0.1, actor_id=756de42b586bcd9f279ba5a901000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x141a55b20>)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/default_ppo_rl_module.py\", line 31, in setup\n",
      "    self.catalog.actor_critic_encoder_config.base_encoder_config,\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'actor_critic_encoder_config'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=98435, ip=127.0.0.1, actor_id=756de42b586bcd9f279ba5a901000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x141a55b20>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 130, in __init__\n",
      "    self.make_module()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 888, in make_module\n",
      "    self.module = module_spec.build()\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 606, in build\n",
      "    module = self.multi_rl_module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 126, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 470, in __init__\n",
      "    self.setup()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 144, in setup\n",
      "    self._rl_modules[module_id] = rl_module_spec.build()\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 98, in build\n",
      "    module = self.module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py\", line 24, in __init__\n",
      "    super().__init__(*args, **kwargs, catalog_class=catalog_class)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py\", line 50, in __init__\n",
      "    RLModule.__init__(self, *args, **kwargs)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 473, in __init__\n",
      "    raise (self._catalog_ctor_error or e)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 426, in __init__\n",
      "    self.catalog = catalog_class(\n",
      "                   ^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py\", line 74, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 122, in __init__\n",
      "    self._determine_components_hook()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 142, in _determine_components_hook\n",
      "    self._encoder_config = self._get_encoder_config(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 361, in _get_encoder_config\n",
      "    raise ValueError(\n",
      "ValueError: No default encoder config for obs space=<bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x147c58f80>>, lstm=False found.), taking actor 1 out of service.\n",
      "2025-10-07 13:10:53,976\tERROR actor_manager.py:873 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=98434, ip=127.0.0.1, actor_id=ab5f847f51fc0453296ecd9b01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x164951df0>)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/default_ppo_rl_module.py\", line 31, in setup\n",
      "    self.catalog.actor_critic_encoder_config.base_encoder_config,\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'actor_critic_encoder_config'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=98434, ip=127.0.0.1, actor_id=ab5f847f51fc0453296ecd9b01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x164951df0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 130, in __init__\n",
      "    self.make_module()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 888, in make_module\n",
      "    self.module = module_spec.build()\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 606, in build\n",
      "    module = self.multi_rl_module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 126, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 470, in __init__\n",
      "    self.setup()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 144, in setup\n",
      "    self._rl_modules[module_id] = rl_module_spec.build()\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 98, in build\n",
      "    module = self.module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py\", line 24, in __init__\n",
      "    super().__init__(*args, **kwargs, catalog_class=catalog_class)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py\", line 50, in __init__\n",
      "    RLModule.__init__(self, *args, **kwargs)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 473, in __init__\n",
      "    raise (self._catalog_ctor_error or e)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 426, in __init__\n",
      "    self.catalog = catalog_class(\n",
      "                   ^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py\", line 74, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 122, in __init__\n",
      "    self._determine_components_hook()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 142, in _determine_components_hook\n",
      "    self._encoder_config = self._get_encoder_config(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 361, in _get_encoder_config\n",
      "    raise ValueError(\n",
      "ValueError: No default encoder config for obs space=<bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x16977e420>>, lstm=False found.), taking actor 2 out of service.\n",
      "2025-10-07 13:10:53,977\tERROR env_runner_group.py:758 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=98435, ip=127.0.0.1, actor_id=756de42b586bcd9f279ba5a901000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x141a55b20>)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/default_ppo_rl_module.py\", line 31, in setup\n",
      "    self.catalog.actor_critic_encoder_config.base_encoder_config,\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'actor_critic_encoder_config'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=98435, ip=127.0.0.1, actor_id=756de42b586bcd9f279ba5a901000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x141a55b20>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 130, in __init__\n",
      "    self.make_module()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 888, in make_module\n",
      "    self.module = module_spec.build()\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 606, in build\n",
      "    module = self.multi_rl_module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 126, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 470, in __init__\n",
      "    self.setup()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 144, in setup\n",
      "    self._rl_modules[module_id] = rl_module_spec.build()\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 98, in build\n",
      "    module = self.module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py\", line 24, in __init__\n",
      "    super().__init__(*args, **kwargs, catalog_class=catalog_class)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py\", line 50, in __init__\n",
      "    RLModule.__init__(self, *args, **kwargs)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 473, in __init__\n",
      "    raise (self._catalog_ctor_error or e)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 426, in __init__\n",
      "    self.catalog = catalog_class(\n",
      "                   ^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py\", line 74, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 122, in __init__\n",
      "    self._determine_components_hook()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 142, in _determine_components_hook\n",
      "    self._encoder_config = self._get_encoder_config(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 361, in _get_encoder_config\n",
      "    raise ValueError(\n",
      "ValueError: No default encoder config for obs space=<bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x147c58f80>>, lstm=False found.\n",
      "2025-10-07 13:10:53,977\tERROR env_runner_group.py:758 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=98434, ip=127.0.0.1, actor_id=ab5f847f51fc0453296ecd9b01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x164951df0>)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/default_ppo_rl_module.py\", line 31, in setup\n",
      "    self.catalog.actor_critic_encoder_config.base_encoder_config,\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'actor_critic_encoder_config'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=98434, ip=127.0.0.1, actor_id=ab5f847f51fc0453296ecd9b01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x164951df0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 130, in __init__\n",
      "    self.make_module()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 888, in make_module\n",
      "    self.module = module_spec.build()\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 606, in build\n",
      "    module = self.multi_rl_module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 126, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 470, in __init__\n",
      "    self.setup()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 144, in setup\n",
      "    self._rl_modules[module_id] = rl_module_spec.build()\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 98, in build\n",
      "    module = self.module_class(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py\", line 24, in __init__\n",
      "    super().__init__(*args, **kwargs, catalog_class=catalog_class)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py\", line 50, in __init__\n",
      "    RLModule.__init__(self, *args, **kwargs)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 473, in __init__\n",
      "    raise (self._catalog_ctor_error or e)\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 426, in __init__\n",
      "    self.catalog = catalog_class(\n",
      "                   ^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py\", line 74, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 122, in __init__\n",
      "    self._determine_components_hook()\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 142, in _determine_components_hook\n",
      "    self._encoder_config = self._get_encoder_config(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 361, in _get_encoder_config\n",
      "    raise ValueError(\n",
      "ValueError: No default encoder config for obs space=<bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x16977e420>>, lstm=False found.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     15\u001b[39m config = (\n\u001b[32m     16\u001b[39m     PPOConfig()\n\u001b[32m     17\u001b[39m     .environment(\u001b[33m\"\u001b[39m\u001b[33msimple_v3\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     )\n\u001b[32m     26\u001b[39m )\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Build the Algorithm.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m algo = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Train for one iteration, which is 2000 timesteps (1 train batch).\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(algo.train())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm_config.py:1001\u001b[39m, in \u001b[36mAlgorithmConfig.build_algo\u001b[39m\u001b[34m(self, env, logger_creator, use_copy)\u001b[39m\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.algo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    999\u001b[39m     algo_class = get_trainable_cls(\u001b[38;5;28mself\u001b[39m.algo_class)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:536\u001b[39m, in \u001b[36mAlgorithm.__init__\u001b[39m\u001b[34m(self, config, env, logger_creator, **kwargs)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# Evaluation EnvRunnerGroup and metrics last returned by `self.evaluate()`.\u001b[39;00m\n\u001b[32m    534\u001b[39m \u001b[38;5;28mself\u001b[39m.eval_env_runner_group: Optional[EnvRunnerGroup] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/trainable/trainable.py:158\u001b[39m, in \u001b[36mTrainable.__init__\u001b[39m\u001b[34m(self, config, logger_creator, storage)\u001b[39m\n\u001b[32m    154\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._open_logfiles(stdout_file, stderr_file)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m setup_time = time.time() - \u001b[38;5;28mself\u001b[39m._start_time\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m setup_time > SETUP_TIME_THRESHOLD:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:644\u001b[39m, in \u001b[36mAlgorithm.setup\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    640\u001b[39m     \u001b[38;5;28mself\u001b[39m.offline_data = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_online \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.enable_env_runner_and_connector_v2:\n\u001b[32m    643\u001b[39m     \u001b[38;5;66;03m# Create a set of env runner actors via a EnvRunnerGroup.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     \u001b[38;5;28mself\u001b[39m.env_runner_group = \u001b[43mEnvRunnerGroup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# New API stack: User decides whether to create local env runner.\u001b[39;49;00m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Old API stack: Always create local EnvRunner.\u001b[39;49;00m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    653\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43menable_env_runner_and_connector_v2\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_local_env_runner\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtune_trial_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;66;03m# Compile, validate, and freeze an evaluation config.\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluation_config = \u001b[38;5;28mself\u001b[39m.config.get_evaluation_config_object()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py:198\u001b[39m, in \u001b[36mEnvRunnerGroup.__init__\u001b[39m\u001b[34m(self, env_creator, validate_env, default_policy_class, config, local_env_runner, logdir, _setup, tune_trial_id, pg_offset, num_env_runners, num_workers, local_worker)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _setup:\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m                \u001b[49m\u001b[43mnum_env_runners\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    204\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_env_runners\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     \u001b[38;5;66;03m# EnvRunnerGroup creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[32m    209\u001b[39m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[32m    210\u001b[39m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    212\u001b[39m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[32m    213\u001b[39m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[32m    214\u001b[39m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py:286\u001b[39m, in \u001b[36mEnvRunnerGroup._setup\u001b[39m\u001b[34m(self, validate_env, config, num_env_runners, local_env_runner)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;66;03m# If num_env_runners > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    281\u001b[39m     local_env_runner\n\u001b[32m    282\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._worker_manager.num_actors() > \u001b[32m0\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.create_env_on_local_worker\n\u001b[32m    284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config.observation_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.action_space)\n\u001b[32m    285\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     spaces = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    288\u001b[39m     spaces = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py:314\u001b[39m, in \u001b[36mEnvRunnerGroup.get_spaces\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# Get ID of the first remote worker.\u001b[39;00m\n\u001b[32m    308\u001b[39m remote_worker_ids = (\n\u001b[32m    309\u001b[39m     [\u001b[38;5;28mself\u001b[39m._worker_manager.actor_ids()[\u001b[32m0\u001b[39m]]\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._worker_manager.actor_ids()\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    312\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m spaces = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforeach_env_runner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv_runner\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    320\u001b[39m logger.info(\n\u001b[32m    321\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mInferred observation/action spaces from remote \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mworker (local worker has no env): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspaces\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m )\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m spaces\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from mpe2 import simple_v3\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "from supersuit import flatten_v0\n",
    "\n",
    "def make_env(env_config=None):\n",
    "    env = simple_v3.parallel_env(max_cycles=25, render_mode=None)  # parallel API is best for RLlib\n",
    "    return flatten_v0(env)\n",
    "\n",
    "# Register custom environment with Rllib\n",
    "register_env(\"simple_v3\", lambda config: PettingZooEnv(make_env(config))) # we also have to use the Pettingzoo wrapper\n",
    "\n",
    "# Configure.\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"simple_v3\")\n",
    "    .training(\n",
    "        train_batch_size_per_learner=2000, # num of steps per agent\n",
    "        lr=0.0004,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build the Algorithm.\n",
    "algo = config.build_algo()\n",
    "\n",
    "# Train for one iteration, which is 2000 timesteps (1 train batch).\n",
    "print(algo.train())\n",
    "\n",
    "# Saves to a folder \"ppo_model_checkpoint\"\n",
    "algo.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391279d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last observation: [ 0.          0.         -0.953831   -0.34676746]\n",
      "Last reward: 0.0\n",
      "Last termination: False\n",
      "Last truncation: False\n",
      "Last info: {}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiAgentEnvRunner' object has no attribute 'get_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m         action = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     23\u001b[39m         \u001b[38;5;66;03m#action = env.action_space(agent).sample() \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         action = \u001b[43malgo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_single_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_observation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdefault_policy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_fetch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     env.step(action)\n\u001b[32m     26\u001b[39m env.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/deprecation.py:128\u001b[39m, in \u001b[36mDeprecated.<locals>._inner.<locals>._ctor\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m     deprecation_warning(\n\u001b[32m    122\u001b[39m         old=old \u001b[38;5;129;01mor\u001b[39;00m obj.\u001b[34m__name__\u001b[39m,\n\u001b[32m    123\u001b[39m         new=new,\n\u001b[32m    124\u001b[39m         help=help,\n\u001b[32m    125\u001b[39m         error=error,\n\u001b[32m    126\u001b[39m     )\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Call the deprecated method/function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:4271\u001b[39m, in \u001b[36mAlgorithm.compute_single_action\u001b[39m\u001b[34m(self, observation, state, prev_action, prev_reward, info, input_dict, policy_id, full_fetch, explore, timestep, episode, unsquash_action, clip_action)\u001b[39m\n\u001b[32m   4268\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4269\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m observation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, err_msg\n\u001b[32m-> \u001b[39m\u001b[32m4271\u001b[39m policy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4272\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4273\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   4274\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPolicyID \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found in PolicyMap of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4275\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAlgorithm\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms local worker!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4276\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:2398\u001b[39m, in \u001b[36mAlgorithm.get_policy\u001b[39m\u001b[34m(self, policy_id)\u001b[39m\n\u001b[32m   2391\u001b[39m \u001b[38;5;129m@OldAPIStack\u001b[39m\n\u001b[32m   2392\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_policy\u001b[39m(\u001b[38;5;28mself\u001b[39m, policy_id: PolicyID = DEFAULT_POLICY_ID) -> Policy:\n\u001b[32m   2393\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return policy for the specified id, or None.\u001b[39;00m\n\u001b[32m   2394\u001b[39m \n\u001b[32m   2395\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   2396\u001b[39m \u001b[33;03m        policy_id: ID of the policy to return.\u001b[39;00m\n\u001b[32m   2397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_policy\u001b[49m(policy_id)\n",
      "\u001b[31mAttributeError\u001b[39m: 'MultiAgentEnvRunner' object has no attribute 'get_policy'"
     ]
    }
   ],
   "source": [
    "# Now, within let's do some inference in simple_v3 and visualize\n",
    "env = simple_v3.parallel_env(max_cycles=100, render_mode='human')\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "import os\n",
    "\n",
    "# This is our trained policy.\n",
    "trained_policy = algo.get_module(\"default_policy\")\n",
    "#checkpoint_path = os.path.expanduser(\"\")\n",
    "#PPOagent = PPO.from_checkpoint(checkpoint_path)\n",
    "\n",
    "env.reset() # Do seed=42 for reproducibility\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    last_observation, last_reward, last_termination, last_truncation, last_info = env.last()\n",
    "    print(f\"Last observation: {last_observation}\")\n",
    "    print(f\"Last reward: {last_reward}\")\n",
    "    print(f\"Last termination: {last_termination}\")\n",
    "    print(f\"Last truncation: {last_truncation}\")\n",
    "    print(f\"Last info: {last_info}\")\n",
    "    if last_termination or last_truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        #action = env.action_space(agent).sample() \n",
    "        action = algo.compute_single_action(observation = last_observation, policy_id=\"default_policy\", full_fetch=True)\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d84fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ray.rllib.algorithms.ppo.torch.default_ppo_torch_rl_module.DefaultPPOTorchRLModule"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.compute_single_action(observation = last_observation, prev_reward = last_reward, info = last_info, episodes = , explore = , timestep = , is_training = , policy_id = , clip_actions = True, full_fetch = False, explore_kwargs = None, timestep_kwargs = None, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562f6f3",
   "metadata": {},
   "source": [
    "# Trying our own multi-agent environment particle_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6be988",
   "metadata": {},
   "source": [
    "Now, let's try with our own environment particle_v1 that is within this project. We have multiple agents and can also define more food sources (landmarks), which are marked in orange. First, let's use a  random policy again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67604150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import particle_v1\n",
    "env = particle_v1.env(num_agents=10, num_food_sources=1, flow = 'none', max_cycles=25, render_mode='human')\n",
    "\n",
    "env.reset() # Do seed=42 for reproducibility\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        action = env.action_space(agent).sample() # this is where you would insert your policy\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883853b",
   "metadata": {},
   "source": [
    "Now, let's try with PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d30e675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "2025-10-06 15:36:08,403\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-10-06 15:36:11,182\tINFO worker.py:1951 -- Started a local Ray instance.\n",
      "2025-10-06 15:36:11,182\tINFO worker.py:1951 -- Started a local Ray instance.\n",
      "[2025-10-06 15:36:12,687 E 29470 986908] core_worker.cc:2246: Actor with class name: 'RolloutWorker' and ID: 'feb32aed41911b97fa2b5fdd01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "[2025-10-06 15:36:12,687 E 29470 986908] core_worker.cc:2246: Actor with class name: 'RolloutWorker' and ID: 'feb32aed41911b97fa2b5fdd01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(RolloutWorker pid=29485)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(RolloutWorker pid=29485)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(RolloutWorker pid=29485)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(RolloutWorker pid=29485)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "2025-10-06 15:36:15,523\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "2025-10-06 15:36:15,523\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m 2025-10-06 15:36:15,525\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: 'aec_to_parallel_wrapper' object has no attribute 'agent_selection'\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 108, in <lambda>\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     (lambda w: w.sample(**random_action_kwargs))\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 677, in sample\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/evaluation/sampler.py\", line 59, in next\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m                ^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/evaluation/sampler.py\", line 225, in get_data\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 329, in run\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     outputs = self.step()\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m               ^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 348, in step\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     ) = self._base_env.poll()\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env.py\", line 530, in poll\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     ) = env_state.poll()\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m         ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env.py\", line 717, in poll\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     self.reset()\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env.py\", line 801, in reset\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     raise e\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env.py\", line 795, in reset\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     obs_and_infos = self.env.reset(seed=seed, options=options)\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py\", line 147, in reset\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     {self.env.agent_selection: self.env.observe(self.env.agent_selection)},\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m AttributeError: 'aec_to_parallel_wrapper' object has no attribute 'agent_selection'\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m 2025-10-06 15:36:15,525\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: 'aec_to_parallel_wrapper' object has no attribute 'agent_selection'\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 108, in <lambda>\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     (lambda w: w.sample(**random_action_kwargs))\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 677, in sample\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/evaluation/sampler.py\", line 59, in next\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m                ^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/evaluation/sampler.py\", line 225, in get_data\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 329, in run\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     outputs = self.step()\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m               ^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 348, in step\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     ) = self._base_env.poll()\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env.py\", line 530, in poll\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     ) = env_state.poll()\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m         ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env.py\", line 717, in poll\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     self.reset()\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env.py\", line 801, in reset\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     raise e\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env.py\", line 795, in reset\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     obs_and_infos = self.env.reset(seed=seed, options=options)\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py\", line 147, in reset\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m     {self.env.agent_selection: self.env.observe(self.env.agent_selection)},\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RolloutWorker pid=29486)\u001b[0m AttributeError: 'aec_to_parallel_wrapper' object has no attribute 'agent_selection'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m algo = config.build()\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     result = \u001b[43malgo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/tune/trainable/trainable.py:328\u001b[39m, in \u001b[36mTrainable.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    326\u001b[39m start = time.time()\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    330\u001b[39m     skipped = skip_exceptions(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:1040\u001b[39m, in \u001b[36mAlgorithm.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1035\u001b[39m         train_results, train_iter_ctx = \u001b[38;5;28mself\u001b[39m._run_one_training_iteration()\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1037\u001b[39m         (\n\u001b[32m   1038\u001b[39m             train_results,\n\u001b[32m   1039\u001b[39m             train_iter_ctx,\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m         ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_one_training_iteration_old_api_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.evaluation_parallel_to_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:4098\u001b[39m, in \u001b[36mAlgorithm._run_one_training_iteration_old_api_stack\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   4091\u001b[39m         \u001b[38;5;28mself\u001b[39m._make_on_env_runners_recreated_callbacks(\n\u001b[32m   4092\u001b[39m             config=\u001b[38;5;28mself\u001b[39m.config,\n\u001b[32m   4093\u001b[39m             env_runner_group=\u001b[38;5;28mself\u001b[39m.env_runner_group,\n\u001b[32m   4094\u001b[39m             restored_env_runner_indices=restored,\n\u001b[32m   4095\u001b[39m         )\n\u001b[32m   4097\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._timers[TRAINING_STEP_TIMER]:\n\u001b[32m-> \u001b[39m\u001b[32m4098\u001b[39m     training_step_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_step_results:\n\u001b[32m   4101\u001b[39m     results = training_step_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo.py:391\u001b[39m, in \u001b[36mPPO.training_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;129m@override\u001b[39m(Algorithm)\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# Old API stack (Policy, RolloutWorker, Connector).\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.enable_env_runner_and_connector_v2:\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_training_step_old_api_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m     \u001b[38;5;66;03m# Collect batches from sample workers until we have a full batch.\u001b[39;00m\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metrics.log_time((TIMERS, ENV_RUNNER_SAMPLING_TIMER)):\n\u001b[32m    395\u001b[39m         \u001b[38;5;66;03m# Sample in parallel from the workers.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo.py:470\u001b[39m, in \u001b[36mPPO._training_step_old_api_stack\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    464\u001b[39m     train_batch = synchronous_parallel_sample(\n\u001b[32m    465\u001b[39m         worker_set=\u001b[38;5;28mself\u001b[39m.env_runner_group,\n\u001b[32m    466\u001b[39m         max_agent_steps=\u001b[38;5;28mself\u001b[39m.config.total_train_batch_size,\n\u001b[32m    467\u001b[39m         sample_timeout_s=\u001b[38;5;28mself\u001b[39m.config.sample_timeout_s,\n\u001b[32m    468\u001b[39m     )\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m     train_batch = \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_runner_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtotal_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;66;03m# Return early if all our workers failed.\u001b[39;00m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m train_batch:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py:106\u001b[39m, in \u001b[36msynchronous_parallel_sample\u001b[39m\u001b[34m(worker_set, max_agent_steps, max_env_steps, concat, sample_timeout_s, random_actions, _uses_new_env_runners, _return_metrics)\u001b[39m\n\u001b[32m    103\u001b[39m         stats_dicts = [worker_set.local_env_runner.get_metrics()]\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     sampled_data = \u001b[43mworker_set\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforeach_env_runner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrandom_action_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_return_metrics\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrandom_action_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# Nothing was returned (maybe all workers are stalling) or no healthy\u001b[39;00m\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# remote workers left: Break.\u001b[39;00m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# There is no point staying in this loop, since we will not be able to\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# get any new samples if we don't have any healthy remote workers left.\u001b[39;00m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampled_data \u001b[38;5;129;01mor\u001b[39;00m worker_set.num_healthy_remote_workers() <= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py:843\u001b[39m, in \u001b[36mEnvRunnerGroup.foreach_env_runner\u001b[39m\u001b[34m(self, func, kwargs, local_env_runner, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[39m\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._worker_manager.actor_ids():\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m local_result\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m remote_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_worker_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhealthy_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhealthy_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    853\u001b[39m FaultTolerantActorManager.handle_remote_call_result_errors(\n\u001b[32m    854\u001b[39m     remote_results, ignore_ray_errors=\u001b[38;5;28mself\u001b[39m._ignore_ray_errors_on_env_runners\n\u001b[32m    855\u001b[39m )\n\u001b[32m    857\u001b[39m \u001b[38;5;66;03m# With application errors handled, return good results.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py:461\u001b[39m, in \u001b[36mFaultTolerantActorManager.foreach_actor\u001b[39m\u001b[34m(self, func, kwargs, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[39m\n\u001b[32m    454\u001b[39m remote_calls = \u001b[38;5;28mself\u001b[39m._call_actors(\n\u001b[32m    455\u001b[39m     func=func,\n\u001b[32m    456\u001b[39m     kwargs=kwargs,\n\u001b[32m    457\u001b[39m     remote_actor_ids=remote_actor_ids,\n\u001b[32m    458\u001b[39m )\n\u001b[32m    460\u001b[39m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m _, remote_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py:839\u001b[39m, in \u001b[36mFaultTolerantActorManager._fetch_result\u001b[39m\u001b[34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m readies, _ = \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[32m    848\u001b[39m remote_results = RemoteCallResults()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22\u001b[39m, in \u001b[36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauto_init_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     21\u001b[39m     auto_init_ray()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33minit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/_private/worker.py:3113\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[39m\n\u001b[32m   3111\u001b[39m timeout = timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m10\u001b[39m**\u001b[32m6\u001b[39m\n\u001b[32m   3112\u001b[39m timeout_milliseconds = \u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3113\u001b[39m ready_ids, remaining_ids = \u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore_worker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3118\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpython/ray/_raylet.pyx:3486\u001b[39m, in \u001b[36mray._raylet.CoreWorker.wait\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpython/ray/includes/common.pxi:96\u001b[39m, in \u001b[36mray._raylet.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fffffffffffffffffeb32aed41911b97fa2b5fdd01000000 Worker ID: 8b462e4d6e0f41525463d5d1e13e882c645d6c3b06bb2360ecdb284a Node ID: 7ea1cf40fc6d0c0466ca488ab0d26efead8253b283ada5315d5125b3 Worker IP address: 127.0.0.1 Worker port: 64053 Worker PID: 29485 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1. The process receives a SIGTERM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=29492)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(RolloutWorker pid=29492)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a6eb1b5efc397222c14788e01000000 Worker ID: 2c382788931680dd0e170e8d023adecd0ae2193797a05f2cd4061998 Node ID: 7ea1cf40fc6d0c0466ca488ab0d26efead8253b283ada5315d5125b3 Worker IP address: 127.0.0.1 Worker port: 64061 Worker PID: 29486 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1. The process receives a SIGTERM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=29489)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(RolloutWorker pid=29489)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m 2025-10-06 16:34:30,137\tWARNING rl_module.py:432 -- Didn't create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m 2025-10-06 16:34:30,137\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=29487, ip=127.0.0.1, actor_id=50bb60f477e6e6541352179401000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x38a2511f0>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/default_ppo_rl_module.py\", line 31, in setup\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m     self.catalog.actor_critic_encoder_config.base_encoder_config,\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m AttributeError: 'NoneType' object has no attribute 'actor_critic_encoder_config'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=29487, ip=127.0.0.1, actor_id=50bb60f477e6e6541352179401000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x38a2511f0>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 122, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m     self.make_module()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 713, in make_module\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m     self.module = module_spec.build()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m                   ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 98, in build\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m     module = self.module_class(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m              ^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py\", line 24, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m     super().__init__(*args, **kwargs, catalog_class=catalog_class)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py\", line 50, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m     RLModule.__init__(self, *args, **kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 473, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m     raise (self._catalog_ctor_error or e)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 426, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m     self.catalog = catalog_class(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m                    ^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py\", line 74, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m     super().__init__(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 122, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m     self._determine_components_hook()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 142, in _determine_components_hook\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m     self._encoder_config = self._get_encoder_config(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 361, in _get_encoder_config\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29487)\u001b[0m ValueError: No default encoder config for obs space=Dict('agent_0': Box(-inf, inf, (4,), float32)), lstm=False found.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29493)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=29493)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m 2025-10-06 16:36:40,404\tWARNING rl_module.py:432 -- Didn't create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m 2025-10-06 16:36:40,404\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=29494, ip=127.0.0.1, actor_id=dcf8f86d01f4dba8d4916b0001000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x3884e1a60>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/default_ppo_rl_module.py\", line 31, in setup\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m     self.catalog.actor_critic_encoder_config.base_encoder_config,\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m AttributeError: 'NoneType' object has no attribute 'actor_critic_encoder_config'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=29494, ip=127.0.0.1, actor_id=dcf8f86d01f4dba8d4916b0001000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x3884e1a60>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 122, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m     self.make_module()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 713, in make_module\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m     self.module = module_spec.build()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m                   ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 98, in build\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m     module = self.module_class(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m              ^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py\", line 24, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m     super().__init__(*args, **kwargs, catalog_class=catalog_class)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py\", line 50, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m     RLModule.__init__(self, *args, **kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 473, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m     raise (self._catalog_ctor_error or e)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 426, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m     self.catalog = catalog_class(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m                    ^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py\", line 74, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m     super().__init__(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 122, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m     self._determine_components_hook()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 142, in _determine_components_hook\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m     self._encoder_config = self._get_encoder_config(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 361, in _get_encoder_config\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29494)\u001b[0m ValueError: No default encoder config for obs space=Dict('agent_0': Box(-inf, inf, (4,), float32)), lstm=False found.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29484)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=29484)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m 2025-10-06 16:39:38,785\tERROR algorithm_config.py:1032 -- Your `config.env_to_module_connector` function seems to have a wrong or outdated signature! It should be: `def myfunc(env, spaces, device): ...`, where any of these arguments are optional and may be None.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m `env` is the (vectorized) gym env.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m `spaces` is a dict of structure `{'__env__': ([vectorized env obs. space, vectorized env act. space]),'__env_single__': ([env obs. space, env act. space])}`.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m `device` is a (torch) device.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m 2025-10-06 16:39:38,878\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29491)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:227: UserWarning: \u001b[33mWARN: Expects `terminated` signal to be a boolean, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   logger.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:231: UserWarning: \u001b[33mWARN: Expects `truncated` signal to be a boolean, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   logger.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   logger.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m 2025-10-06 16:39:40,856\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: float() argument must be a string or a real number, not 'dict'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 218, in sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 337, in _sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m     results = self._try_env_step(actions_for_env)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 247, in _try_env_step\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m     raise e\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 232, in _try_env_step\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m     results = self.env.step(actions)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/wrappers/vector/dict_info_to_list.py\", line 82, in step\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m     observation, reward, terminated, truncated, infos = self.env.step(actions)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m                                                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/vector/sync_vector_env.py\", line 260, in step\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m     self._rewards[i],\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m     ~~~~~~~~~~~~~^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29490)\u001b[0m TypeError: float() argument must be a string or a real number, not 'dict'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5820ed88c8f77a6fa6c12b0501000000 Worker ID: e860c72c7caf06394455e3d00ac84e28ca13d7d33d875af391d7440f Node ID: 7ea1cf40fc6d0c0466ca488ab0d26efead8253b283ada5315d5125b3 Worker IP address: 127.0.0.1 Worker port: 64095 Worker PID: 29490 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m   from pkg_resources import resource_stream, resource_exists\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29491)\u001b[0m 2025-10-06 16:39:40,652\tERROR algorithm_config.py:1032 -- Your `config.env_to_module_connector` function seems to have a wrong or outdated signature! It should be: `def myfunc(env, spaces, device): ...`, where any of these arguments are optional and may be None.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29491)\u001b[0m `env` is the (vectorized) gym env.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29491)\u001b[0m `spaces` is a dict of structure `{'__env__': ([vectorized env obs. space, vectorized env act. space]),'__env_single__': ([env obs. space, env act. space])}`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29491)\u001b[0m `device` is a (torch) device.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29491)\u001b[0m 2025-10-06 16:39:40,728\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:227: UserWarning: \u001b[33mWARN: Expects `terminated` signal to be a boolean, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m   logger.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:231: UserWarning: \u001b[33mWARN: Expects `truncated` signal to be a boolean, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m 2025-10-06 16:39:40,856\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: float() argument must be a string or a real number, not 'dict'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 218, in sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 337, in _sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m     results = self._try_env_step(actions_for_env)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 232, in _try_env_step\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m     raise e\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m     results = self.env.step(actions)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m                                                         ^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/vector/sync_vector_env.py\", line 260, in step\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m     observation, reward, terminated, truncated, infos = self.env.step(actions)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m     self._rewards[i],\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m     ~~~~~~~~~~~~~^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=29488)\u001b[0m TypeError: float() argument must be a string or a real number, not 'dict'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:227: UserWarning: \u001b[33mWARN: Expects `terminated` signal to be a boolean, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:231: UserWarning: \u001b[33mWARN: Expects `truncated` signal to be a boolean, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m 2025-10-06 16:40:44,474\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: float() argument must be a string or a real number, not 'dict'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 218, in sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 337, in _sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m     results = self._try_env_step(actions_for_env)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m     raise e\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m     results = self.env.step(actions)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m     observation, reward, terminated, truncated, infos = self.env.step(actions)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m     self._rewards[i],\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m     ~~~~~~~~~~~~~^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30269)\u001b[0m TypeError: float() argument must be a string or a real number, not 'dict'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fffffffffffffffff42829a68ba556fe214d17d801000000 Worker ID: 742de1593b4cd5e29ae63ee76a217f88544b9153c4b5de97e4d82d01 Node ID: 7ea1cf40fc6d0c0466ca488ab0d26efead8253b283ada5315d5125b3 Worker IP address: 127.0.0.1 Worker port: 49292 Worker PID: 30270 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m   from pkg_resources import resource_stream, resource_exists\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m 2025-10-06 16:40:43,703\tERROR algorithm_config.py:1032 -- Your `config.env_to_module_connector` function seems to have a wrong or outdated signature! It should be: `def myfunc(env, spaces, device): ...`, where any of these arguments are optional and may be None.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m `env` is the (vectorized) gym env.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m `spaces` is a dict of structure `{'__env__': ([vectorized env obs. space, vectorized env act. space]),'__env_single__': ([env obs. space, env act. space])}`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m `device` is a (torch) device.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m 2025-10-06 16:40:43,790\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m   logger.warn(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 232, in _try_env_step\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m                                                         ^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/vector/sync_vector_env.py\", line 260, in step\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:227: UserWarning: \u001b[33mWARN: Expects `terminated` signal to be a boolean, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:231: UserWarning: \u001b[33mWARN: Expects `truncated` signal to be a boolean, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m 2025-10-06 16:40:44,474\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: float() argument must be a string or a real number, not 'dict'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 218, in sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 337, in _sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m     results = self._try_env_step(actions_for_env)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m     raise e\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m     results = self.env.step(actions)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m     observation, reward, terminated, truncated, infos = self.env.step(actions)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m     self._rewards[i],\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m     ~~~~~~~~~~~~~^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30270)\u001b[0m TypeError: float() argument must be a string or a real number, not 'dict'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=30277)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:227: UserWarning: \u001b[33mWARN: Expects `terminated` signal to be a boolean, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:231: UserWarning: \u001b[33mWARN: Expects `truncated` signal to be a boolean, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m 2025-10-06 16:41:48,545\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: float() argument must be a string or a real number, not 'dict'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 218, in sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 337, in _sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m     results = self._try_env_step(actions_for_env)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m     raise e\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m     results = self.env.step(actions)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m     observation, reward, terminated, truncated, infos = self.env.step(actions)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m     self._rewards[i],\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m     ~~~~~~~~~~~~~^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30278)\u001b[0m TypeError: float() argument must be a string or a real number, not 'dict'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m 2025-10-06 16:42:08,645\tWARNING rl_module.py:432 -- Didn't create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m 2025-10-06 16:42:08,645\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=30294, ip=127.0.0.1, actor_id=7225893e664729a718ae430801000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x157b58740>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/default_ppo_rl_module.py\", line 31, in setup\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m     self.catalog.actor_critic_encoder_config.base_encoder_config,\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m AttributeError: 'NoneType' object has no attribute 'actor_critic_encoder_config'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=30294, ip=127.0.0.1, actor_id=7225893e664729a718ae430801000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x157b58740>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 122, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m     self.make_module()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 713, in make_module\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m     self.module = module_spec.build()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m                   ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 98, in build\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m     module = self.module_class(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m              ^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py\", line 24, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m     super().__init__(*args, **kwargs, catalog_class=catalog_class)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py\", line 50, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m     RLModule.__init__(self, *args, **kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 473, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m     raise (self._catalog_ctor_error or e)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 426, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m     self.catalog = catalog_class(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m                    ^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py\", line 74, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m     super().__init__(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 122, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m     self._determine_components_hook()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 142, in _determine_components_hook\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m     self._encoder_config = self._get_encoder_config(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/core/models/catalog.py\", line 361, in _get_encoder_config\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30294)\u001b[0m ValueError: No default encoder config for obs space=Dict('agent_0': Box(-inf, inf, (4,), float32)), lstm=False found.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30293)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=30293)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5820ed88c8f77a6fa6c12b0501000000 Worker ID: e235c771c9f189ceb8c285015b2be103c1fab92abe300659f87f4bd1 Node ID: 7ea1cf40fc6d0c0466ca488ab0d26efead8253b283ada5315d5125b3 Worker IP address: 127.0.0.1 Worker port: 49335 Worker PID: 30277 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=30306)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30306)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30306)\u001b[0m 2025-10-06 16:42:51,372\tERROR algorithm_config.py:1032 -- Your `config.env_to_module_connector` function seems to have a wrong or outdated signature! It should be: `def myfunc(env, spaces, device): ...`, where any of these arguments are optional and may be None.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30306)\u001b[0m `env` is the (vectorized) gym env.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30306)\u001b[0m `spaces` is a dict of structure `{'__env__': ([vectorized env obs. space, vectorized env act. space]),'__env_single__': ([env obs. space, env act. space])}`.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30306)\u001b[0m `device` is a (torch) device.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30306)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=30305)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=30306)\u001b[0m 2025-10-06 16:42:51,467\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=30309)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=30320)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:227: UserWarning: \u001b[33mWARN: Expects `terminated` signal to be a boolean, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   logger.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:231: UserWarning: \u001b[33mWARN: Expects `truncated` signal to be a boolean, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   logger.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m /Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'dict'>\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   logger.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m 2025-10-06 16:42:55,495\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: float() argument must be a string or a real number, not 'dict'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 218, in sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 337, in _sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m     results = self._try_env_step(actions_for_env)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 247, in _try_env_step\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m     raise e\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 232, in _try_env_step\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m     results = self.env.step(actions)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/wrappers/vector/dict_info_to_list.py\", line 82, in step\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m     observation, reward, terminated, truncated, infos = self.env.step(actions)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m                                                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m   File \"/Users/paula/.pyenv/versions/part-sim/lib/python3.12/site-packages/gymnasium/vector/sync_vector_env.py\", line 260, in step\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m     self._rewards[i],\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m     ~~~~~~~~~~~~~^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=30308)\u001b[0m TypeError: float() argument must be a string or a real number, not 'dict'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff8c71b5742b77e2baade6b01601000000 Worker ID: 56dbe1b149c4a762b20224c061be095cd5413243cdf746f95819914a Node ID: 7ea1cf40fc6d0c0466ca488ab0d26efead8253b283ada5315d5125b3 Worker IP address: 127.0.0.1 Worker port: 49425 Worker PID: 30309 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune import run\n",
    "import particle_v1\n",
    "from pettingzoo.utils import parallel_to_aec\n",
    "from supersuit import pettingzoo_env_to_vec_env_v1, concat_vec_envs_v1\n",
    "import os\n",
    "\n",
    "# Wrap our custom environment\n",
    "def make_env(env_config=None):\n",
    "    env = particle_v1.parallel_env(num_agents=10, num_food_sources=1, flow='none', max_cycles=25, render_mode=None)  # parallel API is best for RLlib\n",
    "    return env\n",
    "\n",
    "# Register custom environment with Rllib\n",
    "register_env(\"particle_v1\", lambda config: PettingZooEnv(make_env(config))) # we also have to use the Pettingzoo wrapper\n",
    "\n",
    "# Use PPOConfig from RLlib and call our environment\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)\n",
    "    .framework(\"torch\")  # Ensure PyTorch framework\n",
    "    .environment(\"particle_v1\", env_config={\"num_agents\": 10, \"num_food_sources\": 1, \"flow\": 'none', \"max_cycles\": 25, \"render_mode\": None})\n",
    "    .rl_module()  # Use RLModule for neural network configuration\n",
    "    .multi_agent(\n",
    "        policies={\"shared_policy\"},  # Define shared policy\n",
    "        policy_mapping_fn=lambda aid, *a, **kw: \"shared_policy\"\n",
    "    )\n",
    "    .env_runners(num_env_runners=2)  # Replace RolloutWorker with EnvRunners\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "for i in range(10):\n",
    "    result = algo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3841648",
   "metadata": {},
   "source": [
    "Let's look at the results object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 14:34:59,591\tWARNING deprecation.py:50 -- DeprecationWarning: `compute_single_action` has been deprecated. `Algorithm.compute_single_action` should no longer be used. Get the RLModule instance through `Algorithm.get_module([module ID])`, then compute actions through `RLModule.forward_inference({'obs': [obs batch]})`. This will raise an error in the future!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiAgentEnvRunner' object has no attribute 'get_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m         action = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m         action, _, _ = \u001b[43malgo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_single_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpolicy_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolicy_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# like SB3 deterministic=True\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     env.step(action)\n\u001b[32m     19\u001b[39m env.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/utils/deprecation.py:128\u001b[39m, in \u001b[36mDeprecated.<locals>._inner.<locals>._ctor\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m     deprecation_warning(\n\u001b[32m    122\u001b[39m         old=old \u001b[38;5;129;01mor\u001b[39;00m obj.\u001b[34m__name__\u001b[39m,\n\u001b[32m    123\u001b[39m         new=new,\n\u001b[32m    124\u001b[39m         help=help,\n\u001b[32m    125\u001b[39m         error=error,\n\u001b[32m    126\u001b[39m     )\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Call the deprecated method/function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:4271\u001b[39m, in \u001b[36mAlgorithm.compute_single_action\u001b[39m\u001b[34m(self, observation, state, prev_action, prev_reward, info, input_dict, policy_id, full_fetch, explore, timestep, episode, unsquash_action, clip_action)\u001b[39m\n\u001b[32m   4268\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4269\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m observation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, err_msg\n\u001b[32m-> \u001b[39m\u001b[32m4271\u001b[39m policy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4272\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4273\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   4274\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPolicyID \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found in PolicyMap of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4275\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAlgorithm\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms local worker!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4276\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/part-sim/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:2398\u001b[39m, in \u001b[36mAlgorithm.get_policy\u001b[39m\u001b[34m(self, policy_id)\u001b[39m\n\u001b[32m   2391\u001b[39m \u001b[38;5;129m@OldAPIStack\u001b[39m\n\u001b[32m   2392\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_policy\u001b[39m(\u001b[38;5;28mself\u001b[39m, policy_id: PolicyID = DEFAULT_POLICY_ID) -> Policy:\n\u001b[32m   2393\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return policy for the specified id, or None.\u001b[39;00m\n\u001b[32m   2394\u001b[39m \n\u001b[32m   2395\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   2396\u001b[39m \u001b[33;03m        policy_id: ID of the policy to return.\u001b[39;00m\n\u001b[32m   2397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_policy\u001b[49m(policy_id)\n",
      "\u001b[31mAttributeError\u001b[39m: 'MultiAgentEnvRunner' object has no attribute 'get_policy'"
     ]
    }
   ],
   "source": [
    "# Like before, iterate over agents, but now use a trained PPO model to select actions\n",
    "env = particle_v1.parallel_env(num_agents=10, num_food_sources=1, flow = 'none', max_cycles=25, render_mode='human')\n",
    "env.reset(seed=42) # Do seed=42 for reproducibility\n",
    "policy_id = \"shared_policy\"\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        action, _, _ = algo.compute_single_action(\n",
    "            observation,\n",
    "            policy_id=policy_id,\n",
    "            explore=False   # like SB3 deterministic=True\n",
    "        )\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "part-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
